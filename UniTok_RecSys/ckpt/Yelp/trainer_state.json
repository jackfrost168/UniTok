{
  "best_metric": 1.0987824201583862,
  "best_model_checkpoint": "./ckpt/Yelp/checkpoint-38720",
  "epoch": 108.0,
  "eval_steps": 1000,
  "global_step": 47520,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 18.873544692993164,
      "learning_rate": 5.681818181818182e-06,
      "loss": 21.5213,
      "step": 10
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 15.595904350280762,
      "learning_rate": 1.1363636363636365e-05,
      "loss": 20.4445,
      "step": 20
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 10.03122615814209,
      "learning_rate": 1.7045454545454546e-05,
      "loss": 18.5891,
      "step": 30
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 6.194453716278076,
      "learning_rate": 2.272727272727273e-05,
      "loss": 16.8822,
      "step": 40
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 4.557052135467529,
      "learning_rate": 2.840909090909091e-05,
      "loss": 15.3912,
      "step": 50
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 4.521078109741211,
      "learning_rate": 3.409090909090909e-05,
      "loss": 14.0576,
      "step": 60
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 3.995685338973999,
      "learning_rate": 3.9772727272727275e-05,
      "loss": 12.3973,
      "step": 70
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 3.419309616088867,
      "learning_rate": 4.545454545454546e-05,
      "loss": 10.7174,
      "step": 80
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 2.7929470539093018,
      "learning_rate": 5.113636363636364e-05,
      "loss": 9.4598,
      "step": 90
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.9300962686538696,
      "learning_rate": 5.681818181818182e-05,
      "loss": 8.164,
      "step": 100
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.5695222616195679,
      "learning_rate": 6.25e-05,
      "loss": 7.3695,
      "step": 110
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 1.2588778734207153,
      "learning_rate": 6.818181818181818e-05,
      "loss": 6.8804,
      "step": 120
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 1.1515722274780273,
      "learning_rate": 7.386363636363637e-05,
      "loss": 6.6277,
      "step": 130
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 1.166514277458191,
      "learning_rate": 7.954545454545455e-05,
      "loss": 6.4708,
      "step": 140
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 1.1030378341674805,
      "learning_rate": 8.522727272727272e-05,
      "loss": 6.3038,
      "step": 150
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 1.0073387622833252,
      "learning_rate": 9.090909090909092e-05,
      "loss": 6.1069,
      "step": 160
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 1.0059293508529663,
      "learning_rate": 9.659090909090909e-05,
      "loss": 6.0196,
      "step": 170
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.9424241185188293,
      "learning_rate": 0.00010227272727272728,
      "loss": 5.9052,
      "step": 180
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.9248895049095154,
      "learning_rate": 0.00010795454545454545,
      "loss": 5.839,
      "step": 190
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.8867608308792114,
      "learning_rate": 0.00011363636363636364,
      "loss": 5.7136,
      "step": 200
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.8596612811088562,
      "learning_rate": 0.00011931818181818182,
      "loss": 5.6594,
      "step": 210
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.9132897853851318,
      "learning_rate": 0.000125,
      "loss": 5.5799,
      "step": 220
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.859880268573761,
      "learning_rate": 0.00013068181818181817,
      "loss": 5.5846,
      "step": 230
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.8110321164131165,
      "learning_rate": 0.00013636363636363637,
      "loss": 5.4521,
      "step": 240
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.7925430536270142,
      "learning_rate": 0.00014204545454545457,
      "loss": 5.4549,
      "step": 250
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.7882115840911865,
      "learning_rate": 0.00014772727272727274,
      "loss": 5.3699,
      "step": 260
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.7756584882736206,
      "learning_rate": 0.0001534090909090909,
      "loss": 5.3515,
      "step": 270
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.787725567817688,
      "learning_rate": 0.0001590909090909091,
      "loss": 5.3009,
      "step": 280
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.7145629525184631,
      "learning_rate": 0.00016477272727272727,
      "loss": 5.2635,
      "step": 290
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.7156474590301514,
      "learning_rate": 0.00017045454545454544,
      "loss": 5.1703,
      "step": 300
    },
    {
      "epoch": 0.7045454545454546,
      "grad_norm": 0.6882849335670471,
      "learning_rate": 0.00017613636363636364,
      "loss": 5.1446,
      "step": 310
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.7054064869880676,
      "learning_rate": 0.00018181818181818183,
      "loss": 5.1604,
      "step": 320
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.6714275479316711,
      "learning_rate": 0.0001875,
      "loss": 5.0805,
      "step": 330
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.6923878788948059,
      "learning_rate": 0.00019318181818181817,
      "loss": 5.0509,
      "step": 340
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 0.654762327671051,
      "learning_rate": 0.00019886363636363637,
      "loss": 5.0015,
      "step": 350
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.7179489135742188,
      "learning_rate": 0.00020454545454545457,
      "loss": 4.9741,
      "step": 360
    },
    {
      "epoch": 0.8409090909090909,
      "grad_norm": 0.5942093133926392,
      "learning_rate": 0.00021022727272727274,
      "loss": 4.9392,
      "step": 370
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 1.5094249248504639,
      "learning_rate": 0.0002159090909090909,
      "loss": 4.86,
      "step": 380
    },
    {
      "epoch": 0.8863636363636364,
      "grad_norm": 0.6164212226867676,
      "learning_rate": 0.0002215909090909091,
      "loss": 4.8338,
      "step": 390
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.5931631326675415,
      "learning_rate": 0.00022727272727272727,
      "loss": 4.7778,
      "step": 400
    },
    {
      "epoch": 0.9318181818181818,
      "grad_norm": 0.6569234132766724,
      "learning_rate": 0.00023295454545454544,
      "loss": 4.7672,
      "step": 410
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.5647724270820618,
      "learning_rate": 0.00023863636363636364,
      "loss": 4.7038,
      "step": 420
    },
    {
      "epoch": 0.9772727272727273,
      "grad_norm": 0.6262201070785522,
      "learning_rate": 0.0002443181818181818,
      "loss": 4.6454,
      "step": 430
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6593677997589111,
      "learning_rate": 0.00025,
      "loss": 4.609,
      "step": 440
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.213801145553589,
      "eval_runtime": 9.3057,
      "eval_samples_per_second": 3270.153,
      "eval_steps_per_second": 12.788,
      "step": 440
    },
    {
      "epoch": 1.0227272727272727,
      "grad_norm": 0.5574148893356323,
      "learning_rate": 0.0002556818181818182,
      "loss": 4.6072,
      "step": 450
    },
    {
      "epoch": 1.0454545454545454,
      "grad_norm": 0.5743900537490845,
      "learning_rate": 0.00026136363636363634,
      "loss": 4.5432,
      "step": 460
    },
    {
      "epoch": 1.0681818181818181,
      "grad_norm": 0.48290494084358215,
      "learning_rate": 0.00026704545454545454,
      "loss": 4.4635,
      "step": 470
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.5582849383354187,
      "learning_rate": 0.00027272727272727274,
      "loss": 4.4503,
      "step": 480
    },
    {
      "epoch": 1.1136363636363635,
      "grad_norm": 0.5423017740249634,
      "learning_rate": 0.0002784090909090909,
      "loss": 4.4289,
      "step": 490
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 0.4639117419719696,
      "learning_rate": 0.00028409090909090913,
      "loss": 4.3494,
      "step": 500
    },
    {
      "epoch": 1.1590909090909092,
      "grad_norm": 0.45603442192077637,
      "learning_rate": 0.0002897727272727273,
      "loss": 4.3166,
      "step": 510
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.4838819205760956,
      "learning_rate": 0.00029545454545454547,
      "loss": 4.2623,
      "step": 520
    },
    {
      "epoch": 1.2045454545454546,
      "grad_norm": 0.44252637028694153,
      "learning_rate": 0.00030113636363636367,
      "loss": 4.2085,
      "step": 530
    },
    {
      "epoch": 1.2272727272727273,
      "grad_norm": 0.48518630862236023,
      "learning_rate": 0.0003068181818181818,
      "loss": 4.1539,
      "step": 540
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.458543062210083,
      "learning_rate": 0.0003125,
      "loss": 4.1566,
      "step": 550
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.5786274075508118,
      "learning_rate": 0.0003181818181818182,
      "loss": 4.0956,
      "step": 560
    },
    {
      "epoch": 1.2954545454545454,
      "grad_norm": 0.4928729832172394,
      "learning_rate": 0.00032386363636363635,
      "loss": 4.0502,
      "step": 570
    },
    {
      "epoch": 1.3181818181818181,
      "grad_norm": 0.4077252149581909,
      "learning_rate": 0.00032954545454545454,
      "loss": 4.0013,
      "step": 580
    },
    {
      "epoch": 1.3409090909090908,
      "grad_norm": 0.6298971176147461,
      "learning_rate": 0.00033522727272727274,
      "loss": 3.9451,
      "step": 590
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.40315595269203186,
      "learning_rate": 0.0003409090909090909,
      "loss": 3.9399,
      "step": 600
    },
    {
      "epoch": 1.3863636363636362,
      "grad_norm": 0.4174027442932129,
      "learning_rate": 0.00034659090909090913,
      "loss": 3.8832,
      "step": 610
    },
    {
      "epoch": 1.4090909090909092,
      "grad_norm": 0.5095511078834534,
      "learning_rate": 0.0003522727272727273,
      "loss": 3.8162,
      "step": 620
    },
    {
      "epoch": 1.4318181818181819,
      "grad_norm": 0.4808728098869324,
      "learning_rate": 0.00035795454545454547,
      "loss": 3.7855,
      "step": 630
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.3880552649497986,
      "learning_rate": 0.00036363636363636367,
      "loss": 3.7552,
      "step": 640
    },
    {
      "epoch": 1.4772727272727273,
      "grad_norm": 0.46811386942863464,
      "learning_rate": 0.0003693181818181818,
      "loss": 3.7031,
      "step": 650
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.35046738386154175,
      "learning_rate": 0.000375,
      "loss": 3.6564,
      "step": 660
    },
    {
      "epoch": 1.5227272727272727,
      "grad_norm": 0.4259721040725708,
      "learning_rate": 0.0003806818181818182,
      "loss": 3.6219,
      "step": 670
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.3969365656375885,
      "learning_rate": 0.00038636363636363635,
      "loss": 3.5714,
      "step": 680
    },
    {
      "epoch": 1.5681818181818183,
      "grad_norm": 0.7413539290428162,
      "learning_rate": 0.00039204545454545454,
      "loss": 3.54,
      "step": 690
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 0.327938050031662,
      "learning_rate": 0.00039772727272727274,
      "loss": 3.5202,
      "step": 700
    },
    {
      "epoch": 1.6136363636363638,
      "grad_norm": 0.3418932259082794,
      "learning_rate": 0.0004034090909090909,
      "loss": 3.4592,
      "step": 710
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 0.34309616684913635,
      "learning_rate": 0.00040909090909090913,
      "loss": 3.411,
      "step": 720
    },
    {
      "epoch": 1.6590909090909092,
      "grad_norm": 0.3770619332790375,
      "learning_rate": 0.0004147727272727273,
      "loss": 3.3758,
      "step": 730
    },
    {
      "epoch": 1.6818181818181817,
      "grad_norm": 0.3554236888885498,
      "learning_rate": 0.0004204545454545455,
      "loss": 3.3514,
      "step": 740
    },
    {
      "epoch": 1.7045454545454546,
      "grad_norm": 0.32006099820137024,
      "learning_rate": 0.00042613636363636367,
      "loss": 3.319,
      "step": 750
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.3189331293106079,
      "learning_rate": 0.0004318181818181818,
      "loss": 3.2858,
      "step": 760
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.29158440232276917,
      "learning_rate": 0.0004375,
      "loss": 3.2146,
      "step": 770
    },
    {
      "epoch": 1.7727272727272727,
      "grad_norm": 0.2997976541519165,
      "learning_rate": 0.0004431818181818182,
      "loss": 3.1924,
      "step": 780
    },
    {
      "epoch": 1.7954545454545454,
      "grad_norm": 0.3179197907447815,
      "learning_rate": 0.00044886363636363635,
      "loss": 3.1537,
      "step": 790
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.30064961314201355,
      "learning_rate": 0.00045454545454545455,
      "loss": 3.1279,
      "step": 800
    },
    {
      "epoch": 1.8409090909090908,
      "grad_norm": 0.2709900736808777,
      "learning_rate": 0.00046022727272727274,
      "loss": 3.0996,
      "step": 810
    },
    {
      "epoch": 1.8636363636363638,
      "grad_norm": 0.2782762944698334,
      "learning_rate": 0.0004659090909090909,
      "loss": 3.0592,
      "step": 820
    },
    {
      "epoch": 1.8863636363636362,
      "grad_norm": 0.370268851518631,
      "learning_rate": 0.00047159090909090914,
      "loss": 3.0168,
      "step": 830
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.2986060082912445,
      "learning_rate": 0.0004772727272727273,
      "loss": 2.9915,
      "step": 840
    },
    {
      "epoch": 1.9318181818181817,
      "grad_norm": 0.2712177336215973,
      "learning_rate": 0.0004829545454545455,
      "loss": 2.9767,
      "step": 850
    },
    {
      "epoch": 1.9545454545454546,
      "grad_norm": 0.25602054595947266,
      "learning_rate": 0.0004886363636363636,
      "loss": 2.9295,
      "step": 860
    },
    {
      "epoch": 1.9772727272727273,
      "grad_norm": 0.2675725221633911,
      "learning_rate": 0.0004943181818181818,
      "loss": 2.9189,
      "step": 870
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.3031087815761566,
      "learning_rate": 0.0005,
      "loss": 2.8819,
      "step": 880
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.397136926651001,
      "eval_runtime": 9.4359,
      "eval_samples_per_second": 3225.012,
      "eval_steps_per_second": 12.611,
      "step": 880
    },
    {
      "epoch": 2.022727272727273,
      "grad_norm": 0.3134467601776123,
      "learning_rate": 0.0004999999837454848,
      "loss": 2.8552,
      "step": 890
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 0.2114914506673813,
      "learning_rate": 0.0004999999349819414,
      "loss": 2.8352,
      "step": 900
    },
    {
      "epoch": 2.0681818181818183,
      "grad_norm": 0.24449336528778076,
      "learning_rate": 0.0004999998537093762,
      "loss": 2.8192,
      "step": 910
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 0.21913039684295654,
      "learning_rate": 0.0004999997399277996,
      "loss": 2.7786,
      "step": 920
    },
    {
      "epoch": 2.1136363636363638,
      "grad_norm": 0.3273962438106537,
      "learning_rate": 0.0004999995936372264,
      "loss": 2.7613,
      "step": 930
    },
    {
      "epoch": 2.1363636363636362,
      "grad_norm": 0.22723649442195892,
      "learning_rate": 0.0004999994148376757,
      "loss": 2.7377,
      "step": 940
    },
    {
      "epoch": 2.159090909090909,
      "grad_norm": 0.19678302109241486,
      "learning_rate": 0.0004999992035291708,
      "loss": 2.7335,
      "step": 950
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.41892606019973755,
      "learning_rate": 0.0004999989597117392,
      "loss": 2.7291,
      "step": 960
    },
    {
      "epoch": 2.2045454545454546,
      "grad_norm": 0.19658738374710083,
      "learning_rate": 0.0004999986833854125,
      "loss": 2.6916,
      "step": 970
    },
    {
      "epoch": 2.227272727272727,
      "grad_norm": 0.3581089675426483,
      "learning_rate": 0.0004999983745502266,
      "loss": 2.6898,
      "step": 980
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.28518280386924744,
      "learning_rate": 0.0004999980332062218,
      "loss": 2.6948,
      "step": 990
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 0.20644602179527283,
      "learning_rate": 0.0004999976593534424,
      "loss": 2.6455,
      "step": 1000
    },
    {
      "epoch": 2.2954545454545454,
      "grad_norm": 0.21840713918209076,
      "learning_rate": 0.0004999972529919369,
      "loss": 2.6404,
      "step": 1010
    },
    {
      "epoch": 2.3181818181818183,
      "grad_norm": 0.16939668357372284,
      "learning_rate": 0.0004999968141217585,
      "loss": 2.6032,
      "step": 1020
    },
    {
      "epoch": 2.340909090909091,
      "grad_norm": 0.2655399441719055,
      "learning_rate": 0.0004999963427429638,
      "loss": 2.6227,
      "step": 1030
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.18656019866466522,
      "learning_rate": 0.0004999958388556144,
      "loss": 2.5953,
      "step": 1040
    },
    {
      "epoch": 2.3863636363636362,
      "grad_norm": 0.15538696944713593,
      "learning_rate": 0.0004999953024597758,
      "loss": 2.5759,
      "step": 1050
    },
    {
      "epoch": 2.409090909090909,
      "grad_norm": 0.23312905430793762,
      "learning_rate": 0.0004999947335555177,
      "loss": 2.5897,
      "step": 1060
    },
    {
      "epoch": 2.4318181818181817,
      "grad_norm": 0.18038807809352875,
      "learning_rate": 0.000499994132142914,
      "loss": 2.5429,
      "step": 1070
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 0.18459577858448029,
      "learning_rate": 0.0004999934982220431,
      "loss": 2.5591,
      "step": 1080
    },
    {
      "epoch": 2.4772727272727275,
      "grad_norm": 0.21284441649913788,
      "learning_rate": 0.0004999928317929873,
      "loss": 2.5634,
      "step": 1090
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.15853697061538696,
      "learning_rate": 0.0004999921328558333,
      "loss": 2.5314,
      "step": 1100
    },
    {
      "epoch": 2.5227272727272725,
      "grad_norm": 0.16075149178504944,
      "learning_rate": 0.0004999914014106719,
      "loss": 2.5411,
      "step": 1110
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.17799444496631622,
      "learning_rate": 0.0004999906374575983,
      "loss": 2.5282,
      "step": 1120
    },
    {
      "epoch": 2.5681818181818183,
      "grad_norm": 0.16287736594676971,
      "learning_rate": 0.0004999898409967119,
      "loss": 2.5169,
      "step": 1130
    },
    {
      "epoch": 2.590909090909091,
      "grad_norm": 0.18689052760601044,
      "learning_rate": 0.0004999890120281162,
      "loss": 2.516,
      "step": 1140
    },
    {
      "epoch": 2.6136363636363638,
      "grad_norm": 0.17038923501968384,
      "learning_rate": 0.0004999881505519188,
      "loss": 2.495,
      "step": 1150
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 0.5834903717041016,
      "learning_rate": 0.0004999872565682321,
      "loss": 2.5002,
      "step": 1160
    },
    {
      "epoch": 2.659090909090909,
      "grad_norm": 0.17921969294548035,
      "learning_rate": 0.0004999863300771721,
      "loss": 2.4918,
      "step": 1170
    },
    {
      "epoch": 2.6818181818181817,
      "grad_norm": 0.14537309110164642,
      "learning_rate": 0.0004999853710788594,
      "loss": 2.4826,
      "step": 1180
    },
    {
      "epoch": 2.7045454545454546,
      "grad_norm": 0.25685814023017883,
      "learning_rate": 0.0004999843795734186,
      "loss": 2.4789,
      "step": 1190
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.20499934256076813,
      "learning_rate": 0.0004999833555609786,
      "loss": 2.4725,
      "step": 1200
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.19432999193668365,
      "learning_rate": 0.0004999822990416727,
      "loss": 2.4739,
      "step": 1210
    },
    {
      "epoch": 2.7727272727272725,
      "grad_norm": 0.2252788543701172,
      "learning_rate": 0.0004999812100156383,
      "loss": 2.4622,
      "step": 1220
    },
    {
      "epoch": 2.7954545454545454,
      "grad_norm": 0.2698975205421448,
      "learning_rate": 0.0004999800884830168,
      "loss": 2.4735,
      "step": 1230
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 0.203206866979599,
      "learning_rate": 0.0004999789344439543,
      "loss": 2.4514,
      "step": 1240
    },
    {
      "epoch": 2.840909090909091,
      "grad_norm": 0.17881861329078674,
      "learning_rate": 0.0004999777478986007,
      "loss": 2.4578,
      "step": 1250
    },
    {
      "epoch": 2.8636363636363638,
      "grad_norm": 0.14038652181625366,
      "learning_rate": 0.0004999765288471102,
      "loss": 2.4446,
      "step": 1260
    },
    {
      "epoch": 2.8863636363636362,
      "grad_norm": 0.1654592752456665,
      "learning_rate": 0.0004999752772896415,
      "loss": 2.4358,
      "step": 1270
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.1571284532546997,
      "learning_rate": 0.0004999739932263574,
      "loss": 2.4311,
      "step": 1280
    },
    {
      "epoch": 2.9318181818181817,
      "grad_norm": 0.18622446060180664,
      "learning_rate": 0.0004999726766574248,
      "loss": 2.4187,
      "step": 1290
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 0.2603206932544708,
      "learning_rate": 0.0004999713275830147,
      "loss": 2.4131,
      "step": 1300
    },
    {
      "epoch": 2.9772727272727275,
      "grad_norm": 0.14157693088054657,
      "learning_rate": 0.0004999699460033029,
      "loss": 2.4225,
      "step": 1310
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.23387283086776733,
      "learning_rate": 0.0004999685319184688,
      "loss": 2.4107,
      "step": 1320
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.185960054397583,
      "eval_runtime": 9.2107,
      "eval_samples_per_second": 3303.888,
      "eval_steps_per_second": 12.92,
      "step": 1320
    },
    {
      "epoch": 3.022727272727273,
      "grad_norm": 0.14779983460903168,
      "learning_rate": 0.0004999670853286963,
      "loss": 2.4078,
      "step": 1330
    },
    {
      "epoch": 3.0454545454545454,
      "grad_norm": 0.12688207626342773,
      "learning_rate": 0.0004999656062341735,
      "loss": 2.4162,
      "step": 1340
    },
    {
      "epoch": 3.0681818181818183,
      "grad_norm": 0.1670898050069809,
      "learning_rate": 0.0004999640946350929,
      "loss": 2.4206,
      "step": 1350
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 0.14263997972011566,
      "learning_rate": 0.0004999625505316509,
      "loss": 2.4219,
      "step": 1360
    },
    {
      "epoch": 3.1136363636363638,
      "grad_norm": 0.136114701628685,
      "learning_rate": 0.0004999609739240484,
      "loss": 2.4263,
      "step": 1370
    },
    {
      "epoch": 3.1363636363636362,
      "grad_norm": 0.21179144084453583,
      "learning_rate": 0.0004999593648124903,
      "loss": 2.3888,
      "step": 1380
    },
    {
      "epoch": 3.159090909090909,
      "grad_norm": 0.29349762201309204,
      "learning_rate": 0.0004999577231971859,
      "loss": 2.4039,
      "step": 1390
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.17442801594734192,
      "learning_rate": 0.0004999560490783487,
      "loss": 2.4027,
      "step": 1400
    },
    {
      "epoch": 3.2045454545454546,
      "grad_norm": 0.1390119194984436,
      "learning_rate": 0.0004999543424561964,
      "loss": 2.4049,
      "step": 1410
    },
    {
      "epoch": 3.227272727272727,
      "grad_norm": 0.2832498252391815,
      "learning_rate": 0.0004999526033309509,
      "loss": 2.4108,
      "step": 1420
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.19304604828357697,
      "learning_rate": 0.0004999508317028382,
      "loss": 2.3946,
      "step": 1430
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 0.2985190749168396,
      "learning_rate": 0.0004999490275720888,
      "loss": 2.4093,
      "step": 1440
    },
    {
      "epoch": 3.2954545454545454,
      "grad_norm": 0.1803879290819168,
      "learning_rate": 0.0004999471909389374,
      "loss": 2.3887,
      "step": 1450
    },
    {
      "epoch": 3.3181818181818183,
      "grad_norm": 0.3999904692173004,
      "learning_rate": 0.0004999453218036227,
      "loss": 2.393,
      "step": 1460
    },
    {
      "epoch": 3.340909090909091,
      "grad_norm": 0.16113333404064178,
      "learning_rate": 0.0004999434201663879,
      "loss": 2.3978,
      "step": 1470
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 0.16489636898040771,
      "learning_rate": 0.0004999414860274801,
      "loss": 2.3663,
      "step": 1480
    },
    {
      "epoch": 3.3863636363636362,
      "grad_norm": 0.14142875373363495,
      "learning_rate": 0.0004999395193871508,
      "loss": 2.3833,
      "step": 1490
    },
    {
      "epoch": 3.409090909090909,
      "grad_norm": 0.43213459849357605,
      "learning_rate": 0.000499937520245656,
      "loss": 2.3684,
      "step": 1500
    },
    {
      "epoch": 3.4318181818181817,
      "grad_norm": 0.19311732053756714,
      "learning_rate": 0.0004999354886032555,
      "loss": 2.3751,
      "step": 1510
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 0.1760486513376236,
      "learning_rate": 0.0004999334244602133,
      "loss": 2.3975,
      "step": 1520
    },
    {
      "epoch": 3.4772727272727275,
      "grad_norm": 0.14218860864639282,
      "learning_rate": 0.000499931327816798,
      "loss": 2.3574,
      "step": 1530
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.13836278021335602,
      "learning_rate": 0.0004999291986732823,
      "loss": 2.377,
      "step": 1540
    },
    {
      "epoch": 3.5227272727272725,
      "grad_norm": 0.1276436150074005,
      "learning_rate": 0.0004999270370299429,
      "loss": 2.3603,
      "step": 1550
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 0.13276582956314087,
      "learning_rate": 0.000499924842887061,
      "loss": 2.3766,
      "step": 1560
    },
    {
      "epoch": 3.5681818181818183,
      "grad_norm": 0.12187112867832184,
      "learning_rate": 0.0004999226162449219,
      "loss": 2.3718,
      "step": 1570
    },
    {
      "epoch": 3.590909090909091,
      "grad_norm": 0.13730402290821075,
      "learning_rate": 0.0004999203571038152,
      "loss": 2.3658,
      "step": 1580
    },
    {
      "epoch": 3.6136363636363638,
      "grad_norm": 0.14621083438396454,
      "learning_rate": 0.0004999180654640346,
      "loss": 2.3848,
      "step": 1590
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.19424159824848175,
      "learning_rate": 0.0004999157413258782,
      "loss": 2.3649,
      "step": 1600
    },
    {
      "epoch": 3.659090909090909,
      "grad_norm": 0.12114351987838745,
      "learning_rate": 0.0004999133846896479,
      "loss": 2.3691,
      "step": 1610
    },
    {
      "epoch": 3.6818181818181817,
      "grad_norm": 0.14249064028263092,
      "learning_rate": 0.0004999109955556505,
      "loss": 2.3686,
      "step": 1620
    },
    {
      "epoch": 3.7045454545454546,
      "grad_norm": 0.18614064157009125,
      "learning_rate": 0.0004999085739241966,
      "loss": 2.3881,
      "step": 1630
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 0.15826097130775452,
      "learning_rate": 0.000499906119795601,
      "loss": 2.375,
      "step": 1640
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.1302146166563034,
      "learning_rate": 0.0004999036331701828,
      "loss": 2.3592,
      "step": 1650
    },
    {
      "epoch": 3.7727272727272725,
      "grad_norm": 0.13513241708278656,
      "learning_rate": 0.0004999011140482655,
      "loss": 2.3602,
      "step": 1660
    },
    {
      "epoch": 3.7954545454545454,
      "grad_norm": 0.1200261041522026,
      "learning_rate": 0.0004998985624301766,
      "loss": 2.3647,
      "step": 1670
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 0.13678383827209473,
      "learning_rate": 0.0004998959783162479,
      "loss": 2.3618,
      "step": 1680
    },
    {
      "epoch": 3.840909090909091,
      "grad_norm": 0.13750296831130981,
      "learning_rate": 0.0004998933617068154,
      "loss": 2.3675,
      "step": 1690
    },
    {
      "epoch": 3.8636363636363638,
      "grad_norm": 0.11971031129360199,
      "learning_rate": 0.0004998907126022194,
      "loss": 2.3668,
      "step": 1700
    },
    {
      "epoch": 3.8863636363636362,
      "grad_norm": 0.12987294793128967,
      "learning_rate": 0.0004998880310028044,
      "loss": 2.3575,
      "step": 1710
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 0.16513007879257202,
      "learning_rate": 0.0004998853169089191,
      "loss": 2.3642,
      "step": 1720
    },
    {
      "epoch": 3.9318181818181817,
      "grad_norm": 0.26089805364608765,
      "learning_rate": 0.0004998825703209162,
      "loss": 2.3642,
      "step": 1730
    },
    {
      "epoch": 3.9545454545454546,
      "grad_norm": 0.19488373398780823,
      "learning_rate": 0.0004998797912391532,
      "loss": 2.3506,
      "step": 1740
    },
    {
      "epoch": 3.9772727272727275,
      "grad_norm": 0.12174920737743378,
      "learning_rate": 0.0004998769796639913,
      "loss": 2.3527,
      "step": 1750
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.21627983450889587,
      "learning_rate": 0.0004998741355957963,
      "loss": 2.3386,
      "step": 1760
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.151483178138733,
      "eval_runtime": 9.4503,
      "eval_samples_per_second": 3220.122,
      "eval_steps_per_second": 12.592,
      "step": 1760
    },
    {
      "epoch": 4.0227272727272725,
      "grad_norm": 0.13588394224643707,
      "learning_rate": 0.0004998712590349377,
      "loss": 2.3514,
      "step": 1770
    },
    {
      "epoch": 4.045454545454546,
      "grad_norm": 0.1317405104637146,
      "learning_rate": 0.0004998683499817899,
      "loss": 2.338,
      "step": 1780
    },
    {
      "epoch": 4.068181818181818,
      "grad_norm": 0.1885981559753418,
      "learning_rate": 0.0004998654084367309,
      "loss": 2.3596,
      "step": 1790
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.13800783455371857,
      "learning_rate": 0.0004998624344001432,
      "loss": 2.3555,
      "step": 1800
    },
    {
      "epoch": 4.113636363636363,
      "grad_norm": 0.2822968065738678,
      "learning_rate": 0.0004998594278724139,
      "loss": 2.3395,
      "step": 1810
    },
    {
      "epoch": 4.136363636363637,
      "grad_norm": 0.13453447818756104,
      "learning_rate": 0.0004998563888539336,
      "loss": 2.337,
      "step": 1820
    },
    {
      "epoch": 4.159090909090909,
      "grad_norm": 0.12470405548810959,
      "learning_rate": 0.0004998533173450975,
      "loss": 2.3459,
      "step": 1830
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 0.14419513940811157,
      "learning_rate": 0.0004998502133463052,
      "loss": 2.3405,
      "step": 1840
    },
    {
      "epoch": 4.204545454545454,
      "grad_norm": 0.21838831901550293,
      "learning_rate": 0.0004998470768579603,
      "loss": 2.3451,
      "step": 1850
    },
    {
      "epoch": 4.2272727272727275,
      "grad_norm": 0.5115958452224731,
      "learning_rate": 0.0004998439078804705,
      "loss": 2.3434,
      "step": 1860
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.13310186564922333,
      "learning_rate": 0.0004998407064142479,
      "loss": 2.333,
      "step": 1870
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 0.11417439579963684,
      "learning_rate": 0.000499837472459709,
      "loss": 2.334,
      "step": 1880
    },
    {
      "epoch": 4.295454545454546,
      "grad_norm": 0.14850285649299622,
      "learning_rate": 0.0004998342060172741,
      "loss": 2.3431,
      "step": 1890
    },
    {
      "epoch": 4.318181818181818,
      "grad_norm": 0.27447834610939026,
      "learning_rate": 0.0004998309070873682,
      "loss": 2.3503,
      "step": 1900
    },
    {
      "epoch": 4.340909090909091,
      "grad_norm": 0.127812922000885,
      "learning_rate": 0.0004998275756704201,
      "loss": 2.3301,
      "step": 1910
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 0.12581776082515717,
      "learning_rate": 0.000499824211766863,
      "loss": 2.3584,
      "step": 1920
    },
    {
      "epoch": 4.386363636363637,
      "grad_norm": 0.31337031722068787,
      "learning_rate": 0.0004998208153771343,
      "loss": 2.3365,
      "step": 1930
    },
    {
      "epoch": 4.409090909090909,
      "grad_norm": 0.18170946836471558,
      "learning_rate": 0.0004998173865016759,
      "loss": 2.337,
      "step": 1940
    },
    {
      "epoch": 4.431818181818182,
      "grad_norm": 0.12183833867311478,
      "learning_rate": 0.0004998139251409334,
      "loss": 2.3259,
      "step": 1950
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 0.12821249663829803,
      "learning_rate": 0.000499810431295357,
      "loss": 2.3557,
      "step": 1960
    },
    {
      "epoch": 4.4772727272727275,
      "grad_norm": 0.11935140192508698,
      "learning_rate": 0.0004998069049654011,
      "loss": 2.3312,
      "step": 1970
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.11315266788005829,
      "learning_rate": 0.0004998033461515242,
      "loss": 2.3255,
      "step": 1980
    },
    {
      "epoch": 4.5227272727272725,
      "grad_norm": 0.12461169064044952,
      "learning_rate": 0.000499799754854189,
      "loss": 2.3271,
      "step": 1990
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 0.11922182142734528,
      "learning_rate": 0.0004997961310738625,
      "loss": 2.3377,
      "step": 2000
    },
    {
      "epoch": 4.568181818181818,
      "grad_norm": 0.1317555159330368,
      "learning_rate": 0.0004997924748110161,
      "loss": 2.3233,
      "step": 2010
    },
    {
      "epoch": 4.590909090909091,
      "grad_norm": 0.1594119817018509,
      "learning_rate": 0.0004997887860661251,
      "loss": 2.3273,
      "step": 2020
    },
    {
      "epoch": 4.613636363636363,
      "grad_norm": 0.1372622400522232,
      "learning_rate": 0.0004997850648396692,
      "loss": 2.3367,
      "step": 2030
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 0.15091174840927124,
      "learning_rate": 0.0004997813111321322,
      "loss": 2.334,
      "step": 2040
    },
    {
      "epoch": 4.659090909090909,
      "grad_norm": 0.19092229008674622,
      "learning_rate": 0.0004997775249440024,
      "loss": 2.3154,
      "step": 2050
    },
    {
      "epoch": 4.681818181818182,
      "grad_norm": 0.13640668988227844,
      "learning_rate": 0.0004997737062757721,
      "loss": 2.3253,
      "step": 2060
    },
    {
      "epoch": 4.704545454545455,
      "grad_norm": 0.15484334528446198,
      "learning_rate": 0.0004997698551279377,
      "loss": 2.3311,
      "step": 2070
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 0.162332221865654,
      "learning_rate": 0.0004997659715010002,
      "loss": 2.333,
      "step": 2080
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.16250689327716827,
      "learning_rate": 0.0004997620553954645,
      "loss": 2.3245,
      "step": 2090
    },
    {
      "epoch": 4.7727272727272725,
      "grad_norm": 0.15828937292099,
      "learning_rate": 0.0004997581068118397,
      "loss": 2.3413,
      "step": 2100
    },
    {
      "epoch": 4.795454545454545,
      "grad_norm": 0.1154371127486229,
      "learning_rate": 0.0004997541257506397,
      "loss": 2.3218,
      "step": 2110
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 0.12693820893764496,
      "learning_rate": 0.0004997501122123816,
      "loss": 2.3196,
      "step": 2120
    },
    {
      "epoch": 4.840909090909091,
      "grad_norm": 0.16651873290538788,
      "learning_rate": 0.0004997460661975877,
      "loss": 2.3332,
      "step": 2130
    },
    {
      "epoch": 4.863636363636363,
      "grad_norm": 0.13042113184928894,
      "learning_rate": 0.000499741987706784,
      "loss": 2.3234,
      "step": 2140
    },
    {
      "epoch": 4.886363636363637,
      "grad_norm": 0.263160765171051,
      "learning_rate": 0.0004997378767405009,
      "loss": 2.3168,
      "step": 2150
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 0.14800478518009186,
      "learning_rate": 0.000499733733299273,
      "loss": 2.3218,
      "step": 2160
    },
    {
      "epoch": 4.931818181818182,
      "grad_norm": 0.12560009956359863,
      "learning_rate": 0.000499729557383639,
      "loss": 2.3256,
      "step": 2170
    },
    {
      "epoch": 4.954545454545455,
      "grad_norm": 0.28566721081733704,
      "learning_rate": 0.000499725348994142,
      "loss": 2.3263,
      "step": 2180
    },
    {
      "epoch": 4.9772727272727275,
      "grad_norm": 0.1352599859237671,
      "learning_rate": 0.0004997211081313291,
      "loss": 2.3292,
      "step": 2190
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.2671622335910797,
      "learning_rate": 0.000499716834795752,
      "loss": 2.3083,
      "step": 2200
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.1396358013153076,
      "eval_runtime": 9.2887,
      "eval_samples_per_second": 3276.131,
      "eval_steps_per_second": 12.811,
      "step": 2200
    },
    {
      "epoch": 5.0227272727272725,
      "grad_norm": 0.10481209307909012,
      "learning_rate": 0.0004997125289879662,
      "loss": 2.3135,
      "step": 2210
    },
    {
      "epoch": 5.045454545454546,
      "grad_norm": 0.13360489904880524,
      "learning_rate": 0.0004997081907085317,
      "loss": 2.3218,
      "step": 2220
    },
    {
      "epoch": 5.068181818181818,
      "grad_norm": 0.11302418261766434,
      "learning_rate": 0.0004997038199580126,
      "loss": 2.3146,
      "step": 2230
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 0.2118397057056427,
      "learning_rate": 0.0004996994167369772,
      "loss": 2.3294,
      "step": 2240
    },
    {
      "epoch": 5.113636363636363,
      "grad_norm": 0.15011322498321533,
      "learning_rate": 0.0004996949810459982,
      "loss": 2.2967,
      "step": 2250
    },
    {
      "epoch": 5.136363636363637,
      "grad_norm": 0.11466944962739944,
      "learning_rate": 0.0004996905128856524,
      "loss": 2.2929,
      "step": 2260
    },
    {
      "epoch": 5.159090909090909,
      "grad_norm": 0.11636719852685928,
      "learning_rate": 0.0004996860122565206,
      "loss": 2.3191,
      "step": 2270
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 0.12399230897426605,
      "learning_rate": 0.0004996814791591883,
      "loss": 2.3252,
      "step": 2280
    },
    {
      "epoch": 5.204545454545454,
      "grad_norm": 0.11845375597476959,
      "learning_rate": 0.0004996769135942449,
      "loss": 2.3129,
      "step": 2290
    },
    {
      "epoch": 5.2272727272727275,
      "grad_norm": 0.13968108594417572,
      "learning_rate": 0.000499672315562284,
      "loss": 2.3217,
      "step": 2300
    },
    {
      "epoch": 5.25,
      "grad_norm": 0.13299055397510529,
      "learning_rate": 0.0004996676850639036,
      "loss": 2.3141,
      "step": 2310
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 0.1268475353717804,
      "learning_rate": 0.0004996630220997058,
      "loss": 2.3047,
      "step": 2320
    },
    {
      "epoch": 5.295454545454546,
      "grad_norm": 0.13762108981609344,
      "learning_rate": 0.000499658326670297,
      "loss": 2.3062,
      "step": 2330
    },
    {
      "epoch": 5.318181818181818,
      "grad_norm": 0.1228453665971756,
      "learning_rate": 0.0004996535987762875,
      "loss": 2.3151,
      "step": 2340
    },
    {
      "epoch": 5.340909090909091,
      "grad_norm": 0.10893146693706512,
      "learning_rate": 0.0004996488384182926,
      "loss": 2.3093,
      "step": 2350
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 0.1259285807609558,
      "learning_rate": 0.000499644045596931,
      "loss": 2.3116,
      "step": 2360
    },
    {
      "epoch": 5.386363636363637,
      "grad_norm": 0.11751672625541687,
      "learning_rate": 0.0004996392203128259,
      "loss": 2.3254,
      "step": 2370
    },
    {
      "epoch": 5.409090909090909,
      "grad_norm": 0.11801135540008545,
      "learning_rate": 0.0004996343625666049,
      "loss": 2.3195,
      "step": 2380
    },
    {
      "epoch": 5.431818181818182,
      "grad_norm": 0.12098370492458344,
      "learning_rate": 0.0004996294723588996,
      "loss": 2.3056,
      "step": 2390
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.16399243474006653,
      "learning_rate": 0.000499624549690346,
      "loss": 2.3187,
      "step": 2400
    },
    {
      "epoch": 5.4772727272727275,
      "grad_norm": 0.1385815441608429,
      "learning_rate": 0.0004996195945615841,
      "loss": 2.3146,
      "step": 2410
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.15773136913776398,
      "learning_rate": 0.0004996146069732582,
      "loss": 2.3164,
      "step": 2420
    },
    {
      "epoch": 5.5227272727272725,
      "grad_norm": 0.13869868218898773,
      "learning_rate": 0.0004996095869260172,
      "loss": 2.3132,
      "step": 2430
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 0.1204802617430687,
      "learning_rate": 0.0004996045344205135,
      "loss": 2.304,
      "step": 2440
    },
    {
      "epoch": 5.568181818181818,
      "grad_norm": 0.12123727053403854,
      "learning_rate": 0.0004995994494574043,
      "loss": 2.3016,
      "step": 2450
    },
    {
      "epoch": 5.590909090909091,
      "grad_norm": 0.11815857142210007,
      "learning_rate": 0.0004995943320373509,
      "loss": 2.3077,
      "step": 2460
    },
    {
      "epoch": 5.613636363636363,
      "grad_norm": 0.11231483519077301,
      "learning_rate": 0.0004995891821610184,
      "loss": 2.3041,
      "step": 2470
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 0.11667130142450333,
      "learning_rate": 0.0004995839998290769,
      "loss": 2.318,
      "step": 2480
    },
    {
      "epoch": 5.659090909090909,
      "grad_norm": 0.11825010180473328,
      "learning_rate": 0.0004995787850422001,
      "loss": 2.3172,
      "step": 2490
    },
    {
      "epoch": 5.681818181818182,
      "grad_norm": 0.12635554373264313,
      "learning_rate": 0.0004995735378010661,
      "loss": 2.3072,
      "step": 2500
    },
    {
      "epoch": 5.704545454545455,
      "grad_norm": 0.10586341470479965,
      "learning_rate": 0.0004995682581063572,
      "loss": 2.3109,
      "step": 2510
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 0.14423827826976776,
      "learning_rate": 0.00049956294595876,
      "loss": 2.2931,
      "step": 2520
    },
    {
      "epoch": 5.75,
      "grad_norm": 0.1417856514453888,
      "learning_rate": 0.0004995576013589653,
      "loss": 2.3197,
      "step": 2530
    },
    {
      "epoch": 5.7727272727272725,
      "grad_norm": 0.10450369119644165,
      "learning_rate": 0.0004995522243076681,
      "loss": 2.314,
      "step": 2540
    },
    {
      "epoch": 5.795454545454545,
      "grad_norm": 0.11162327975034714,
      "learning_rate": 0.0004995468148055675,
      "loss": 2.3133,
      "step": 2550
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 0.11504137516021729,
      "learning_rate": 0.000499541372853367,
      "loss": 2.312,
      "step": 2560
    },
    {
      "epoch": 5.840909090909091,
      "grad_norm": 0.47236117720603943,
      "learning_rate": 0.0004995358984517744,
      "loss": 2.3153,
      "step": 2570
    },
    {
      "epoch": 5.863636363636363,
      "grad_norm": 0.11330530047416687,
      "learning_rate": 0.0004995303916015013,
      "loss": 2.3171,
      "step": 2580
    },
    {
      "epoch": 5.886363636363637,
      "grad_norm": 0.10859999805688858,
      "learning_rate": 0.0004995248523032639,
      "loss": 2.2881,
      "step": 2590
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.13277560472488403,
      "learning_rate": 0.0004995192805577825,
      "loss": 2.3044,
      "step": 2600
    },
    {
      "epoch": 5.931818181818182,
      "grad_norm": 0.11422315984964371,
      "learning_rate": 0.0004995136763657817,
      "loss": 2.3065,
      "step": 2610
    },
    {
      "epoch": 5.954545454545455,
      "grad_norm": 0.12855224311351776,
      "learning_rate": 0.0004995080397279902,
      "loss": 2.3051,
      "step": 2620
    },
    {
      "epoch": 5.9772727272727275,
      "grad_norm": 0.10710185766220093,
      "learning_rate": 0.000499502370645141,
      "loss": 2.2959,
      "step": 2630
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.18805311620235443,
      "learning_rate": 0.0004994966691179711,
      "loss": 2.2968,
      "step": 2640
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.1302781105041504,
      "eval_runtime": 9.2647,
      "eval_samples_per_second": 3284.608,
      "eval_steps_per_second": 12.844,
      "step": 2640
    },
    {
      "epoch": 6.0227272727272725,
      "grad_norm": 0.11748630553483963,
      "learning_rate": 0.0004994909351472221,
      "loss": 2.2873,
      "step": 2650
    },
    {
      "epoch": 6.045454545454546,
      "grad_norm": 0.1164461299777031,
      "learning_rate": 0.0004994851687336397,
      "loss": 2.2954,
      "step": 2660
    },
    {
      "epoch": 6.068181818181818,
      "grad_norm": 0.10901089757680893,
      "learning_rate": 0.0004994793698779735,
      "loss": 2.3248,
      "step": 2670
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 0.11027029156684875,
      "learning_rate": 0.0004994735385809777,
      "loss": 2.2958,
      "step": 2680
    },
    {
      "epoch": 6.113636363636363,
      "grad_norm": 0.1040135994553566,
      "learning_rate": 0.0004994676748434106,
      "loss": 2.2963,
      "step": 2690
    },
    {
      "epoch": 6.136363636363637,
      "grad_norm": 0.10826277732849121,
      "learning_rate": 0.0004994617786660346,
      "loss": 2.3053,
      "step": 2700
    },
    {
      "epoch": 6.159090909090909,
      "grad_norm": 0.11593108624219894,
      "learning_rate": 0.0004994558500496165,
      "loss": 2.3004,
      "step": 2710
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 0.12805095314979553,
      "learning_rate": 0.0004994498889949273,
      "loss": 2.3055,
      "step": 2720
    },
    {
      "epoch": 6.204545454545454,
      "grad_norm": 0.12354087829589844,
      "learning_rate": 0.0004994438955027419,
      "loss": 2.3015,
      "step": 2730
    },
    {
      "epoch": 6.2272727272727275,
      "grad_norm": 0.10696002095937729,
      "learning_rate": 0.0004994378695738398,
      "loss": 2.2973,
      "step": 2740
    },
    {
      "epoch": 6.25,
      "grad_norm": 0.2676198482513428,
      "learning_rate": 0.0004994318112090048,
      "loss": 2.2971,
      "step": 2750
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 0.1013089120388031,
      "learning_rate": 0.0004994257204090244,
      "loss": 2.3007,
      "step": 2760
    },
    {
      "epoch": 6.295454545454546,
      "grad_norm": 0.12086131423711777,
      "learning_rate": 0.0004994195971746907,
      "loss": 2.3099,
      "step": 2770
    },
    {
      "epoch": 6.318181818181818,
      "grad_norm": 0.1246572881937027,
      "learning_rate": 0.0004994134415068002,
      "loss": 2.2998,
      "step": 2780
    },
    {
      "epoch": 6.340909090909091,
      "grad_norm": 0.11094377189874649,
      "learning_rate": 0.0004994072534061529,
      "loss": 2.3027,
      "step": 2790
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.1058407872915268,
      "learning_rate": 0.0004994010328735538,
      "loss": 2.2901,
      "step": 2800
    },
    {
      "epoch": 6.386363636363637,
      "grad_norm": 0.10474316775798798,
      "learning_rate": 0.0004993947799098119,
      "loss": 2.2902,
      "step": 2810
    },
    {
      "epoch": 6.409090909090909,
      "grad_norm": 0.11628887802362442,
      "learning_rate": 0.00049938849451574,
      "loss": 2.2936,
      "step": 2820
    },
    {
      "epoch": 6.431818181818182,
      "grad_norm": 0.10986002534627914,
      "learning_rate": 0.0004993821766921555,
      "loss": 2.2913,
      "step": 2830
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 0.09935496002435684,
      "learning_rate": 0.00049937582643988,
      "loss": 2.2851,
      "step": 2840
    },
    {
      "epoch": 6.4772727272727275,
      "grad_norm": 0.12262071669101715,
      "learning_rate": 0.0004993694437597394,
      "loss": 2.3063,
      "step": 2850
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.10506058484315872,
      "learning_rate": 0.0004993630286525634,
      "loss": 2.3057,
      "step": 2860
    },
    {
      "epoch": 6.5227272727272725,
      "grad_norm": 0.11735939979553223,
      "learning_rate": 0.0004993565811191864,
      "loss": 2.2971,
      "step": 2870
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 0.11645225435495377,
      "learning_rate": 0.0004993501011604467,
      "loss": 2.2801,
      "step": 2880
    },
    {
      "epoch": 6.568181818181818,
      "grad_norm": 0.10939221829175949,
      "learning_rate": 0.0004993435887771871,
      "loss": 2.2984,
      "step": 2890
    },
    {
      "epoch": 6.590909090909091,
      "grad_norm": 0.12462899088859558,
      "learning_rate": 0.0004993370439702543,
      "loss": 2.2949,
      "step": 2900
    },
    {
      "epoch": 6.613636363636363,
      "grad_norm": 0.10547643154859543,
      "learning_rate": 0.0004993304667404993,
      "loss": 2.2971,
      "step": 2910
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 0.1272580921649933,
      "learning_rate": 0.0004993238570887775,
      "loss": 2.2986,
      "step": 2920
    },
    {
      "epoch": 6.659090909090909,
      "grad_norm": 0.10784296691417694,
      "learning_rate": 0.0004993172150159484,
      "loss": 2.2941,
      "step": 2930
    },
    {
      "epoch": 6.681818181818182,
      "grad_norm": 0.10721580684185028,
      "learning_rate": 0.0004993105405228756,
      "loss": 2.2925,
      "step": 2940
    },
    {
      "epoch": 6.704545454545455,
      "grad_norm": 0.12356492131948471,
      "learning_rate": 0.0004993038336104271,
      "loss": 2.2948,
      "step": 2950
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.09532486647367477,
      "learning_rate": 0.0004992970942794751,
      "loss": 2.293,
      "step": 2960
    },
    {
      "epoch": 6.75,
      "grad_norm": 0.11719423532485962,
      "learning_rate": 0.0004992903225308958,
      "loss": 2.2887,
      "step": 2970
    },
    {
      "epoch": 6.7727272727272725,
      "grad_norm": 0.1065400242805481,
      "learning_rate": 0.00049928351836557,
      "loss": 2.2867,
      "step": 2980
    },
    {
      "epoch": 6.795454545454545,
      "grad_norm": 0.11328931152820587,
      "learning_rate": 0.0004992766817843822,
      "loss": 2.2963,
      "step": 2990
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.10665110498666763,
      "learning_rate": 0.0004992698127882216,
      "loss": 2.2993,
      "step": 3000
    },
    {
      "epoch": 6.840909090909091,
      "grad_norm": 0.10377825051546097,
      "learning_rate": 0.0004992629113779814,
      "loss": 2.2916,
      "step": 3010
    },
    {
      "epoch": 6.863636363636363,
      "grad_norm": 0.10522298514842987,
      "learning_rate": 0.000499255977554559,
      "loss": 2.28,
      "step": 3020
    },
    {
      "epoch": 6.886363636363637,
      "grad_norm": 0.09385650604963303,
      "learning_rate": 0.000499249011318856,
      "loss": 2.2949,
      "step": 3030
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 0.12640146911144257,
      "learning_rate": 0.0004992420126717783,
      "loss": 2.2985,
      "step": 3040
    },
    {
      "epoch": 6.931818181818182,
      "grad_norm": 0.09687202423810959,
      "learning_rate": 0.0004992349816142361,
      "loss": 2.2955,
      "step": 3050
    },
    {
      "epoch": 6.954545454545455,
      "grad_norm": 0.09650514274835587,
      "learning_rate": 0.0004992279181471435,
      "loss": 2.2821,
      "step": 3060
    },
    {
      "epoch": 6.9772727272727275,
      "grad_norm": 0.11983104795217514,
      "learning_rate": 0.0004992208222714191,
      "loss": 2.2998,
      "step": 3070
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.22302214801311493,
      "learning_rate": 0.0004992136939879857,
      "loss": 2.2848,
      "step": 3080
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.1251848936080933,
      "eval_runtime": 8.7074,
      "eval_samples_per_second": 3494.842,
      "eval_steps_per_second": 13.667,
      "step": 3080
    },
    {
      "epoch": 7.0227272727272725,
      "grad_norm": 0.14202597737312317,
      "learning_rate": 0.00049920653329777,
      "loss": 2.2924,
      "step": 3090
    },
    {
      "epoch": 7.045454545454546,
      "grad_norm": 0.1232745572924614,
      "learning_rate": 0.0004991993402017033,
      "loss": 2.2915,
      "step": 3100
    },
    {
      "epoch": 7.068181818181818,
      "grad_norm": 0.09802607446908951,
      "learning_rate": 0.000499192114700721,
      "loss": 2.2974,
      "step": 3110
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 0.09177261590957642,
      "learning_rate": 0.0004991848567957625,
      "loss": 2.2935,
      "step": 3120
    },
    {
      "epoch": 7.113636363636363,
      "grad_norm": 0.1208406612277031,
      "learning_rate": 0.0004991775664877719,
      "loss": 2.2816,
      "step": 3130
    },
    {
      "epoch": 7.136363636363637,
      "grad_norm": 0.10714414715766907,
      "learning_rate": 0.000499170243777697,
      "loss": 2.2866,
      "step": 3140
    },
    {
      "epoch": 7.159090909090909,
      "grad_norm": 0.19174039363861084,
      "learning_rate": 0.0004991628886664899,
      "loss": 2.2847,
      "step": 3150
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 0.10108236223459244,
      "learning_rate": 0.0004991555011551074,
      "loss": 2.2845,
      "step": 3160
    },
    {
      "epoch": 7.204545454545454,
      "grad_norm": 0.1119374930858612,
      "learning_rate": 0.0004991480812445097,
      "loss": 2.2825,
      "step": 3170
    },
    {
      "epoch": 7.2272727272727275,
      "grad_norm": 0.10106098651885986,
      "learning_rate": 0.0004991406289356619,
      "loss": 2.2859,
      "step": 3180
    },
    {
      "epoch": 7.25,
      "grad_norm": 0.097340427339077,
      "learning_rate": 0.0004991331442295331,
      "loss": 2.2942,
      "step": 3190
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.0902409702539444,
      "learning_rate": 0.0004991256271270965,
      "loss": 2.2858,
      "step": 3200
    },
    {
      "epoch": 7.295454545454546,
      "grad_norm": 0.14177891612052917,
      "learning_rate": 0.0004991180776293296,
      "loss": 2.2958,
      "step": 3210
    },
    {
      "epoch": 7.318181818181818,
      "grad_norm": 0.10511958599090576,
      "learning_rate": 0.0004991104957372142,
      "loss": 2.289,
      "step": 3220
    },
    {
      "epoch": 7.340909090909091,
      "grad_norm": 0.11107346415519714,
      "learning_rate": 0.000499102881451736,
      "loss": 2.2703,
      "step": 3230
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 0.09446592628955841,
      "learning_rate": 0.0004990952347738853,
      "loss": 2.2879,
      "step": 3240
    },
    {
      "epoch": 7.386363636363637,
      "grad_norm": 0.13227595388889313,
      "learning_rate": 0.0004990875557046565,
      "loss": 2.2596,
      "step": 3250
    },
    {
      "epoch": 7.409090909090909,
      "grad_norm": 0.09698014706373215,
      "learning_rate": 0.000499079844245048,
      "loss": 2.283,
      "step": 3260
    },
    {
      "epoch": 7.431818181818182,
      "grad_norm": 0.0882890373468399,
      "learning_rate": 0.0004990721003960627,
      "loss": 2.2784,
      "step": 3270
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 0.09614548087120056,
      "learning_rate": 0.0004990643241587075,
      "loss": 2.2816,
      "step": 3280
    },
    {
      "epoch": 7.4772727272727275,
      "grad_norm": 0.11285046488046646,
      "learning_rate": 0.0004990565155339935,
      "loss": 2.2846,
      "step": 3290
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.09934530407190323,
      "learning_rate": 0.0004990486745229364,
      "loss": 2.2835,
      "step": 3300
    },
    {
      "epoch": 7.5227272727272725,
      "grad_norm": 0.25809401273727417,
      "learning_rate": 0.0004990408011265556,
      "loss": 2.2964,
      "step": 3310
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 0.10954021662473679,
      "learning_rate": 0.0004990328953458749,
      "loss": 2.2797,
      "step": 3320
    },
    {
      "epoch": 7.568181818181818,
      "grad_norm": 0.10213073343038559,
      "learning_rate": 0.0004990249571819224,
      "loss": 2.305,
      "step": 3330
    },
    {
      "epoch": 7.590909090909091,
      "grad_norm": 0.13746413588523865,
      "learning_rate": 0.0004990169866357304,
      "loss": 2.2817,
      "step": 3340
    },
    {
      "epoch": 7.613636363636363,
      "grad_norm": 0.11787717789411545,
      "learning_rate": 0.0004990089837083352,
      "loss": 2.2857,
      "step": 3350
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 0.09924912452697754,
      "learning_rate": 0.0004990009484007776,
      "loss": 2.2873,
      "step": 3360
    },
    {
      "epoch": 7.659090909090909,
      "grad_norm": 0.10806982219219208,
      "learning_rate": 0.0004989928807141025,
      "loss": 2.2791,
      "step": 3370
    },
    {
      "epoch": 7.681818181818182,
      "grad_norm": 0.09993090480566025,
      "learning_rate": 0.0004989847806493589,
      "loss": 2.2736,
      "step": 3380
    },
    {
      "epoch": 7.704545454545455,
      "grad_norm": 0.15939368307590485,
      "learning_rate": 0.0004989766482076002,
      "loss": 2.2814,
      "step": 3390
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.10909666866064072,
      "learning_rate": 0.0004989684833898838,
      "loss": 2.2887,
      "step": 3400
    },
    {
      "epoch": 7.75,
      "grad_norm": 0.5495234727859497,
      "learning_rate": 0.0004989602861972715,
      "loss": 2.2947,
      "step": 3410
    },
    {
      "epoch": 7.7727272727272725,
      "grad_norm": 0.10954595357179642,
      "learning_rate": 0.0004989520566308291,
      "loss": 2.2862,
      "step": 3420
    },
    {
      "epoch": 7.795454545454545,
      "grad_norm": 0.20079287886619568,
      "learning_rate": 0.000498943794691627,
      "loss": 2.2888,
      "step": 3430
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 0.11397048830986023,
      "learning_rate": 0.0004989355003807393,
      "loss": 2.3003,
      "step": 3440
    },
    {
      "epoch": 7.840909090909091,
      "grad_norm": 0.10440672934055328,
      "learning_rate": 0.0004989271736992448,
      "loss": 2.2897,
      "step": 3450
    },
    {
      "epoch": 7.863636363636363,
      "grad_norm": 0.11124473810195923,
      "learning_rate": 0.0004989188146482259,
      "loss": 2.3051,
      "step": 3460
    },
    {
      "epoch": 7.886363636363637,
      "grad_norm": 0.11023023724555969,
      "learning_rate": 0.00049891042322877,
      "loss": 2.2875,
      "step": 3470
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 0.10922950506210327,
      "learning_rate": 0.000498901999441968,
      "loss": 2.2773,
      "step": 3480
    },
    {
      "epoch": 7.931818181818182,
      "grad_norm": 0.2711382508277893,
      "learning_rate": 0.0004988935432889154,
      "loss": 2.2781,
      "step": 3490
    },
    {
      "epoch": 7.954545454545455,
      "grad_norm": 0.33293214440345764,
      "learning_rate": 0.0004988850547707119,
      "loss": 2.2985,
      "step": 3500
    },
    {
      "epoch": 7.9772727272727275,
      "grad_norm": 0.12175361067056656,
      "learning_rate": 0.0004988765338884611,
      "loss": 2.2911,
      "step": 3510
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.20803575217723846,
      "learning_rate": 0.0004988679806432712,
      "loss": 2.2855,
      "step": 3520
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.1227723360061646,
      "eval_runtime": 8.6973,
      "eval_samples_per_second": 3498.921,
      "eval_steps_per_second": 13.682,
      "step": 3520
    },
    {
      "epoch": 8.022727272727273,
      "grad_norm": 0.12341845780611038,
      "learning_rate": 0.0004988593950362543,
      "loss": 2.293,
      "step": 3530
    },
    {
      "epoch": 8.045454545454545,
      "grad_norm": 0.10752526670694351,
      "learning_rate": 0.0004988507770685268,
      "loss": 2.2884,
      "step": 3540
    },
    {
      "epoch": 8.068181818181818,
      "grad_norm": 0.09586146473884583,
      "learning_rate": 0.0004988421267412096,
      "loss": 2.289,
      "step": 3550
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 0.12868958711624146,
      "learning_rate": 0.0004988334440554274,
      "loss": 2.2787,
      "step": 3560
    },
    {
      "epoch": 8.113636363636363,
      "grad_norm": 0.09534168243408203,
      "learning_rate": 0.0004988247290123093,
      "loss": 2.2965,
      "step": 3570
    },
    {
      "epoch": 8.136363636363637,
      "grad_norm": 0.094718337059021,
      "learning_rate": 0.0004988159816129885,
      "loss": 2.3002,
      "step": 3580
    },
    {
      "epoch": 8.159090909090908,
      "grad_norm": 0.43347543478012085,
      "learning_rate": 0.0004988072018586024,
      "loss": 2.2613,
      "step": 3590
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.11181925237178802,
      "learning_rate": 0.000498798389750293,
      "loss": 2.2841,
      "step": 3600
    },
    {
      "epoch": 8.204545454545455,
      "grad_norm": 0.12924574315547943,
      "learning_rate": 0.0004987895452892058,
      "loss": 2.2776,
      "step": 3610
    },
    {
      "epoch": 8.227272727272727,
      "grad_norm": 0.2307356297969818,
      "learning_rate": 0.0004987806684764913,
      "loss": 2.2738,
      "step": 3620
    },
    {
      "epoch": 8.25,
      "grad_norm": 0.11675592511892319,
      "learning_rate": 0.0004987717593133034,
      "loss": 2.2755,
      "step": 3630
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 0.10205350071191788,
      "learning_rate": 0.0004987628178008009,
      "loss": 2.2931,
      "step": 3640
    },
    {
      "epoch": 8.295454545454545,
      "grad_norm": 0.1048198863863945,
      "learning_rate": 0.0004987538439401465,
      "loss": 2.2777,
      "step": 3650
    },
    {
      "epoch": 8.318181818181818,
      "grad_norm": 0.10695548355579376,
      "learning_rate": 0.0004987448377325069,
      "loss": 2.2809,
      "step": 3660
    },
    {
      "epoch": 8.340909090909092,
      "grad_norm": 0.1309029906988144,
      "learning_rate": 0.0004987357991790534,
      "loss": 2.2822,
      "step": 3670
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 0.10458826273679733,
      "learning_rate": 0.0004987267282809615,
      "loss": 2.281,
      "step": 3680
    },
    {
      "epoch": 8.386363636363637,
      "grad_norm": 0.12488137185573578,
      "learning_rate": 0.0004987176250394104,
      "loss": 2.2671,
      "step": 3690
    },
    {
      "epoch": 8.409090909090908,
      "grad_norm": 0.12432984262704849,
      "learning_rate": 0.0004987084894555841,
      "loss": 2.2728,
      "step": 3700
    },
    {
      "epoch": 8.431818181818182,
      "grad_norm": 0.10695606470108032,
      "learning_rate": 0.0004986993215306705,
      "loss": 2.2753,
      "step": 3710
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 0.5454839468002319,
      "learning_rate": 0.0004986901212658617,
      "loss": 2.285,
      "step": 3720
    },
    {
      "epoch": 8.477272727272727,
      "grad_norm": 0.10891155898571014,
      "learning_rate": 0.0004986808886623541,
      "loss": 2.2669,
      "step": 3730
    },
    {
      "epoch": 8.5,
      "grad_norm": 0.11894021928310394,
      "learning_rate": 0.0004986716237213483,
      "loss": 2.2836,
      "step": 3740
    },
    {
      "epoch": 8.522727272727273,
      "grad_norm": 0.10428285598754883,
      "learning_rate": 0.0004986623264440491,
      "loss": 2.2759,
      "step": 3750
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 0.10299704223871231,
      "learning_rate": 0.0004986529968316653,
      "loss": 2.2726,
      "step": 3760
    },
    {
      "epoch": 8.568181818181818,
      "grad_norm": 0.10131204128265381,
      "learning_rate": 0.0004986436348854104,
      "loss": 2.2837,
      "step": 3770
    },
    {
      "epoch": 8.590909090909092,
      "grad_norm": 0.1087932214140892,
      "learning_rate": 0.0004986342406065015,
      "loss": 2.2867,
      "step": 3780
    },
    {
      "epoch": 8.613636363636363,
      "grad_norm": 0.10908540338277817,
      "learning_rate": 0.0004986248139961603,
      "loss": 2.2975,
      "step": 3790
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.12514741718769073,
      "learning_rate": 0.0004986153550556127,
      "loss": 2.273,
      "step": 3800
    },
    {
      "epoch": 8.659090909090908,
      "grad_norm": 0.09946563094854355,
      "learning_rate": 0.0004986058637860885,
      "loss": 2.2692,
      "step": 3810
    },
    {
      "epoch": 8.681818181818182,
      "grad_norm": 0.09871824830770493,
      "learning_rate": 0.0004985963401888221,
      "loss": 2.2951,
      "step": 3820
    },
    {
      "epoch": 8.704545454545455,
      "grad_norm": 0.11321276426315308,
      "learning_rate": 0.0004985867842650519,
      "loss": 2.2682,
      "step": 3830
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 0.1061442419886589,
      "learning_rate": 0.0004985771960160203,
      "loss": 2.2772,
      "step": 3840
    },
    {
      "epoch": 8.75,
      "grad_norm": 0.10478998720645905,
      "learning_rate": 0.0004985675754429744,
      "loss": 2.2745,
      "step": 3850
    },
    {
      "epoch": 8.772727272727273,
      "grad_norm": 0.15356892347335815,
      "learning_rate": 0.000498557922547165,
      "loss": 2.2957,
      "step": 3860
    },
    {
      "epoch": 8.795454545454545,
      "grad_norm": 0.09630323946475983,
      "learning_rate": 0.0004985482373298474,
      "loss": 2.28,
      "step": 3870
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 0.10350599139928818,
      "learning_rate": 0.0004985385197922811,
      "loss": 2.2899,
      "step": 3880
    },
    {
      "epoch": 8.840909090909092,
      "grad_norm": 0.10786345601081848,
      "learning_rate": 0.0004985287699357297,
      "loss": 2.2969,
      "step": 3890
    },
    {
      "epoch": 8.863636363636363,
      "grad_norm": 0.09624218195676804,
      "learning_rate": 0.0004985189877614609,
      "loss": 2.2849,
      "step": 3900
    },
    {
      "epoch": 8.886363636363637,
      "grad_norm": 0.12994469702243805,
      "learning_rate": 0.000498509173270747,
      "loss": 2.2718,
      "step": 3910
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 0.10493968427181244,
      "learning_rate": 0.0004984993264648639,
      "loss": 2.2638,
      "step": 3920
    },
    {
      "epoch": 8.931818181818182,
      "grad_norm": 0.15237198770046234,
      "learning_rate": 0.0004984894473450924,
      "loss": 2.2776,
      "step": 3930
    },
    {
      "epoch": 8.954545454545455,
      "grad_norm": 0.13157108426094055,
      "learning_rate": 0.0004984795359127168,
      "loss": 2.2785,
      "step": 3940
    },
    {
      "epoch": 8.977272727272727,
      "grad_norm": 0.09788185358047485,
      "learning_rate": 0.0004984695921690262,
      "loss": 2.2737,
      "step": 3950
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.15510819852352142,
      "learning_rate": 0.0004984596161153135,
      "loss": 2.273,
      "step": 3960
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.1191484928131104,
      "eval_runtime": 8.6943,
      "eval_samples_per_second": 3500.098,
      "eval_steps_per_second": 13.687,
      "step": 3960
    },
    {
      "epoch": 9.022727272727273,
      "grad_norm": 0.10133318603038788,
      "learning_rate": 0.0004984496077528761,
      "loss": 2.2782,
      "step": 3970
    },
    {
      "epoch": 9.045454545454545,
      "grad_norm": 0.10238578170537949,
      "learning_rate": 0.0004984395670830152,
      "loss": 2.2973,
      "step": 3980
    },
    {
      "epoch": 9.068181818181818,
      "grad_norm": 0.1015762910246849,
      "learning_rate": 0.0004984294941070367,
      "loss": 2.2692,
      "step": 3990
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.10132406651973724,
      "learning_rate": 0.0004984193888262504,
      "loss": 2.285,
      "step": 4000
    },
    {
      "epoch": 9.113636363636363,
      "grad_norm": 0.11825099587440491,
      "learning_rate": 0.0004984092512419703,
      "loss": 2.2742,
      "step": 4010
    },
    {
      "epoch": 9.136363636363637,
      "grad_norm": 0.08617610484361649,
      "learning_rate": 0.0004983990813555146,
      "loss": 2.2727,
      "step": 4020
    },
    {
      "epoch": 9.159090909090908,
      "grad_norm": 0.08631104230880737,
      "learning_rate": 0.0004983888791682058,
      "loss": 2.2709,
      "step": 4030
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 0.0929797813296318,
      "learning_rate": 0.0004983786446813706,
      "loss": 2.268,
      "step": 4040
    },
    {
      "epoch": 9.204545454545455,
      "grad_norm": 0.12085703760385513,
      "learning_rate": 0.0004983683778963398,
      "loss": 2.2726,
      "step": 4050
    },
    {
      "epoch": 9.227272727272727,
      "grad_norm": 0.10786331444978714,
      "learning_rate": 0.0004983580788144484,
      "loss": 2.2742,
      "step": 4060
    },
    {
      "epoch": 9.25,
      "grad_norm": 0.09366658329963684,
      "learning_rate": 0.0004983477474370358,
      "loss": 2.2675,
      "step": 4070
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 0.5207132697105408,
      "learning_rate": 0.0004983373837654454,
      "loss": 2.2785,
      "step": 4080
    },
    {
      "epoch": 9.295454545454545,
      "grad_norm": 0.11242463439702988,
      "learning_rate": 0.0004983269878010247,
      "loss": 2.2829,
      "step": 4090
    },
    {
      "epoch": 9.318181818181818,
      "grad_norm": 0.14942477643489838,
      "learning_rate": 0.0004983165595451258,
      "loss": 2.2822,
      "step": 4100
    },
    {
      "epoch": 9.340909090909092,
      "grad_norm": 0.08933909982442856,
      "learning_rate": 0.0004983060989991045,
      "loss": 2.2679,
      "step": 4110
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 0.10663524270057678,
      "learning_rate": 0.0004982956061643213,
      "loss": 2.2825,
      "step": 4120
    },
    {
      "epoch": 9.386363636363637,
      "grad_norm": 0.11292167007923126,
      "learning_rate": 0.0004982850810421405,
      "loss": 2.2846,
      "step": 4130
    },
    {
      "epoch": 9.409090909090908,
      "grad_norm": 0.0937899500131607,
      "learning_rate": 0.0004982745236339306,
      "loss": 2.2692,
      "step": 4140
    },
    {
      "epoch": 9.431818181818182,
      "grad_norm": 0.08667515218257904,
      "learning_rate": 0.0004982639339410648,
      "loss": 2.2689,
      "step": 4150
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 0.09374863654375076,
      "learning_rate": 0.0004982533119649199,
      "loss": 2.2753,
      "step": 4160
    },
    {
      "epoch": 9.477272727272727,
      "grad_norm": 0.13905112445354462,
      "learning_rate": 0.0004982426577068771,
      "loss": 2.2657,
      "step": 4170
    },
    {
      "epoch": 9.5,
      "grad_norm": 0.08789791166782379,
      "learning_rate": 0.0004982319711683221,
      "loss": 2.2802,
      "step": 4180
    },
    {
      "epoch": 9.522727272727273,
      "grad_norm": 0.0881257951259613,
      "learning_rate": 0.0004982212523506443,
      "loss": 2.2734,
      "step": 4190
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 0.12278193235397339,
      "learning_rate": 0.0004982105012552375,
      "loss": 2.287,
      "step": 4200
    },
    {
      "epoch": 9.568181818181818,
      "grad_norm": 0.12457283586263657,
      "learning_rate": 0.0004981997178834999,
      "loss": 2.2769,
      "step": 4210
    },
    {
      "epoch": 9.590909090909092,
      "grad_norm": 0.13783229887485504,
      "learning_rate": 0.0004981889022368337,
      "loss": 2.2826,
      "step": 4220
    },
    {
      "epoch": 9.613636363636363,
      "grad_norm": 0.10920265316963196,
      "learning_rate": 0.0004981780543166453,
      "loss": 2.2801,
      "step": 4230
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 0.10252094268798828,
      "learning_rate": 0.0004981671741243454,
      "loss": 2.2689,
      "step": 4240
    },
    {
      "epoch": 9.659090909090908,
      "grad_norm": 0.09416281431913376,
      "learning_rate": 0.0004981562616613484,
      "loss": 2.2703,
      "step": 4250
    },
    {
      "epoch": 9.681818181818182,
      "grad_norm": 0.09136011451482773,
      "learning_rate": 0.000498145316929074,
      "loss": 2.2638,
      "step": 4260
    },
    {
      "epoch": 9.704545454545455,
      "grad_norm": 0.0932149887084961,
      "learning_rate": 0.0004981343399289447,
      "loss": 2.2729,
      "step": 4270
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 0.08931416273117065,
      "learning_rate": 0.0004981233306623885,
      "loss": 2.2684,
      "step": 4280
    },
    {
      "epoch": 9.75,
      "grad_norm": 0.10132787376642227,
      "learning_rate": 0.0004981122891308368,
      "loss": 2.2731,
      "step": 4290
    },
    {
      "epoch": 9.772727272727273,
      "grad_norm": 0.09909166395664215,
      "learning_rate": 0.0004981012153357252,
      "loss": 2.2707,
      "step": 4300
    },
    {
      "epoch": 9.795454545454545,
      "grad_norm": 0.09654399007558823,
      "learning_rate": 0.0004980901092784939,
      "loss": 2.2697,
      "step": 4310
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 0.12885449826717377,
      "learning_rate": 0.000498078970960587,
      "loss": 2.2821,
      "step": 4320
    },
    {
      "epoch": 9.840909090909092,
      "grad_norm": 0.1669842004776001,
      "learning_rate": 0.000498067800383453,
      "loss": 2.2947,
      "step": 4330
    },
    {
      "epoch": 9.863636363636363,
      "grad_norm": 0.12361236661672592,
      "learning_rate": 0.0004980565975485444,
      "loss": 2.2734,
      "step": 4340
    },
    {
      "epoch": 9.886363636363637,
      "grad_norm": 0.2426585853099823,
      "learning_rate": 0.0004980453624573179,
      "loss": 2.2677,
      "step": 4350
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 0.10383862257003784,
      "learning_rate": 0.0004980340951112345,
      "loss": 2.2678,
      "step": 4360
    },
    {
      "epoch": 9.931818181818182,
      "grad_norm": 0.09067923575639725,
      "learning_rate": 0.0004980227955117595,
      "loss": 2.2629,
      "step": 4370
    },
    {
      "epoch": 9.954545454545455,
      "grad_norm": 0.09855160862207413,
      "learning_rate": 0.0004980114636603622,
      "loss": 2.2582,
      "step": 4380
    },
    {
      "epoch": 9.977272727272727,
      "grad_norm": 0.0981409028172493,
      "learning_rate": 0.000498000099558516,
      "loss": 2.2598,
      "step": 4390
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.16658052802085876,
      "learning_rate": 0.0004979887032076989,
      "loss": 2.2721,
      "step": 4400
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.1173568964004517,
      "eval_runtime": 8.7068,
      "eval_samples_per_second": 3495.097,
      "eval_steps_per_second": 13.668,
      "step": 4400
    },
    {
      "epoch": 10.022727272727273,
      "grad_norm": 0.10034075379371643,
      "learning_rate": 0.0004979772746093926,
      "loss": 2.2716,
      "step": 4410
    },
    {
      "epoch": 10.045454545454545,
      "grad_norm": 0.09099997580051422,
      "learning_rate": 0.0004979658137650832,
      "loss": 2.2653,
      "step": 4420
    },
    {
      "epoch": 10.068181818181818,
      "grad_norm": 0.08928009867668152,
      "learning_rate": 0.0004979543206762614,
      "loss": 2.2539,
      "step": 4430
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 0.11197104305028915,
      "learning_rate": 0.0004979427953444212,
      "loss": 2.2741,
      "step": 4440
    },
    {
      "epoch": 10.113636363636363,
      "grad_norm": 0.08986672759056091,
      "learning_rate": 0.0004979312377710618,
      "loss": 2.2618,
      "step": 4450
    },
    {
      "epoch": 10.136363636363637,
      "grad_norm": 0.09650164097547531,
      "learning_rate": 0.0004979196479576856,
      "loss": 2.2742,
      "step": 4460
    },
    {
      "epoch": 10.159090909090908,
      "grad_norm": 0.08929415047168732,
      "learning_rate": 0.0004979080259058002,
      "loss": 2.2721,
      "step": 4470
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 0.09273563325405121,
      "learning_rate": 0.0004978963716169166,
      "loss": 2.2692,
      "step": 4480
    },
    {
      "epoch": 10.204545454545455,
      "grad_norm": 0.11646749079227448,
      "learning_rate": 0.0004978846850925503,
      "loss": 2.2887,
      "step": 4490
    },
    {
      "epoch": 10.227272727272727,
      "grad_norm": 0.1146550104022026,
      "learning_rate": 0.000497872966334221,
      "loss": 2.2582,
      "step": 4500
    },
    {
      "epoch": 10.25,
      "grad_norm": 0.09397290647029877,
      "learning_rate": 0.0004978612153434526,
      "loss": 2.2824,
      "step": 4510
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 0.09054341167211533,
      "learning_rate": 0.0004978494321217731,
      "loss": 2.2788,
      "step": 4520
    },
    {
      "epoch": 10.295454545454545,
      "grad_norm": 0.22924399375915527,
      "learning_rate": 0.0004978376166707148,
      "loss": 2.2585,
      "step": 4530
    },
    {
      "epoch": 10.318181818181818,
      "grad_norm": 0.08805672079324722,
      "learning_rate": 0.000497825768991814,
      "loss": 2.2713,
      "step": 4540
    },
    {
      "epoch": 10.340909090909092,
      "grad_norm": 0.10721690207719803,
      "learning_rate": 0.0004978138890866116,
      "loss": 2.2695,
      "step": 4550
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 0.13601256906986237,
      "learning_rate": 0.000497801976956652,
      "loss": 2.2768,
      "step": 4560
    },
    {
      "epoch": 10.386363636363637,
      "grad_norm": 0.09382213652133942,
      "learning_rate": 0.0004977900326034847,
      "loss": 2.2748,
      "step": 4570
    },
    {
      "epoch": 10.409090909090908,
      "grad_norm": 0.10211103409528732,
      "learning_rate": 0.0004977780560286624,
      "loss": 2.2514,
      "step": 4580
    },
    {
      "epoch": 10.431818181818182,
      "grad_norm": 0.08933422714471817,
      "learning_rate": 0.0004977660472337428,
      "loss": 2.2815,
      "step": 4590
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 0.09612297266721725,
      "learning_rate": 0.0004977540062202875,
      "loss": 2.2689,
      "step": 4600
    },
    {
      "epoch": 10.477272727272727,
      "grad_norm": 0.0983891710639,
      "learning_rate": 0.0004977419329898622,
      "loss": 2.2598,
      "step": 4610
    },
    {
      "epoch": 10.5,
      "grad_norm": 0.10382875055074692,
      "learning_rate": 0.0004977298275440367,
      "loss": 2.2526,
      "step": 4620
    },
    {
      "epoch": 10.522727272727273,
      "grad_norm": 0.212294802069664,
      "learning_rate": 0.0004977176898843854,
      "loss": 2.2677,
      "step": 4630
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 0.12288377434015274,
      "learning_rate": 0.0004977055200124865,
      "loss": 2.2604,
      "step": 4640
    },
    {
      "epoch": 10.568181818181818,
      "grad_norm": 0.08917989581823349,
      "learning_rate": 0.0004976933179299224,
      "loss": 2.2736,
      "step": 4650
    },
    {
      "epoch": 10.590909090909092,
      "grad_norm": 0.09296339750289917,
      "learning_rate": 0.0004976810836382801,
      "loss": 2.268,
      "step": 4660
    },
    {
      "epoch": 10.613636363636363,
      "grad_norm": 0.09020394831895828,
      "learning_rate": 0.0004976688171391503,
      "loss": 2.2597,
      "step": 4670
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 0.07998944818973541,
      "learning_rate": 0.0004976565184341281,
      "loss": 2.278,
      "step": 4680
    },
    {
      "epoch": 10.659090909090908,
      "grad_norm": 0.09105410426855087,
      "learning_rate": 0.0004976441875248128,
      "loss": 2.2706,
      "step": 4690
    },
    {
      "epoch": 10.681818181818182,
      "grad_norm": 0.09186649322509766,
      "learning_rate": 0.000497631824412808,
      "loss": 2.2767,
      "step": 4700
    },
    {
      "epoch": 10.704545454545455,
      "grad_norm": 0.258043497800827,
      "learning_rate": 0.0004976194290997211,
      "loss": 2.2819,
      "step": 4710
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 0.1632080078125,
      "learning_rate": 0.0004976070015871641,
      "loss": 2.259,
      "step": 4720
    },
    {
      "epoch": 10.75,
      "grad_norm": 0.09440639615058899,
      "learning_rate": 0.0004975945418767529,
      "loss": 2.2743,
      "step": 4730
    },
    {
      "epoch": 10.772727272727273,
      "grad_norm": 0.09266960620880127,
      "learning_rate": 0.0004975820499701079,
      "loss": 2.2641,
      "step": 4740
    },
    {
      "epoch": 10.795454545454545,
      "grad_norm": 0.0964404046535492,
      "learning_rate": 0.0004975695258688534,
      "loss": 2.2649,
      "step": 4750
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 0.08219455182552338,
      "learning_rate": 0.000497556969574618,
      "loss": 2.2593,
      "step": 4760
    },
    {
      "epoch": 10.840909090909092,
      "grad_norm": 0.08587652444839478,
      "learning_rate": 0.0004975443810890344,
      "loss": 2.2572,
      "step": 4770
    },
    {
      "epoch": 10.863636363636363,
      "grad_norm": 0.09229261428117752,
      "learning_rate": 0.0004975317604137396,
      "loss": 2.2849,
      "step": 4780
    },
    {
      "epoch": 10.886363636363637,
      "grad_norm": 0.22288034856319427,
      "learning_rate": 0.0004975191075503748,
      "loss": 2.2711,
      "step": 4790
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.10862069576978683,
      "learning_rate": 0.0004975064225005853,
      "loss": 2.2676,
      "step": 4800
    },
    {
      "epoch": 10.931818181818182,
      "grad_norm": 0.08629241585731506,
      "learning_rate": 0.0004974937052660206,
      "loss": 2.2604,
      "step": 4810
    },
    {
      "epoch": 10.954545454545455,
      "grad_norm": 0.1132945641875267,
      "learning_rate": 0.0004974809558483344,
      "loss": 2.2581,
      "step": 4820
    },
    {
      "epoch": 10.977272727272727,
      "grad_norm": 0.08274530619382858,
      "learning_rate": 0.0004974681742491846,
      "loss": 2.2652,
      "step": 4830
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.17019808292388916,
      "learning_rate": 0.0004974553604702333,
      "loss": 2.2713,
      "step": 4840
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.1148062944412231,
      "eval_runtime": 8.7326,
      "eval_samples_per_second": 3484.766,
      "eval_steps_per_second": 13.627,
      "step": 4840
    },
    {
      "epoch": 11.022727272727273,
      "grad_norm": 0.09628096967935562,
      "learning_rate": 0.0004974425145131466,
      "loss": 2.2737,
      "step": 4850
    },
    {
      "epoch": 11.045454545454545,
      "grad_norm": 0.1025407612323761,
      "learning_rate": 0.000497429636379595,
      "loss": 2.2613,
      "step": 4860
    },
    {
      "epoch": 11.068181818181818,
      "grad_norm": 0.1856035590171814,
      "learning_rate": 0.0004974167260712533,
      "loss": 2.266,
      "step": 4870
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 0.23788370192050934,
      "learning_rate": 0.0004974037835898001,
      "loss": 2.2494,
      "step": 4880
    },
    {
      "epoch": 11.113636363636363,
      "grad_norm": 0.10315649211406708,
      "learning_rate": 0.0004973908089369185,
      "loss": 2.271,
      "step": 4890
    },
    {
      "epoch": 11.136363636363637,
      "grad_norm": 0.10621805489063263,
      "learning_rate": 0.0004973778021142957,
      "loss": 2.2623,
      "step": 4900
    },
    {
      "epoch": 11.159090909090908,
      "grad_norm": 0.09716617316007614,
      "learning_rate": 0.000497364763123623,
      "loss": 2.2697,
      "step": 4910
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 0.09762640297412872,
      "learning_rate": 0.0004973516919665959,
      "loss": 2.2609,
      "step": 4920
    },
    {
      "epoch": 11.204545454545455,
      "grad_norm": 0.1266140192747116,
      "learning_rate": 0.0004973385886449141,
      "loss": 2.2708,
      "step": 4930
    },
    {
      "epoch": 11.227272727272727,
      "grad_norm": 0.15640434622764587,
      "learning_rate": 0.0004973254531602816,
      "loss": 2.2665,
      "step": 4940
    },
    {
      "epoch": 11.25,
      "grad_norm": 0.09204446524381638,
      "learning_rate": 0.0004973122855144066,
      "loss": 2.2649,
      "step": 4950
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 0.08874441683292389,
      "learning_rate": 0.0004972990857090011,
      "loss": 2.2607,
      "step": 4960
    },
    {
      "epoch": 11.295454545454545,
      "grad_norm": 0.09746167063713074,
      "learning_rate": 0.0004972858537457816,
      "loss": 2.2599,
      "step": 4970
    },
    {
      "epoch": 11.318181818181818,
      "grad_norm": 0.09071342647075653,
      "learning_rate": 0.000497272589626469,
      "loss": 2.262,
      "step": 4980
    },
    {
      "epoch": 11.340909090909092,
      "grad_norm": 0.10048530995845795,
      "learning_rate": 0.0004972592933527877,
      "loss": 2.2654,
      "step": 4990
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 0.08415596932172775,
      "learning_rate": 0.000497245964926467,
      "loss": 2.2488,
      "step": 5000
    },
    {
      "epoch": 11.386363636363637,
      "grad_norm": 0.11282172054052353,
      "learning_rate": 0.00049723260434924,
      "loss": 2.2734,
      "step": 5010
    },
    {
      "epoch": 11.409090909090908,
      "grad_norm": 0.09483831375837326,
      "learning_rate": 0.0004972192116228442,
      "loss": 2.2628,
      "step": 5020
    },
    {
      "epoch": 11.431818181818182,
      "grad_norm": 0.08938722312450409,
      "learning_rate": 0.0004972057867490209,
      "loss": 2.2571,
      "step": 5030
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 0.08743973821401596,
      "learning_rate": 0.0004971923297295158,
      "loss": 2.2571,
      "step": 5040
    },
    {
      "epoch": 11.477272727272727,
      "grad_norm": 0.08512599021196365,
      "learning_rate": 0.000497178840566079,
      "loss": 2.2599,
      "step": 5050
    },
    {
      "epoch": 11.5,
      "grad_norm": 0.09387446194887161,
      "learning_rate": 0.0004971653192604645,
      "loss": 2.2633,
      "step": 5060
    },
    {
      "epoch": 11.522727272727273,
      "grad_norm": 0.07888554036617279,
      "learning_rate": 0.0004971517658144305,
      "loss": 2.2575,
      "step": 5070
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 0.9856100082397461,
      "learning_rate": 0.0004971381802297395,
      "loss": 2.285,
      "step": 5080
    },
    {
      "epoch": 11.568181818181818,
      "grad_norm": 0.08162523806095123,
      "learning_rate": 0.000497124562508158,
      "loss": 2.254,
      "step": 5090
    },
    {
      "epoch": 11.590909090909092,
      "grad_norm": 0.09103703498840332,
      "learning_rate": 0.0004971109126514571,
      "loss": 2.263,
      "step": 5100
    },
    {
      "epoch": 11.613636363636363,
      "grad_norm": 0.08712690323591232,
      "learning_rate": 0.0004970972306614115,
      "loss": 2.259,
      "step": 5110
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 0.08709193021059036,
      "learning_rate": 0.0004970835165398004,
      "loss": 2.267,
      "step": 5120
    },
    {
      "epoch": 11.659090909090908,
      "grad_norm": 0.09065112471580505,
      "learning_rate": 0.000497069770288407,
      "loss": 2.2699,
      "step": 5130
    },
    {
      "epoch": 11.681818181818182,
      "grad_norm": 0.09417292475700378,
      "learning_rate": 0.0004970559919090192,
      "loss": 2.2625,
      "step": 5140
    },
    {
      "epoch": 11.704545454545455,
      "grad_norm": 0.14558179676532745,
      "learning_rate": 0.0004970421814034283,
      "loss": 2.2559,
      "step": 5150
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 0.1262933611869812,
      "learning_rate": 0.0004970283387734304,
      "loss": 2.2732,
      "step": 5160
    },
    {
      "epoch": 11.75,
      "grad_norm": 0.18097737431526184,
      "learning_rate": 0.0004970144640208254,
      "loss": 2.2643,
      "step": 5170
    },
    {
      "epoch": 11.772727272727273,
      "grad_norm": 0.09104053676128387,
      "learning_rate": 0.0004970005571474175,
      "loss": 2.2671,
      "step": 5180
    },
    {
      "epoch": 11.795454545454545,
      "grad_norm": 0.08503284305334091,
      "learning_rate": 0.0004969866181550153,
      "loss": 2.2585,
      "step": 5190
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 0.1432395577430725,
      "learning_rate": 0.0004969726470454314,
      "loss": 2.2629,
      "step": 5200
    },
    {
      "epoch": 11.840909090909092,
      "grad_norm": 0.12356263399124146,
      "learning_rate": 0.0004969586438204821,
      "loss": 2.273,
      "step": 5210
    },
    {
      "epoch": 11.863636363636363,
      "grad_norm": 0.09984166920185089,
      "learning_rate": 0.0004969446084819888,
      "loss": 2.2761,
      "step": 5220
    },
    {
      "epoch": 11.886363636363637,
      "grad_norm": 0.0932244136929512,
      "learning_rate": 0.0004969305410317763,
      "loss": 2.2704,
      "step": 5230
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 0.10999913513660431,
      "learning_rate": 0.000496916441471674,
      "loss": 2.2659,
      "step": 5240
    },
    {
      "epoch": 11.931818181818182,
      "grad_norm": 0.6005086302757263,
      "learning_rate": 0.0004969023098035153,
      "loss": 2.2594,
      "step": 5250
    },
    {
      "epoch": 11.954545454545455,
      "grad_norm": 0.0996105968952179,
      "learning_rate": 0.000496888146029138,
      "loss": 2.2718,
      "step": 5260
    },
    {
      "epoch": 11.977272727272727,
      "grad_norm": 0.0916455090045929,
      "learning_rate": 0.0004968739501503838,
      "loss": 2.2756,
      "step": 5270
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.17277009785175323,
      "learning_rate": 0.0004968597221690986,
      "loss": 2.2524,
      "step": 5280
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.1138924360275269,
      "eval_runtime": 8.759,
      "eval_samples_per_second": 3474.272,
      "eval_steps_per_second": 13.586,
      "step": 5280
    },
    {
      "epoch": 12.022727272727273,
      "grad_norm": 0.1013844832777977,
      "learning_rate": 0.0004968454620871327,
      "loss": 2.2372,
      "step": 5290
    },
    {
      "epoch": 12.045454545454545,
      "grad_norm": 0.10664695501327515,
      "learning_rate": 0.0004968311699063402,
      "loss": 2.2522,
      "step": 5300
    },
    {
      "epoch": 12.068181818181818,
      "grad_norm": 0.09370666742324829,
      "learning_rate": 0.0004968168456285796,
      "loss": 2.2558,
      "step": 5310
    },
    {
      "epoch": 12.090909090909092,
      "grad_norm": 0.1275910884141922,
      "learning_rate": 0.0004968024892557139,
      "loss": 2.2584,
      "step": 5320
    },
    {
      "epoch": 12.113636363636363,
      "grad_norm": 0.11913526058197021,
      "learning_rate": 0.0004967881007896098,
      "loss": 2.2675,
      "step": 5330
    },
    {
      "epoch": 12.136363636363637,
      "grad_norm": 0.10456987470388412,
      "learning_rate": 0.0004967736802321382,
      "loss": 2.2551,
      "step": 5340
    },
    {
      "epoch": 12.159090909090908,
      "grad_norm": 0.11643283069133759,
      "learning_rate": 0.0004967592275851744,
      "loss": 2.2607,
      "step": 5350
    },
    {
      "epoch": 12.181818181818182,
      "grad_norm": 0.14141462743282318,
      "learning_rate": 0.0004967447428505976,
      "loss": 2.2578,
      "step": 5360
    },
    {
      "epoch": 12.204545454545455,
      "grad_norm": 0.13844546675682068,
      "learning_rate": 0.0004967302260302916,
      "loss": 2.2558,
      "step": 5370
    },
    {
      "epoch": 12.227272727272727,
      "grad_norm": 0.10357227176427841,
      "learning_rate": 0.0004967156771261439,
      "loss": 2.2741,
      "step": 5380
    },
    {
      "epoch": 12.25,
      "grad_norm": 0.09951594471931458,
      "learning_rate": 0.0004967010961400466,
      "loss": 2.2432,
      "step": 5390
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 0.09347513318061829,
      "learning_rate": 0.0004966864830738954,
      "loss": 2.2692,
      "step": 5400
    },
    {
      "epoch": 12.295454545454545,
      "grad_norm": 0.19077147543430328,
      "learning_rate": 0.0004966718379295909,
      "loss": 2.2661,
      "step": 5410
    },
    {
      "epoch": 12.318181818181818,
      "grad_norm": 0.13282059133052826,
      "learning_rate": 0.0004966571607090373,
      "loss": 2.278,
      "step": 5420
    },
    {
      "epoch": 12.340909090909092,
      "grad_norm": 0.10670916736125946,
      "learning_rate": 0.0004966424514141432,
      "loss": 2.2627,
      "step": 5430
    },
    {
      "epoch": 12.363636363636363,
      "grad_norm": 0.10024088621139526,
      "learning_rate": 0.0004966277100468214,
      "loss": 2.2557,
      "step": 5440
    },
    {
      "epoch": 12.386363636363637,
      "grad_norm": 0.11647235602140427,
      "learning_rate": 0.0004966129366089887,
      "loss": 2.2608,
      "step": 5450
    },
    {
      "epoch": 12.409090909090908,
      "grad_norm": 0.1625855565071106,
      "learning_rate": 0.0004965981311025663,
      "loss": 2.2542,
      "step": 5460
    },
    {
      "epoch": 12.431818181818182,
      "grad_norm": 0.08943009376525879,
      "learning_rate": 0.0004965832935294794,
      "loss": 2.2599,
      "step": 5470
    },
    {
      "epoch": 12.454545454545455,
      "grad_norm": 0.0995447188615799,
      "learning_rate": 0.0004965684238916574,
      "loss": 2.2565,
      "step": 5480
    },
    {
      "epoch": 12.477272727272727,
      "grad_norm": 0.09412317723035812,
      "learning_rate": 0.0004965535221910338,
      "loss": 2.2591,
      "step": 5490
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.09121190756559372,
      "learning_rate": 0.0004965385884295467,
      "loss": 2.2694,
      "step": 5500
    },
    {
      "epoch": 12.522727272727273,
      "grad_norm": 0.14127828180789948,
      "learning_rate": 0.0004965236226091377,
      "loss": 2.2591,
      "step": 5510
    },
    {
      "epoch": 12.545454545454545,
      "grad_norm": 0.09955237060785294,
      "learning_rate": 0.0004965086247317529,
      "loss": 2.2548,
      "step": 5520
    },
    {
      "epoch": 12.568181818181818,
      "grad_norm": 0.08215644955635071,
      "learning_rate": 0.0004964935947993427,
      "loss": 2.266,
      "step": 5530
    },
    {
      "epoch": 12.590909090909092,
      "grad_norm": 0.09892633557319641,
      "learning_rate": 0.0004964785328138615,
      "loss": 2.258,
      "step": 5540
    },
    {
      "epoch": 12.613636363636363,
      "grad_norm": 0.07729611545801163,
      "learning_rate": 0.0004964634387772677,
      "loss": 2.2636,
      "step": 5550
    },
    {
      "epoch": 12.636363636363637,
      "grad_norm": 0.0785832330584526,
      "learning_rate": 0.0004964483126915245,
      "loss": 2.2616,
      "step": 5560
    },
    {
      "epoch": 12.659090909090908,
      "grad_norm": 0.08636747300624847,
      "learning_rate": 0.0004964331545585986,
      "loss": 2.2569,
      "step": 5570
    },
    {
      "epoch": 12.681818181818182,
      "grad_norm": 0.09138568490743637,
      "learning_rate": 0.000496417964380461,
      "loss": 2.267,
      "step": 5580
    },
    {
      "epoch": 12.704545454545455,
      "grad_norm": 0.09779073297977448,
      "learning_rate": 0.0004964027421590871,
      "loss": 2.2755,
      "step": 5590
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 0.09430994093418121,
      "learning_rate": 0.0004963874878964562,
      "loss": 2.2761,
      "step": 5600
    },
    {
      "epoch": 12.75,
      "grad_norm": 0.091185063123703,
      "learning_rate": 0.0004963722015945522,
      "loss": 2.2519,
      "step": 5610
    },
    {
      "epoch": 12.772727272727273,
      "grad_norm": 0.08984274417161942,
      "learning_rate": 0.0004963568832553626,
      "loss": 2.2653,
      "step": 5620
    },
    {
      "epoch": 12.795454545454545,
      "grad_norm": 0.09978772699832916,
      "learning_rate": 0.0004963415328808796,
      "loss": 2.2608,
      "step": 5630
    },
    {
      "epoch": 12.818181818181818,
      "grad_norm": 0.1081877127289772,
      "learning_rate": 0.0004963261504730989,
      "loss": 2.2541,
      "step": 5640
    },
    {
      "epoch": 12.840909090909092,
      "grad_norm": 0.16486383974552155,
      "learning_rate": 0.0004963107360340211,
      "loss": 2.2648,
      "step": 5650
    },
    {
      "epoch": 12.863636363636363,
      "grad_norm": 0.08719600737094879,
      "learning_rate": 0.0004962952895656505,
      "loss": 2.2514,
      "step": 5660
    },
    {
      "epoch": 12.886363636363637,
      "grad_norm": 0.10152066498994827,
      "learning_rate": 0.0004962798110699957,
      "loss": 2.2574,
      "step": 5670
    },
    {
      "epoch": 12.909090909090908,
      "grad_norm": 0.09401123970746994,
      "learning_rate": 0.0004962643005490696,
      "loss": 2.2565,
      "step": 5680
    },
    {
      "epoch": 12.931818181818182,
      "grad_norm": 0.07617134600877762,
      "learning_rate": 0.000496248758004889,
      "loss": 2.2594,
      "step": 5690
    },
    {
      "epoch": 12.954545454545455,
      "grad_norm": 0.07104147225618362,
      "learning_rate": 0.000496233183439475,
      "loss": 2.2489,
      "step": 5700
    },
    {
      "epoch": 12.977272727272727,
      "grad_norm": 0.09104233235120773,
      "learning_rate": 0.0004962175768548528,
      "loss": 2.258,
      "step": 5710
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.17514097690582275,
      "learning_rate": 0.000496201938253052,
      "loss": 2.2596,
      "step": 5720
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.1121432781219482,
      "eval_runtime": 8.9077,
      "eval_samples_per_second": 3416.255,
      "eval_steps_per_second": 13.359,
      "step": 5720
    },
    {
      "epoch": 13.022727272727273,
      "grad_norm": 0.10224523395299911,
      "learning_rate": 0.0004961862676361061,
      "loss": 2.2565,
      "step": 5730
    },
    {
      "epoch": 13.045454545454545,
      "grad_norm": 0.08569962531328201,
      "learning_rate": 0.0004961705650060528,
      "loss": 2.2684,
      "step": 5740
    },
    {
      "epoch": 13.068181818181818,
      "grad_norm": 0.11786065995693207,
      "learning_rate": 0.0004961548303649339,
      "loss": 2.2576,
      "step": 5750
    },
    {
      "epoch": 13.090909090909092,
      "grad_norm": 0.0831814706325531,
      "learning_rate": 0.0004961390637147956,
      "loss": 2.2574,
      "step": 5760
    },
    {
      "epoch": 13.113636363636363,
      "grad_norm": 0.0827740877866745,
      "learning_rate": 0.0004961232650576883,
      "loss": 2.2493,
      "step": 5770
    },
    {
      "epoch": 13.136363636363637,
      "grad_norm": 0.10797405987977982,
      "learning_rate": 0.0004961074343956661,
      "loss": 2.2497,
      "step": 5780
    },
    {
      "epoch": 13.159090909090908,
      "grad_norm": 0.08952908962965012,
      "learning_rate": 0.0004960915717307878,
      "loss": 2.2637,
      "step": 5790
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 0.07735145092010498,
      "learning_rate": 0.0004960756770651159,
      "loss": 2.2564,
      "step": 5800
    },
    {
      "epoch": 13.204545454545455,
      "grad_norm": 0.08405385911464691,
      "learning_rate": 0.0004960597504007175,
      "loss": 2.2533,
      "step": 5810
    },
    {
      "epoch": 13.227272727272727,
      "grad_norm": 0.08743879199028015,
      "learning_rate": 0.0004960437917396635,
      "loss": 2.2541,
      "step": 5820
    },
    {
      "epoch": 13.25,
      "grad_norm": 0.0797770544886589,
      "learning_rate": 0.000496027801084029,
      "loss": 2.2593,
      "step": 5830
    },
    {
      "epoch": 13.272727272727273,
      "grad_norm": 0.08933153748512268,
      "learning_rate": 0.0004960117784358936,
      "loss": 2.2637,
      "step": 5840
    },
    {
      "epoch": 13.295454545454545,
      "grad_norm": 0.08853594213724136,
      "learning_rate": 0.0004959957237973408,
      "loss": 2.2634,
      "step": 5850
    },
    {
      "epoch": 13.318181818181818,
      "grad_norm": 0.08960717916488647,
      "learning_rate": 0.0004959796371704581,
      "loss": 2.2649,
      "step": 5860
    },
    {
      "epoch": 13.340909090909092,
      "grad_norm": 0.11458049714565277,
      "learning_rate": 0.0004959635185573375,
      "loss": 2.2588,
      "step": 5870
    },
    {
      "epoch": 13.363636363636363,
      "grad_norm": 0.08743660897016525,
      "learning_rate": 0.0004959473679600749,
      "loss": 2.2507,
      "step": 5880
    },
    {
      "epoch": 13.386363636363637,
      "grad_norm": 0.07988061010837555,
      "learning_rate": 0.0004959311853807705,
      "loss": 2.2526,
      "step": 5890
    },
    {
      "epoch": 13.409090909090908,
      "grad_norm": 0.08368636667728424,
      "learning_rate": 0.0004959149708215286,
      "loss": 2.2467,
      "step": 5900
    },
    {
      "epoch": 13.431818181818182,
      "grad_norm": 0.08118198066949844,
      "learning_rate": 0.0004958987242844579,
      "loss": 2.2437,
      "step": 5910
    },
    {
      "epoch": 13.454545454545455,
      "grad_norm": 0.13418348133563995,
      "learning_rate": 0.0004958824457716707,
      "loss": 2.2582,
      "step": 5920
    },
    {
      "epoch": 13.477272727272727,
      "grad_norm": 0.08139874786138535,
      "learning_rate": 0.0004958661352852838,
      "loss": 2.2531,
      "step": 5930
    },
    {
      "epoch": 13.5,
      "grad_norm": 0.08207660913467407,
      "learning_rate": 0.0004958497928274184,
      "loss": 2.2548,
      "step": 5940
    },
    {
      "epoch": 13.522727272727273,
      "grad_norm": 0.07803849875926971,
      "learning_rate": 0.0004958334184001997,
      "loss": 2.2657,
      "step": 5950
    },
    {
      "epoch": 13.545454545454545,
      "grad_norm": 0.08198416978120804,
      "learning_rate": 0.0004958170120057565,
      "loss": 2.2539,
      "step": 5960
    },
    {
      "epoch": 13.568181818181818,
      "grad_norm": 0.1014789491891861,
      "learning_rate": 0.0004958005736462226,
      "loss": 2.2583,
      "step": 5970
    },
    {
      "epoch": 13.590909090909092,
      "grad_norm": 0.08587442338466644,
      "learning_rate": 0.0004957841033237355,
      "loss": 2.2564,
      "step": 5980
    },
    {
      "epoch": 13.613636363636363,
      "grad_norm": 0.09128143638372421,
      "learning_rate": 0.0004957676010404369,
      "loss": 2.2547,
      "step": 5990
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 0.07640038430690765,
      "learning_rate": 0.0004957510667984726,
      "loss": 2.2641,
      "step": 6000
    },
    {
      "epoch": 13.659090909090908,
      "grad_norm": 0.08646079152822495,
      "learning_rate": 0.0004957345005999928,
      "loss": 2.2627,
      "step": 6010
    },
    {
      "epoch": 13.681818181818182,
      "grad_norm": 0.10029152780771255,
      "learning_rate": 0.0004957179024471517,
      "loss": 2.2454,
      "step": 6020
    },
    {
      "epoch": 13.704545454545455,
      "grad_norm": 0.07473566383123398,
      "learning_rate": 0.0004957012723421076,
      "loss": 2.2521,
      "step": 6030
    },
    {
      "epoch": 13.727272727272727,
      "grad_norm": 0.08611131459474564,
      "learning_rate": 0.0004956846102870231,
      "loss": 2.2596,
      "step": 6040
    },
    {
      "epoch": 13.75,
      "grad_norm": 0.08145961165428162,
      "learning_rate": 0.0004956679162840646,
      "loss": 2.2522,
      "step": 6050
    },
    {
      "epoch": 13.772727272727273,
      "grad_norm": 0.09758716076612473,
      "learning_rate": 0.0004956511903354033,
      "loss": 2.2406,
      "step": 6060
    },
    {
      "epoch": 13.795454545454545,
      "grad_norm": 0.12818704545497894,
      "learning_rate": 0.000495634432443214,
      "loss": 2.2649,
      "step": 6070
    },
    {
      "epoch": 13.818181818181818,
      "grad_norm": 0.08853234350681305,
      "learning_rate": 0.0004956176426096757,
      "loss": 2.2534,
      "step": 6080
    },
    {
      "epoch": 13.840909090909092,
      "grad_norm": 0.09735623747110367,
      "learning_rate": 0.0004956008208369719,
      "loss": 2.2515,
      "step": 6090
    },
    {
      "epoch": 13.863636363636363,
      "grad_norm": 0.0848451480269432,
      "learning_rate": 0.0004955839671272899,
      "loss": 2.254,
      "step": 6100
    },
    {
      "epoch": 13.886363636363637,
      "grad_norm": 0.09910478442907333,
      "learning_rate": 0.0004955670814828213,
      "loss": 2.2511,
      "step": 6110
    },
    {
      "epoch": 13.909090909090908,
      "grad_norm": 0.08101129531860352,
      "learning_rate": 0.000495550163905762,
      "loss": 2.2706,
      "step": 6120
    },
    {
      "epoch": 13.931818181818182,
      "grad_norm": 0.08796987682580948,
      "learning_rate": 0.0004955332143983118,
      "loss": 2.2532,
      "step": 6130
    },
    {
      "epoch": 13.954545454545455,
      "grad_norm": 0.08150840550661087,
      "learning_rate": 0.0004955162329626745,
      "loss": 2.2551,
      "step": 6140
    },
    {
      "epoch": 13.977272727272727,
      "grad_norm": 0.08702168613672256,
      "learning_rate": 0.0004954992196010588,
      "loss": 2.2502,
      "step": 6150
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.14883728325366974,
      "learning_rate": 0.0004954821743156767,
      "loss": 2.2458,
      "step": 6160
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.1118288040161133,
      "eval_runtime": 8.7144,
      "eval_samples_per_second": 3492.028,
      "eval_steps_per_second": 13.656,
      "step": 6160
    },
    {
      "epoch": 14.022727272727273,
      "grad_norm": 0.1336580514907837,
      "learning_rate": 0.0004954650971087448,
      "loss": 2.2652,
      "step": 6170
    },
    {
      "epoch": 14.045454545454545,
      "grad_norm": 0.08987661451101303,
      "learning_rate": 0.0004954479879824837,
      "loss": 2.266,
      "step": 6180
    },
    {
      "epoch": 14.068181818181818,
      "grad_norm": 0.09279593825340271,
      "learning_rate": 0.0004954308469391182,
      "loss": 2.2639,
      "step": 6190
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 0.09165255725383759,
      "learning_rate": 0.0004954136739808774,
      "loss": 2.2597,
      "step": 6200
    },
    {
      "epoch": 14.113636363636363,
      "grad_norm": 0.08657742291688919,
      "learning_rate": 0.0004953964691099943,
      "loss": 2.2543,
      "step": 6210
    },
    {
      "epoch": 14.136363636363637,
      "grad_norm": 0.08566400408744812,
      "learning_rate": 0.0004953792323287061,
      "loss": 2.2529,
      "step": 6220
    },
    {
      "epoch": 14.159090909090908,
      "grad_norm": 0.09964986145496368,
      "learning_rate": 0.0004953619636392543,
      "loss": 2.2511,
      "step": 6230
    },
    {
      "epoch": 14.181818181818182,
      "grad_norm": 0.0798114612698555,
      "learning_rate": 0.0004953446630438843,
      "loss": 2.2442,
      "step": 6240
    },
    {
      "epoch": 14.204545454545455,
      "grad_norm": 0.08591178059577942,
      "learning_rate": 0.000495327330544846,
      "loss": 2.2374,
      "step": 6250
    },
    {
      "epoch": 14.227272727272727,
      "grad_norm": 0.11828801035881042,
      "learning_rate": 0.0004953099661443932,
      "loss": 2.24,
      "step": 6260
    },
    {
      "epoch": 14.25,
      "grad_norm": 0.08407056331634521,
      "learning_rate": 0.0004952925698447839,
      "loss": 2.2563,
      "step": 6270
    },
    {
      "epoch": 14.272727272727273,
      "grad_norm": 0.14372985064983368,
      "learning_rate": 0.0004952751416482801,
      "loss": 2.244,
      "step": 6280
    },
    {
      "epoch": 14.295454545454545,
      "grad_norm": 0.09485425055027008,
      "learning_rate": 0.0004952576815571483,
      "loss": 2.2652,
      "step": 6290
    },
    {
      "epoch": 14.318181818181818,
      "grad_norm": 0.08600948750972748,
      "learning_rate": 0.0004952401895736588,
      "loss": 2.2435,
      "step": 6300
    },
    {
      "epoch": 14.340909090909092,
      "grad_norm": 0.10666073113679886,
      "learning_rate": 0.0004952226657000863,
      "loss": 2.2465,
      "step": 6310
    },
    {
      "epoch": 14.363636363636363,
      "grad_norm": 0.08945795148611069,
      "learning_rate": 0.0004952051099387095,
      "loss": 2.2358,
      "step": 6320
    },
    {
      "epoch": 14.386363636363637,
      "grad_norm": 0.10496509820222855,
      "learning_rate": 0.0004951875222918112,
      "loss": 2.2557,
      "step": 6330
    },
    {
      "epoch": 14.409090909090908,
      "grad_norm": 0.09154962003231049,
      "learning_rate": 0.0004951699027616784,
      "loss": 2.2456,
      "step": 6340
    },
    {
      "epoch": 14.431818181818182,
      "grad_norm": 0.0855114683508873,
      "learning_rate": 0.0004951522513506024,
      "loss": 2.2406,
      "step": 6350
    },
    {
      "epoch": 14.454545454545455,
      "grad_norm": 0.07792908698320389,
      "learning_rate": 0.0004951345680608787,
      "loss": 2.2508,
      "step": 6360
    },
    {
      "epoch": 14.477272727272727,
      "grad_norm": 0.08777780830860138,
      "learning_rate": 0.0004951168528948063,
      "loss": 2.2571,
      "step": 6370
    },
    {
      "epoch": 14.5,
      "grad_norm": 0.08670366555452347,
      "learning_rate": 0.0004950991058546893,
      "loss": 2.2523,
      "step": 6380
    },
    {
      "epoch": 14.522727272727273,
      "grad_norm": 0.15320618450641632,
      "learning_rate": 0.0004950813269428351,
      "loss": 2.2537,
      "step": 6390
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 0.07198907434940338,
      "learning_rate": 0.0004950635161615557,
      "loss": 2.2546,
      "step": 6400
    },
    {
      "epoch": 14.568181818181818,
      "grad_norm": 0.11700139939785004,
      "learning_rate": 0.0004950456735131672,
      "loss": 2.2551,
      "step": 6410
    },
    {
      "epoch": 14.590909090909092,
      "grad_norm": 0.1249639242887497,
      "learning_rate": 0.0004950277989999897,
      "loss": 2.2535,
      "step": 6420
    },
    {
      "epoch": 14.613636363636363,
      "grad_norm": 0.0954279825091362,
      "learning_rate": 0.0004950098926243477,
      "loss": 2.2505,
      "step": 6430
    },
    {
      "epoch": 14.636363636363637,
      "grad_norm": 0.10312233865261078,
      "learning_rate": 0.0004949919543885694,
      "loss": 2.2579,
      "step": 6440
    },
    {
      "epoch": 14.659090909090908,
      "grad_norm": 0.08892546594142914,
      "learning_rate": 0.0004949739842949878,
      "loss": 2.2441,
      "step": 6450
    },
    {
      "epoch": 14.681818181818182,
      "grad_norm": 0.09165223687887192,
      "learning_rate": 0.0004949559823459393,
      "loss": 2.2492,
      "step": 6460
    },
    {
      "epoch": 14.704545454545455,
      "grad_norm": 0.08779413998126984,
      "learning_rate": 0.000494937948543765,
      "loss": 2.243,
      "step": 6470
    },
    {
      "epoch": 14.727272727272727,
      "grad_norm": 0.07701332867145538,
      "learning_rate": 0.0004949198828908099,
      "loss": 2.2557,
      "step": 6480
    },
    {
      "epoch": 14.75,
      "grad_norm": 0.08366915583610535,
      "learning_rate": 0.000494901785389423,
      "loss": 2.2568,
      "step": 6490
    },
    {
      "epoch": 14.772727272727273,
      "grad_norm": 0.06822162866592407,
      "learning_rate": 0.0004948836560419581,
      "loss": 2.2575,
      "step": 6500
    },
    {
      "epoch": 14.795454545454545,
      "grad_norm": 0.08529834449291229,
      "learning_rate": 0.0004948654948507723,
      "loss": 2.2649,
      "step": 6510
    },
    {
      "epoch": 14.818181818181818,
      "grad_norm": 0.08953725546598434,
      "learning_rate": 0.0004948473018182272,
      "loss": 2.2488,
      "step": 6520
    },
    {
      "epoch": 14.840909090909092,
      "grad_norm": 0.14862997829914093,
      "learning_rate": 0.0004948290769466886,
      "loss": 2.2537,
      "step": 6530
    },
    {
      "epoch": 14.863636363636363,
      "grad_norm": 0.08245426416397095,
      "learning_rate": 0.0004948108202385266,
      "loss": 2.2521,
      "step": 6540
    },
    {
      "epoch": 14.886363636363637,
      "grad_norm": 0.08941414952278137,
      "learning_rate": 0.000494792531696115,
      "loss": 2.2368,
      "step": 6550
    },
    {
      "epoch": 14.909090909090908,
      "grad_norm": 0.10277019441127777,
      "learning_rate": 0.000494774211321832,
      "loss": 2.2458,
      "step": 6560
    },
    {
      "epoch": 14.931818181818182,
      "grad_norm": 0.12239522486925125,
      "learning_rate": 0.0004947558591180599,
      "loss": 2.262,
      "step": 6570
    },
    {
      "epoch": 14.954545454545455,
      "grad_norm": 0.08032724261283875,
      "learning_rate": 0.0004947374750871853,
      "loss": 2.2497,
      "step": 6580
    },
    {
      "epoch": 14.977272727272727,
      "grad_norm": 0.08448272943496704,
      "learning_rate": 0.0004947190592315987,
      "loss": 2.2464,
      "step": 6590
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.1710928976535797,
      "learning_rate": 0.0004947006115536948,
      "loss": 2.2601,
      "step": 6600
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.1111177206039429,
      "eval_runtime": 8.9062,
      "eval_samples_per_second": 3416.824,
      "eval_steps_per_second": 13.361,
      "step": 6600
    },
    {
      "epoch": 15.022727272727273,
      "grad_norm": 0.0802425816655159,
      "learning_rate": 0.0004946821320558723,
      "loss": 2.2389,
      "step": 6610
    },
    {
      "epoch": 15.045454545454545,
      "grad_norm": 0.11696571856737137,
      "learning_rate": 0.0004946636207405345,
      "loss": 2.2511,
      "step": 6620
    },
    {
      "epoch": 15.068181818181818,
      "grad_norm": 0.09115780889987946,
      "learning_rate": 0.0004946450776100884,
      "loss": 2.2458,
      "step": 6630
    },
    {
      "epoch": 15.090909090909092,
      "grad_norm": 0.0957913026213646,
      "learning_rate": 0.0004946265026669455,
      "loss": 2.2417,
      "step": 6640
    },
    {
      "epoch": 15.113636363636363,
      "grad_norm": 0.08649259060621262,
      "learning_rate": 0.0004946078959135208,
      "loss": 2.2662,
      "step": 6650
    },
    {
      "epoch": 15.136363636363637,
      "grad_norm": 0.10551637411117554,
      "learning_rate": 0.0004945892573522341,
      "loss": 2.2468,
      "step": 6660
    },
    {
      "epoch": 15.159090909090908,
      "grad_norm": 0.07821212708950043,
      "learning_rate": 0.000494570586985509,
      "loss": 2.2502,
      "step": 6670
    },
    {
      "epoch": 15.181818181818182,
      "grad_norm": 0.07828733325004578,
      "learning_rate": 0.0004945518848157734,
      "loss": 2.2371,
      "step": 6680
    },
    {
      "epoch": 15.204545454545455,
      "grad_norm": 0.0967734083533287,
      "learning_rate": 0.0004945331508454592,
      "loss": 2.251,
      "step": 6690
    },
    {
      "epoch": 15.227272727272727,
      "grad_norm": 0.08916421979665756,
      "learning_rate": 0.0004945143850770026,
      "loss": 2.256,
      "step": 6700
    },
    {
      "epoch": 15.25,
      "grad_norm": 0.1917470544576645,
      "learning_rate": 0.0004944955875128437,
      "loss": 2.2552,
      "step": 6710
    },
    {
      "epoch": 15.272727272727273,
      "grad_norm": 0.0922958254814148,
      "learning_rate": 0.0004944767581554268,
      "loss": 2.2504,
      "step": 6720
    },
    {
      "epoch": 15.295454545454545,
      "grad_norm": 0.1092362105846405,
      "learning_rate": 0.0004944578970072005,
      "loss": 2.2565,
      "step": 6730
    },
    {
      "epoch": 15.318181818181818,
      "grad_norm": 0.08691606670618057,
      "learning_rate": 0.0004944390040706176,
      "loss": 2.2492,
      "step": 6740
    },
    {
      "epoch": 15.340909090909092,
      "grad_norm": 0.10697395354509354,
      "learning_rate": 0.0004944200793481347,
      "loss": 2.2444,
      "step": 6750
    },
    {
      "epoch": 15.363636363636363,
      "grad_norm": 0.10511483997106552,
      "learning_rate": 0.0004944011228422125,
      "loss": 2.2681,
      "step": 6760
    },
    {
      "epoch": 15.386363636363637,
      "grad_norm": 0.0790891945362091,
      "learning_rate": 0.0004943821345553165,
      "loss": 2.2502,
      "step": 6770
    },
    {
      "epoch": 15.409090909090908,
      "grad_norm": 0.08153779804706573,
      "learning_rate": 0.0004943631144899153,
      "loss": 2.2507,
      "step": 6780
    },
    {
      "epoch": 15.431818181818182,
      "grad_norm": 0.082891546189785,
      "learning_rate": 0.0004943440626484826,
      "loss": 2.2596,
      "step": 6790
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 0.07739462703466415,
      "learning_rate": 0.0004943249790334959,
      "loss": 2.2395,
      "step": 6800
    },
    {
      "epoch": 15.477272727272727,
      "grad_norm": 0.08865432441234589,
      "learning_rate": 0.0004943058636474363,
      "loss": 2.2478,
      "step": 6810
    },
    {
      "epoch": 15.5,
      "grad_norm": 0.08097200840711594,
      "learning_rate": 0.0004942867164927899,
      "loss": 2.241,
      "step": 6820
    },
    {
      "epoch": 15.522727272727273,
      "grad_norm": 0.07385172694921494,
      "learning_rate": 0.0004942675375720464,
      "loss": 2.2385,
      "step": 6830
    },
    {
      "epoch": 15.545454545454545,
      "grad_norm": 0.08324942737817764,
      "learning_rate": 0.0004942483268876996,
      "loss": 2.2378,
      "step": 6840
    },
    {
      "epoch": 15.568181818181818,
      "grad_norm": 0.07755215466022491,
      "learning_rate": 0.0004942290844422478,
      "loss": 2.2389,
      "step": 6850
    },
    {
      "epoch": 15.590909090909092,
      "grad_norm": 0.08133194595575333,
      "learning_rate": 0.0004942098102381932,
      "loss": 2.2445,
      "step": 6860
    },
    {
      "epoch": 15.613636363636363,
      "grad_norm": 0.08522328734397888,
      "learning_rate": 0.000494190504278042,
      "loss": 2.2687,
      "step": 6870
    },
    {
      "epoch": 15.636363636363637,
      "grad_norm": 0.11287696659564972,
      "learning_rate": 0.0004941711665643047,
      "loss": 2.2377,
      "step": 6880
    },
    {
      "epoch": 15.659090909090908,
      "grad_norm": 0.08407147973775864,
      "learning_rate": 0.0004941517970994961,
      "loss": 2.2435,
      "step": 6890
    },
    {
      "epoch": 15.681818181818182,
      "grad_norm": 0.08599699288606644,
      "learning_rate": 0.0004941323958861346,
      "loss": 2.2516,
      "step": 6900
    },
    {
      "epoch": 15.704545454545455,
      "grad_norm": 0.09365334361791611,
      "learning_rate": 0.0004941129629267434,
      "loss": 2.245,
      "step": 6910
    },
    {
      "epoch": 15.727272727272727,
      "grad_norm": 0.0776486024260521,
      "learning_rate": 0.0004940934982238492,
      "loss": 2.2478,
      "step": 6920
    },
    {
      "epoch": 15.75,
      "grad_norm": 0.08266735076904297,
      "learning_rate": 0.0004940740017799833,
      "loss": 2.2516,
      "step": 6930
    },
    {
      "epoch": 15.772727272727273,
      "grad_norm": 0.0881711095571518,
      "learning_rate": 0.000494054473597681,
      "loss": 2.2478,
      "step": 6940
    },
    {
      "epoch": 15.795454545454545,
      "grad_norm": 0.08516852557659149,
      "learning_rate": 0.0004940349136794813,
      "loss": 2.2525,
      "step": 6950
    },
    {
      "epoch": 15.818181818181818,
      "grad_norm": 0.09649433940649033,
      "learning_rate": 0.0004940153220279281,
      "loss": 2.2681,
      "step": 6960
    },
    {
      "epoch": 15.840909090909092,
      "grad_norm": 0.08130967617034912,
      "learning_rate": 0.0004939956986455688,
      "loss": 2.243,
      "step": 6970
    },
    {
      "epoch": 15.863636363636363,
      "grad_norm": 0.09502504765987396,
      "learning_rate": 0.0004939760435349553,
      "loss": 2.2627,
      "step": 6980
    },
    {
      "epoch": 15.886363636363637,
      "grad_norm": 0.07778166234493256,
      "learning_rate": 0.0004939563566986433,
      "loss": 2.2548,
      "step": 6990
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 0.07662884145975113,
      "learning_rate": 0.000493936638139193,
      "loss": 2.247,
      "step": 7000
    },
    {
      "epoch": 15.931818181818182,
      "grad_norm": 0.11138691008090973,
      "learning_rate": 0.0004939168878591683,
      "loss": 2.2455,
      "step": 7010
    },
    {
      "epoch": 15.954545454545455,
      "grad_norm": 0.0759715586900711,
      "learning_rate": 0.0004938971058611377,
      "loss": 2.2472,
      "step": 7020
    },
    {
      "epoch": 15.977272727272727,
      "grad_norm": 0.10277775675058365,
      "learning_rate": 0.0004938772921476733,
      "loss": 2.2456,
      "step": 7030
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.1427599936723709,
      "learning_rate": 0.0004938574467213517,
      "loss": 2.2373,
      "step": 7040
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.1094259023666382,
      "eval_runtime": 8.7129,
      "eval_samples_per_second": 3492.653,
      "eval_steps_per_second": 13.658,
      "step": 7040
    },
    {
      "epoch": 16.022727272727273,
      "grad_norm": 0.0927833542227745,
      "learning_rate": 0.0004938375695847537,
      "loss": 2.2405,
      "step": 7050
    },
    {
      "epoch": 16.045454545454547,
      "grad_norm": 0.08203430473804474,
      "learning_rate": 0.0004938176607404639,
      "loss": 2.2411,
      "step": 7060
    },
    {
      "epoch": 16.068181818181817,
      "grad_norm": 0.08137736469507217,
      "learning_rate": 0.0004937977201910711,
      "loss": 2.2565,
      "step": 7070
    },
    {
      "epoch": 16.09090909090909,
      "grad_norm": 0.07912155240774155,
      "learning_rate": 0.0004937777479391684,
      "loss": 2.2284,
      "step": 7080
    },
    {
      "epoch": 16.113636363636363,
      "grad_norm": 0.0895141065120697,
      "learning_rate": 0.0004937577439873529,
      "loss": 2.2538,
      "step": 7090
    },
    {
      "epoch": 16.136363636363637,
      "grad_norm": 0.0901772528886795,
      "learning_rate": 0.0004937377083382258,
      "loss": 2.2494,
      "step": 7100
    },
    {
      "epoch": 16.15909090909091,
      "grad_norm": 0.07703613489866257,
      "learning_rate": 0.0004937176409943924,
      "loss": 2.2407,
      "step": 7110
    },
    {
      "epoch": 16.181818181818183,
      "grad_norm": 0.07491188496351242,
      "learning_rate": 0.0004936975419584623,
      "loss": 2.2355,
      "step": 7120
    },
    {
      "epoch": 16.204545454545453,
      "grad_norm": 0.09558950364589691,
      "learning_rate": 0.0004936774112330491,
      "loss": 2.2442,
      "step": 7130
    },
    {
      "epoch": 16.227272727272727,
      "grad_norm": 0.09100542962551117,
      "learning_rate": 0.0004936572488207705,
      "loss": 2.2432,
      "step": 7140
    },
    {
      "epoch": 16.25,
      "grad_norm": 0.0743466392159462,
      "learning_rate": 0.0004936370547242482,
      "loss": 2.2529,
      "step": 7150
    },
    {
      "epoch": 16.272727272727273,
      "grad_norm": 0.09093020111322403,
      "learning_rate": 0.0004936168289461085,
      "loss": 2.2467,
      "step": 7160
    },
    {
      "epoch": 16.295454545454547,
      "grad_norm": 0.07074783742427826,
      "learning_rate": 0.000493596571488981,
      "loss": 2.2413,
      "step": 7170
    },
    {
      "epoch": 16.318181818181817,
      "grad_norm": 0.08596523851156235,
      "learning_rate": 0.0004935762823555003,
      "loss": 2.2515,
      "step": 7180
    },
    {
      "epoch": 16.34090909090909,
      "grad_norm": 0.08716800808906555,
      "learning_rate": 0.0004935559615483046,
      "loss": 2.2451,
      "step": 7190
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 0.1194467693567276,
      "learning_rate": 0.0004935356090700363,
      "loss": 2.2525,
      "step": 7200
    },
    {
      "epoch": 16.386363636363637,
      "grad_norm": 0.0791080892086029,
      "learning_rate": 0.000493515224923342,
      "loss": 2.2448,
      "step": 7210
    },
    {
      "epoch": 16.40909090909091,
      "grad_norm": 0.07765480130910873,
      "learning_rate": 0.0004934948091108723,
      "loss": 2.2251,
      "step": 7220
    },
    {
      "epoch": 16.431818181818183,
      "grad_norm": 0.07309556752443314,
      "learning_rate": 0.000493474361635282,
      "loss": 2.2517,
      "step": 7230
    },
    {
      "epoch": 16.454545454545453,
      "grad_norm": 0.09077131748199463,
      "learning_rate": 0.0004934538824992302,
      "loss": 2.2617,
      "step": 7240
    },
    {
      "epoch": 16.477272727272727,
      "grad_norm": 0.07815214991569519,
      "learning_rate": 0.0004934333717053797,
      "loss": 2.2463,
      "step": 7250
    },
    {
      "epoch": 16.5,
      "grad_norm": 0.07159234583377838,
      "learning_rate": 0.0004934128292563977,
      "loss": 2.2376,
      "step": 7260
    },
    {
      "epoch": 16.522727272727273,
      "grad_norm": 0.0951053649187088,
      "learning_rate": 0.0004933922551549556,
      "loss": 2.2469,
      "step": 7270
    },
    {
      "epoch": 16.545454545454547,
      "grad_norm": 0.07676044851541519,
      "learning_rate": 0.0004933716494037285,
      "loss": 2.2406,
      "step": 7280
    },
    {
      "epoch": 16.568181818181817,
      "grad_norm": 0.07916102558374405,
      "learning_rate": 0.0004933510120053963,
      "loss": 2.2474,
      "step": 7290
    },
    {
      "epoch": 16.59090909090909,
      "grad_norm": 0.4104984998703003,
      "learning_rate": 0.0004933303429626423,
      "loss": 2.239,
      "step": 7300
    },
    {
      "epoch": 16.613636363636363,
      "grad_norm": 0.2646985948085785,
      "learning_rate": 0.0004933096422781541,
      "loss": 2.2448,
      "step": 7310
    },
    {
      "epoch": 16.636363636363637,
      "grad_norm": 0.08362730592489243,
      "learning_rate": 0.0004932889099546238,
      "loss": 2.2567,
      "step": 7320
    },
    {
      "epoch": 16.65909090909091,
      "grad_norm": 0.09927057474851608,
      "learning_rate": 0.0004932681459947474,
      "loss": 2.2551,
      "step": 7330
    },
    {
      "epoch": 16.681818181818183,
      "grad_norm": 0.13298805058002472,
      "learning_rate": 0.0004932473504012248,
      "loss": 2.2501,
      "step": 7340
    },
    {
      "epoch": 16.704545454545453,
      "grad_norm": 0.14322663843631744,
      "learning_rate": 0.0004932265231767602,
      "loss": 2.2539,
      "step": 7350
    },
    {
      "epoch": 16.727272727272727,
      "grad_norm": 0.08407292515039444,
      "learning_rate": 0.0004932056643240618,
      "loss": 2.2458,
      "step": 7360
    },
    {
      "epoch": 16.75,
      "grad_norm": 0.08738375455141068,
      "learning_rate": 0.0004931847738458423,
      "loss": 2.2507,
      "step": 7370
    },
    {
      "epoch": 16.772727272727273,
      "grad_norm": 0.08442504703998566,
      "learning_rate": 0.0004931638517448179,
      "loss": 2.242,
      "step": 7380
    },
    {
      "epoch": 16.795454545454547,
      "grad_norm": 0.10605472326278687,
      "learning_rate": 0.0004931428980237093,
      "loss": 2.2439,
      "step": 7390
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 0.09486222267150879,
      "learning_rate": 0.0004931219126852415,
      "loss": 2.2371,
      "step": 7400
    },
    {
      "epoch": 16.84090909090909,
      "grad_norm": 0.0823284462094307,
      "learning_rate": 0.000493100895732143,
      "loss": 2.2512,
      "step": 7410
    },
    {
      "epoch": 16.863636363636363,
      "grad_norm": 0.09620478749275208,
      "learning_rate": 0.0004930798471671471,
      "loss": 2.2523,
      "step": 7420
    },
    {
      "epoch": 16.886363636363637,
      "grad_norm": 0.16635353863239288,
      "learning_rate": 0.0004930587669929905,
      "loss": 2.2427,
      "step": 7430
    },
    {
      "epoch": 16.90909090909091,
      "grad_norm": 0.09379523247480392,
      "learning_rate": 0.0004930376552124146,
      "loss": 2.2525,
      "step": 7440
    },
    {
      "epoch": 16.931818181818183,
      "grad_norm": 0.0751524269580841,
      "learning_rate": 0.0004930165118281648,
      "loss": 2.2477,
      "step": 7450
    },
    {
      "epoch": 16.954545454545453,
      "grad_norm": 0.07734517753124237,
      "learning_rate": 0.0004929953368429902,
      "loss": 2.2419,
      "step": 7460
    },
    {
      "epoch": 16.977272727272727,
      "grad_norm": 0.07658609747886658,
      "learning_rate": 0.0004929741302596445,
      "loss": 2.236,
      "step": 7470
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.14408829808235168,
      "learning_rate": 0.0004929528920808855,
      "loss": 2.2564,
      "step": 7480
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.1086405515670776,
      "eval_runtime": 8.9114,
      "eval_samples_per_second": 3414.856,
      "eval_steps_per_second": 13.354,
      "step": 7480
    },
    {
      "epoch": 17.022727272727273,
      "grad_norm": 0.08902022242546082,
      "learning_rate": 0.0004929316223094745,
      "loss": 2.2408,
      "step": 7490
    },
    {
      "epoch": 17.045454545454547,
      "grad_norm": 0.11872638016939163,
      "learning_rate": 0.0004929103209481777,
      "loss": 2.2547,
      "step": 7500
    },
    {
      "epoch": 17.068181818181817,
      "grad_norm": 0.09294601529836655,
      "learning_rate": 0.0004928889879997649,
      "loss": 2.2504,
      "step": 7510
    },
    {
      "epoch": 17.09090909090909,
      "grad_norm": 0.1342625468969345,
      "learning_rate": 0.0004928676234670102,
      "loss": 2.243,
      "step": 7520
    },
    {
      "epoch": 17.113636363636363,
      "grad_norm": 0.08638916909694672,
      "learning_rate": 0.0004928462273526917,
      "loss": 2.2417,
      "step": 7530
    },
    {
      "epoch": 17.136363636363637,
      "grad_norm": 0.08312701433897018,
      "learning_rate": 0.0004928247996595918,
      "loss": 2.2376,
      "step": 7540
    },
    {
      "epoch": 17.15909090909091,
      "grad_norm": 0.09186457097530365,
      "learning_rate": 0.0004928033403904967,
      "loss": 2.2635,
      "step": 7550
    },
    {
      "epoch": 17.181818181818183,
      "grad_norm": 0.07834962010383606,
      "learning_rate": 0.000492781849548197,
      "loss": 2.2344,
      "step": 7560
    },
    {
      "epoch": 17.204545454545453,
      "grad_norm": 0.08505745232105255,
      "learning_rate": 0.0004927603271354872,
      "loss": 2.2382,
      "step": 7570
    },
    {
      "epoch": 17.227272727272727,
      "grad_norm": 0.1028948649764061,
      "learning_rate": 0.0004927387731551659,
      "loss": 2.2362,
      "step": 7580
    },
    {
      "epoch": 17.25,
      "grad_norm": 0.09367895871400833,
      "learning_rate": 0.0004927171876100363,
      "loss": 2.2372,
      "step": 7590
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 0.09088651090860367,
      "learning_rate": 0.0004926955705029048,
      "loss": 2.244,
      "step": 7600
    },
    {
      "epoch": 17.295454545454547,
      "grad_norm": 0.08444371819496155,
      "learning_rate": 0.0004926739218365828,
      "loss": 2.2461,
      "step": 7610
    },
    {
      "epoch": 17.318181818181817,
      "grad_norm": 0.08064103126525879,
      "learning_rate": 0.0004926522416138851,
      "loss": 2.2441,
      "step": 7620
    },
    {
      "epoch": 17.34090909090909,
      "grad_norm": 0.07675955444574356,
      "learning_rate": 0.0004926305298376312,
      "loss": 2.2344,
      "step": 7630
    },
    {
      "epoch": 17.363636363636363,
      "grad_norm": 0.07766256481409073,
      "learning_rate": 0.0004926087865106442,
      "loss": 2.2448,
      "step": 7640
    },
    {
      "epoch": 17.386363636363637,
      "grad_norm": 0.10629503428936005,
      "learning_rate": 0.0004925870116357516,
      "loss": 2.2409,
      "step": 7650
    },
    {
      "epoch": 17.40909090909091,
      "grad_norm": 0.09355369210243225,
      "learning_rate": 0.0004925652052157849,
      "loss": 2.2485,
      "step": 7660
    },
    {
      "epoch": 17.431818181818183,
      "grad_norm": 0.0798749178647995,
      "learning_rate": 0.0004925433672535799,
      "loss": 2.2506,
      "step": 7670
    },
    {
      "epoch": 17.454545454545453,
      "grad_norm": 0.08398560434579849,
      "learning_rate": 0.0004925214977519759,
      "loss": 2.2475,
      "step": 7680
    },
    {
      "epoch": 17.477272727272727,
      "grad_norm": 0.08171634376049042,
      "learning_rate": 0.0004924995967138171,
      "loss": 2.2442,
      "step": 7690
    },
    {
      "epoch": 17.5,
      "grad_norm": 0.0668952614068985,
      "learning_rate": 0.0004924776641419512,
      "loss": 2.239,
      "step": 7700
    },
    {
      "epoch": 17.522727272727273,
      "grad_norm": 0.2048274725675583,
      "learning_rate": 0.0004924557000392306,
      "loss": 2.2351,
      "step": 7710
    },
    {
      "epoch": 17.545454545454547,
      "grad_norm": 0.08204365521669388,
      "learning_rate": 0.000492433704408511,
      "loss": 2.2401,
      "step": 7720
    },
    {
      "epoch": 17.568181818181817,
      "grad_norm": 0.08018933981657028,
      "learning_rate": 0.0004924116772526527,
      "loss": 2.2395,
      "step": 7730
    },
    {
      "epoch": 17.59090909090909,
      "grad_norm": 0.08370508253574371,
      "learning_rate": 0.0004923896185745203,
      "loss": 2.2188,
      "step": 7740
    },
    {
      "epoch": 17.613636363636363,
      "grad_norm": 0.08903351426124573,
      "learning_rate": 0.0004923675283769819,
      "loss": 2.2378,
      "step": 7750
    },
    {
      "epoch": 17.636363636363637,
      "grad_norm": 0.07998781651258469,
      "learning_rate": 0.0004923454066629102,
      "loss": 2.2381,
      "step": 7760
    },
    {
      "epoch": 17.65909090909091,
      "grad_norm": 0.08460645377635956,
      "learning_rate": 0.0004923232534351818,
      "loss": 2.2292,
      "step": 7770
    },
    {
      "epoch": 17.681818181818183,
      "grad_norm": 0.0837242603302002,
      "learning_rate": 0.0004923010686966774,
      "loss": 2.2451,
      "step": 7780
    },
    {
      "epoch": 17.704545454545453,
      "grad_norm": 0.09633669257164001,
      "learning_rate": 0.0004922788524502818,
      "loss": 2.2435,
      "step": 7790
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 0.08579166978597641,
      "learning_rate": 0.0004922566046988839,
      "loss": 2.2272,
      "step": 7800
    },
    {
      "epoch": 17.75,
      "grad_norm": 0.09077239781618118,
      "learning_rate": 0.0004922343254453768,
      "loss": 2.2477,
      "step": 7810
    },
    {
      "epoch": 17.772727272727273,
      "grad_norm": 0.08094719797372818,
      "learning_rate": 0.0004922120146926576,
      "loss": 2.2428,
      "step": 7820
    },
    {
      "epoch": 17.795454545454547,
      "grad_norm": 0.08166750520467758,
      "learning_rate": 0.0004921896724436274,
      "loss": 2.2423,
      "step": 7830
    },
    {
      "epoch": 17.818181818181817,
      "grad_norm": 0.09554164856672287,
      "learning_rate": 0.0004921672987011916,
      "loss": 2.2492,
      "step": 7840
    },
    {
      "epoch": 17.84090909090909,
      "grad_norm": 0.08816372603178024,
      "learning_rate": 0.0004921448934682596,
      "loss": 2.2464,
      "step": 7850
    },
    {
      "epoch": 17.863636363636363,
      "grad_norm": 0.11341752111911774,
      "learning_rate": 0.0004921224567477447,
      "loss": 2.2501,
      "step": 7860
    },
    {
      "epoch": 17.886363636363637,
      "grad_norm": 0.09246388077735901,
      "learning_rate": 0.0004920999885425646,
      "loss": 2.2391,
      "step": 7870
    },
    {
      "epoch": 17.90909090909091,
      "grad_norm": 0.18691542744636536,
      "learning_rate": 0.0004920774888556411,
      "loss": 2.2504,
      "step": 7880
    },
    {
      "epoch": 17.931818181818183,
      "grad_norm": 0.08344992250204086,
      "learning_rate": 0.0004920549576899,
      "loss": 2.2524,
      "step": 7890
    },
    {
      "epoch": 17.954545454545453,
      "grad_norm": 0.655479907989502,
      "learning_rate": 0.0004920323950482709,
      "loss": 2.2331,
      "step": 7900
    },
    {
      "epoch": 17.977272727272727,
      "grad_norm": 0.10756251960992813,
      "learning_rate": 0.0004920098009336879,
      "loss": 2.2447,
      "step": 7910
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.26673269271850586,
      "learning_rate": 0.0004919871753490891,
      "loss": 2.2441,
      "step": 7920
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.1080141067504883,
      "eval_runtime": 8.7089,
      "eval_samples_per_second": 3494.255,
      "eval_steps_per_second": 13.664,
      "step": 7920
    },
    {
      "epoch": 18.022727272727273,
      "grad_norm": 0.08448315411806107,
      "learning_rate": 0.0004919645182974165,
      "loss": 2.2285,
      "step": 7930
    },
    {
      "epoch": 18.045454545454547,
      "grad_norm": 0.09529616683721542,
      "learning_rate": 0.0004919418297816165,
      "loss": 2.2292,
      "step": 7940
    },
    {
      "epoch": 18.068181818181817,
      "grad_norm": 0.13465511798858643,
      "learning_rate": 0.0004919191098046394,
      "loss": 2.235,
      "step": 7950
    },
    {
      "epoch": 18.09090909090909,
      "grad_norm": 0.08583343029022217,
      "learning_rate": 0.0004918963583694396,
      "loss": 2.2318,
      "step": 7960
    },
    {
      "epoch": 18.113636363636363,
      "grad_norm": 0.08585494756698608,
      "learning_rate": 0.0004918735754789754,
      "loss": 2.2377,
      "step": 7970
    },
    {
      "epoch": 18.136363636363637,
      "grad_norm": 0.08828824758529663,
      "learning_rate": 0.0004918507611362098,
      "loss": 2.246,
      "step": 7980
    },
    {
      "epoch": 18.15909090909091,
      "grad_norm": 0.10408870875835419,
      "learning_rate": 0.0004918279153441091,
      "loss": 2.2372,
      "step": 7990
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 0.09451843053102493,
      "learning_rate": 0.0004918050381056444,
      "loss": 2.2331,
      "step": 8000
    },
    {
      "epoch": 18.204545454545453,
      "grad_norm": 0.09599582850933075,
      "learning_rate": 0.0004917821294237903,
      "loss": 2.2575,
      "step": 8010
    },
    {
      "epoch": 18.227272727272727,
      "grad_norm": 0.08237390965223312,
      "learning_rate": 0.0004917591893015259,
      "loss": 2.2329,
      "step": 8020
    },
    {
      "epoch": 18.25,
      "grad_norm": 0.08201891928911209,
      "learning_rate": 0.0004917362177418342,
      "loss": 2.2428,
      "step": 8030
    },
    {
      "epoch": 18.272727272727273,
      "grad_norm": 0.08780267834663391,
      "learning_rate": 0.0004917132147477023,
      "loss": 2.2429,
      "step": 8040
    },
    {
      "epoch": 18.295454545454547,
      "grad_norm": 0.17012329399585724,
      "learning_rate": 0.0004916901803221216,
      "loss": 2.2396,
      "step": 8050
    },
    {
      "epoch": 18.318181818181817,
      "grad_norm": 0.08011849224567413,
      "learning_rate": 0.0004916671144680872,
      "loss": 2.2378,
      "step": 8060
    },
    {
      "epoch": 18.34090909090909,
      "grad_norm": 0.08385957777500153,
      "learning_rate": 0.0004916440171885986,
      "loss": 2.2351,
      "step": 8070
    },
    {
      "epoch": 18.363636363636363,
      "grad_norm": 0.0923486053943634,
      "learning_rate": 0.0004916208884866593,
      "loss": 2.236,
      "step": 8080
    },
    {
      "epoch": 18.386363636363637,
      "grad_norm": 0.08864054828882217,
      "learning_rate": 0.0004915977283652767,
      "loss": 2.2459,
      "step": 8090
    },
    {
      "epoch": 18.40909090909091,
      "grad_norm": 0.07332464307546616,
      "learning_rate": 0.0004915745368274626,
      "loss": 2.2422,
      "step": 8100
    },
    {
      "epoch": 18.431818181818183,
      "grad_norm": 0.07964841276407242,
      "learning_rate": 0.0004915513138762327,
      "loss": 2.2277,
      "step": 8110
    },
    {
      "epoch": 18.454545454545453,
      "grad_norm": 0.08692658692598343,
      "learning_rate": 0.000491528059514607,
      "loss": 2.2366,
      "step": 8120
    },
    {
      "epoch": 18.477272727272727,
      "grad_norm": 0.07531611621379852,
      "learning_rate": 0.0004915047737456091,
      "loss": 2.2359,
      "step": 8130
    },
    {
      "epoch": 18.5,
      "grad_norm": 0.10174193978309631,
      "learning_rate": 0.0004914814565722671,
      "loss": 2.2369,
      "step": 8140
    },
    {
      "epoch": 18.522727272727273,
      "grad_norm": 0.2715248167514801,
      "learning_rate": 0.0004914581079976132,
      "loss": 2.2437,
      "step": 8150
    },
    {
      "epoch": 18.545454545454547,
      "grad_norm": 0.0934850424528122,
      "learning_rate": 0.0004914347280246833,
      "loss": 2.2249,
      "step": 8160
    },
    {
      "epoch": 18.568181818181817,
      "grad_norm": 0.09527797251939774,
      "learning_rate": 0.0004914113166565179,
      "loss": 2.2368,
      "step": 8170
    },
    {
      "epoch": 18.59090909090909,
      "grad_norm": 0.08078355342149734,
      "learning_rate": 0.0004913878738961612,
      "loss": 2.2459,
      "step": 8180
    },
    {
      "epoch": 18.613636363636363,
      "grad_norm": 0.08078687638044357,
      "learning_rate": 0.0004913643997466617,
      "loss": 2.2382,
      "step": 8190
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 0.07352212816476822,
      "learning_rate": 0.0004913408942110718,
      "loss": 2.2303,
      "step": 8200
    },
    {
      "epoch": 18.65909090909091,
      "grad_norm": 0.08086749166250229,
      "learning_rate": 0.0004913173572924481,
      "loss": 2.2414,
      "step": 8210
    },
    {
      "epoch": 18.681818181818183,
      "grad_norm": 0.08240379393100739,
      "learning_rate": 0.0004912937889938511,
      "loss": 2.2335,
      "step": 8220
    },
    {
      "epoch": 18.704545454545453,
      "grad_norm": 0.08818583190441132,
      "learning_rate": 0.0004912701893183459,
      "loss": 2.2299,
      "step": 8230
    },
    {
      "epoch": 18.727272727272727,
      "grad_norm": 0.07365477830171585,
      "learning_rate": 0.0004912465582690009,
      "loss": 2.2391,
      "step": 8240
    },
    {
      "epoch": 18.75,
      "grad_norm": 0.09056523442268372,
      "learning_rate": 0.0004912228958488892,
      "loss": 2.233,
      "step": 8250
    },
    {
      "epoch": 18.772727272727273,
      "grad_norm": 0.08644174039363861,
      "learning_rate": 0.0004911992020610878,
      "loss": 2.2415,
      "step": 8260
    },
    {
      "epoch": 18.795454545454547,
      "grad_norm": 0.08083773404359818,
      "learning_rate": 0.0004911754769086776,
      "loss": 2.2341,
      "step": 8270
    },
    {
      "epoch": 18.818181818181817,
      "grad_norm": 0.08053220063447952,
      "learning_rate": 0.0004911517203947439,
      "loss": 2.247,
      "step": 8280
    },
    {
      "epoch": 18.84090909090909,
      "grad_norm": 0.08283428847789764,
      "learning_rate": 0.0004911279325223758,
      "loss": 2.2436,
      "step": 8290
    },
    {
      "epoch": 18.863636363636363,
      "grad_norm": 0.07911424338817596,
      "learning_rate": 0.0004911041132946665,
      "loss": 2.2499,
      "step": 8300
    },
    {
      "epoch": 18.886363636363637,
      "grad_norm": 0.07783231139183044,
      "learning_rate": 0.0004910802627147136,
      "loss": 2.2524,
      "step": 8310
    },
    {
      "epoch": 18.90909090909091,
      "grad_norm": 0.08073841780424118,
      "learning_rate": 0.0004910563807856182,
      "loss": 2.2445,
      "step": 8320
    },
    {
      "epoch": 18.931818181818183,
      "grad_norm": 0.09442546963691711,
      "learning_rate": 0.0004910324675104862,
      "loss": 2.2419,
      "step": 8330
    },
    {
      "epoch": 18.954545454545453,
      "grad_norm": 0.07743190973997116,
      "learning_rate": 0.000491008522892427,
      "loss": 2.2353,
      "step": 8340
    },
    {
      "epoch": 18.977272727272727,
      "grad_norm": 0.09492775052785873,
      "learning_rate": 0.0004909845469345542,
      "loss": 2.2445,
      "step": 8350
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.18902085721492767,
      "learning_rate": 0.0004909605396399855,
      "loss": 2.2445,
      "step": 8360
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.1075111627578735,
      "eval_runtime": 8.7217,
      "eval_samples_per_second": 3489.101,
      "eval_steps_per_second": 13.644,
      "step": 8360
    },
    {
      "epoch": 19.022727272727273,
      "grad_norm": 0.09636365622282028,
      "learning_rate": 0.000490936501011843,
      "loss": 2.2397,
      "step": 8370
    },
    {
      "epoch": 19.045454545454547,
      "grad_norm": 0.09337664395570755,
      "learning_rate": 0.0004909124310532524,
      "loss": 2.2435,
      "step": 8380
    },
    {
      "epoch": 19.068181818181817,
      "grad_norm": 0.07811431586742401,
      "learning_rate": 0.0004908883297673435,
      "loss": 2.2315,
      "step": 8390
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 0.09901458024978638,
      "learning_rate": 0.0004908641971572506,
      "loss": 2.2351,
      "step": 8400
    },
    {
      "epoch": 19.113636363636363,
      "grad_norm": 0.07418865710496902,
      "learning_rate": 0.0004908400332261118,
      "loss": 2.228,
      "step": 8410
    },
    {
      "epoch": 19.136363636363637,
      "grad_norm": 0.08874668926000595,
      "learning_rate": 0.0004908158379770692,
      "loss": 2.234,
      "step": 8420
    },
    {
      "epoch": 19.15909090909091,
      "grad_norm": 0.08492461591959,
      "learning_rate": 0.000490791611413269,
      "loss": 2.2399,
      "step": 8430
    },
    {
      "epoch": 19.181818181818183,
      "grad_norm": 0.08389440178871155,
      "learning_rate": 0.0004907673535378617,
      "loss": 2.2308,
      "step": 8440
    },
    {
      "epoch": 19.204545454545453,
      "grad_norm": 0.07417892664670944,
      "learning_rate": 0.0004907430643540014,
      "loss": 2.237,
      "step": 8450
    },
    {
      "epoch": 19.227272727272727,
      "grad_norm": 0.07495580613613129,
      "learning_rate": 0.0004907187438648468,
      "loss": 2.2296,
      "step": 8460
    },
    {
      "epoch": 19.25,
      "grad_norm": 0.09315408021211624,
      "learning_rate": 0.0004906943920735605,
      "loss": 2.2414,
      "step": 8470
    },
    {
      "epoch": 19.272727272727273,
      "grad_norm": 0.0846453383564949,
      "learning_rate": 0.000490670008983309,
      "loss": 2.2419,
      "step": 8480
    },
    {
      "epoch": 19.295454545454547,
      "grad_norm": 0.0740467831492424,
      "learning_rate": 0.0004906455945972631,
      "loss": 2.2344,
      "step": 8490
    },
    {
      "epoch": 19.318181818181817,
      "grad_norm": 0.11250588297843933,
      "learning_rate": 0.0004906211489185973,
      "loss": 2.2497,
      "step": 8500
    },
    {
      "epoch": 19.34090909090909,
      "grad_norm": 0.08216186612844467,
      "learning_rate": 0.0004905966719504906,
      "loss": 2.2272,
      "step": 8510
    },
    {
      "epoch": 19.363636363636363,
      "grad_norm": 0.17359869182109833,
      "learning_rate": 0.0004905721636961259,
      "loss": 2.2262,
      "step": 8520
    },
    {
      "epoch": 19.386363636363637,
      "grad_norm": 0.07516716420650482,
      "learning_rate": 0.00049054762415869,
      "loss": 2.2292,
      "step": 8530
    },
    {
      "epoch": 19.40909090909091,
      "grad_norm": 0.11295207589864731,
      "learning_rate": 0.0004905230533413743,
      "loss": 2.2352,
      "step": 8540
    },
    {
      "epoch": 19.431818181818183,
      "grad_norm": 0.11300817877054214,
      "learning_rate": 0.0004904984512473735,
      "loss": 2.2328,
      "step": 8550
    },
    {
      "epoch": 19.454545454545453,
      "grad_norm": 0.07734331488609314,
      "learning_rate": 0.0004904738178798869,
      "loss": 2.2341,
      "step": 8560
    },
    {
      "epoch": 19.477272727272727,
      "grad_norm": 0.07008245587348938,
      "learning_rate": 0.0004904491532421177,
      "loss": 2.2274,
      "step": 8570
    },
    {
      "epoch": 19.5,
      "grad_norm": 0.07539121061563492,
      "learning_rate": 0.0004904244573372732,
      "loss": 2.2362,
      "step": 8580
    },
    {
      "epoch": 19.522727272727273,
      "grad_norm": 0.07715331763029099,
      "learning_rate": 0.0004903997301685649,
      "loss": 2.238,
      "step": 8590
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 0.09106726199388504,
      "learning_rate": 0.0004903749717392081,
      "loss": 2.2521,
      "step": 8600
    },
    {
      "epoch": 19.568181818181817,
      "grad_norm": 0.09033945947885513,
      "learning_rate": 0.0004903501820524223,
      "loss": 2.2331,
      "step": 8610
    },
    {
      "epoch": 19.59090909090909,
      "grad_norm": 0.08695070445537567,
      "learning_rate": 0.0004903253611114309,
      "loss": 2.2406,
      "step": 8620
    },
    {
      "epoch": 19.613636363636363,
      "grad_norm": 0.08063486218452454,
      "learning_rate": 0.0004903005089194617,
      "loss": 2.2397,
      "step": 8630
    },
    {
      "epoch": 19.636363636363637,
      "grad_norm": 0.07550983130931854,
      "learning_rate": 0.0004902756254797465,
      "loss": 2.2363,
      "step": 8640
    },
    {
      "epoch": 19.65909090909091,
      "grad_norm": 0.07929866015911102,
      "learning_rate": 0.0004902507107955209,
      "loss": 2.2234,
      "step": 8650
    },
    {
      "epoch": 19.681818181818183,
      "grad_norm": 0.07591123878955841,
      "learning_rate": 0.0004902257648700246,
      "loss": 2.2423,
      "step": 8660
    },
    {
      "epoch": 19.704545454545453,
      "grad_norm": 0.1063230112195015,
      "learning_rate": 0.0004902007877065017,
      "loss": 2.2494,
      "step": 8670
    },
    {
      "epoch": 19.727272727272727,
      "grad_norm": 0.09329330176115036,
      "learning_rate": 0.0004901757793081999,
      "loss": 2.2318,
      "step": 8680
    },
    {
      "epoch": 19.75,
      "grad_norm": 0.07565157115459442,
      "learning_rate": 0.0004901507396783714,
      "loss": 2.2411,
      "step": 8690
    },
    {
      "epoch": 19.772727272727273,
      "grad_norm": 0.2595553994178772,
      "learning_rate": 0.0004901256688202721,
      "loss": 2.2319,
      "step": 8700
    },
    {
      "epoch": 19.795454545454547,
      "grad_norm": 0.0829838216304779,
      "learning_rate": 0.0004901005667371622,
      "loss": 2.2372,
      "step": 8710
    },
    {
      "epoch": 19.818181818181817,
      "grad_norm": 0.08065293729305267,
      "learning_rate": 0.0004900754334323059,
      "loss": 2.2387,
      "step": 8720
    },
    {
      "epoch": 19.84090909090909,
      "grad_norm": 0.07000274956226349,
      "learning_rate": 0.0004900502689089713,
      "loss": 2.2268,
      "step": 8730
    },
    {
      "epoch": 19.863636363636363,
      "grad_norm": 0.08154290914535522,
      "learning_rate": 0.0004900250731704309,
      "loss": 2.2329,
      "step": 8740
    },
    {
      "epoch": 19.886363636363637,
      "grad_norm": 0.08224503695964813,
      "learning_rate": 0.0004899998462199609,
      "loss": 2.2352,
      "step": 8750
    },
    {
      "epoch": 19.90909090909091,
      "grad_norm": 0.08016438782215118,
      "learning_rate": 0.0004899745880608418,
      "loss": 2.2308,
      "step": 8760
    },
    {
      "epoch": 19.931818181818183,
      "grad_norm": 0.2616182565689087,
      "learning_rate": 0.0004899492986963581,
      "loss": 2.2322,
      "step": 8770
    },
    {
      "epoch": 19.954545454545453,
      "grad_norm": 0.08248872309923172,
      "learning_rate": 0.0004899239781297982,
      "loss": 2.2254,
      "step": 8780
    },
    {
      "epoch": 19.977272727272727,
      "grad_norm": 0.10269326716661453,
      "learning_rate": 0.0004898986263644547,
      "loss": 2.2369,
      "step": 8790
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.13644371926784515,
      "learning_rate": 0.0004898732434036243,
      "loss": 2.2306,
      "step": 8800
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.106819748878479,
      "eval_runtime": 8.7169,
      "eval_samples_per_second": 3491.022,
      "eval_steps_per_second": 13.652,
      "step": 8800
    },
    {
      "epoch": 20.022727272727273,
      "grad_norm": 0.09812802076339722,
      "learning_rate": 0.0004898478292506078,
      "loss": 2.2337,
      "step": 8810
    },
    {
      "epoch": 20.045454545454547,
      "grad_norm": 0.07973505556583405,
      "learning_rate": 0.0004898223839087098,
      "loss": 2.229,
      "step": 8820
    },
    {
      "epoch": 20.068181818181817,
      "grad_norm": 0.08089639246463776,
      "learning_rate": 0.0004897969073812393,
      "loss": 2.23,
      "step": 8830
    },
    {
      "epoch": 20.09090909090909,
      "grad_norm": 0.07423871755599976,
      "learning_rate": 0.000489771399671509,
      "loss": 2.2319,
      "step": 8840
    },
    {
      "epoch": 20.113636363636363,
      "grad_norm": 0.0852859690785408,
      "learning_rate": 0.0004897458607828358,
      "loss": 2.2419,
      "step": 8850
    },
    {
      "epoch": 20.136363636363637,
      "grad_norm": 0.08503838628530502,
      "learning_rate": 0.0004897202907185407,
      "loss": 2.2451,
      "step": 8860
    },
    {
      "epoch": 20.15909090909091,
      "grad_norm": 0.08947395533323288,
      "learning_rate": 0.0004896946894819489,
      "loss": 2.242,
      "step": 8870
    },
    {
      "epoch": 20.181818181818183,
      "grad_norm": 0.09075836837291718,
      "learning_rate": 0.0004896690570763893,
      "loss": 2.237,
      "step": 8880
    },
    {
      "epoch": 20.204545454545453,
      "grad_norm": 0.10243359208106995,
      "learning_rate": 0.0004896433935051951,
      "loss": 2.2302,
      "step": 8890
    },
    {
      "epoch": 20.227272727272727,
      "grad_norm": 0.10787248611450195,
      "learning_rate": 0.0004896176987717036,
      "loss": 2.2299,
      "step": 8900
    },
    {
      "epoch": 20.25,
      "grad_norm": 0.08813795447349548,
      "learning_rate": 0.0004895919728792558,
      "loss": 2.2287,
      "step": 8910
    },
    {
      "epoch": 20.272727272727273,
      "grad_norm": 0.08530127257108688,
      "learning_rate": 0.0004895662158311972,
      "loss": 2.2424,
      "step": 8920
    },
    {
      "epoch": 20.295454545454547,
      "grad_norm": 0.09130462259054184,
      "learning_rate": 0.0004895404276308771,
      "loss": 2.2438,
      "step": 8930
    },
    {
      "epoch": 20.318181818181817,
      "grad_norm": 0.08579766005277634,
      "learning_rate": 0.0004895146082816488,
      "loss": 2.226,
      "step": 8940
    },
    {
      "epoch": 20.34090909090909,
      "grad_norm": 0.11579892784357071,
      "learning_rate": 0.00048948875778687,
      "loss": 2.2389,
      "step": 8950
    },
    {
      "epoch": 20.363636363636363,
      "grad_norm": 0.07957518100738525,
      "learning_rate": 0.0004894628761499018,
      "loss": 2.2345,
      "step": 8960
    },
    {
      "epoch": 20.386363636363637,
      "grad_norm": 0.08489672094583511,
      "learning_rate": 0.0004894369633741102,
      "loss": 2.2325,
      "step": 8970
    },
    {
      "epoch": 20.40909090909091,
      "grad_norm": 0.13225002586841583,
      "learning_rate": 0.0004894110194628644,
      "loss": 2.2404,
      "step": 8980
    },
    {
      "epoch": 20.431818181818183,
      "grad_norm": 0.08522345870733261,
      "learning_rate": 0.0004893850444195382,
      "loss": 2.2261,
      "step": 8990
    },
    {
      "epoch": 20.454545454545453,
      "grad_norm": 0.08123494684696198,
      "learning_rate": 0.0004893590382475094,
      "loss": 2.2437,
      "step": 9000
    },
    {
      "epoch": 20.477272727272727,
      "grad_norm": 0.0767296552658081,
      "learning_rate": 0.0004893330009501597,
      "loss": 2.2251,
      "step": 9010
    },
    {
      "epoch": 20.5,
      "grad_norm": 0.07850382477045059,
      "learning_rate": 0.0004893069325308747,
      "loss": 2.2297,
      "step": 9020
    },
    {
      "epoch": 20.522727272727273,
      "grad_norm": 0.0788549855351448,
      "learning_rate": 0.0004892808329930445,
      "loss": 2.2295,
      "step": 9030
    },
    {
      "epoch": 20.545454545454547,
      "grad_norm": 0.096012644469738,
      "learning_rate": 0.0004892547023400628,
      "loss": 2.2331,
      "step": 9040
    },
    {
      "epoch": 20.568181818181817,
      "grad_norm": 0.08513451367616653,
      "learning_rate": 0.0004892285405753275,
      "loss": 2.2317,
      "step": 9050
    },
    {
      "epoch": 20.59090909090909,
      "grad_norm": 0.0980152040719986,
      "learning_rate": 0.0004892023477022408,
      "loss": 2.2317,
      "step": 9060
    },
    {
      "epoch": 20.613636363636363,
      "grad_norm": 0.09656322002410889,
      "learning_rate": 0.0004891761237242085,
      "loss": 2.2328,
      "step": 9070
    },
    {
      "epoch": 20.636363636363637,
      "grad_norm": 0.07906030118465424,
      "learning_rate": 0.0004891498686446407,
      "loss": 2.2417,
      "step": 9080
    },
    {
      "epoch": 20.65909090909091,
      "grad_norm": 0.08687689155340195,
      "learning_rate": 0.0004891235824669517,
      "loss": 2.24,
      "step": 9090
    },
    {
      "epoch": 20.681818181818183,
      "grad_norm": 0.10436484962701797,
      "learning_rate": 0.0004890972651945594,
      "loss": 2.2295,
      "step": 9100
    },
    {
      "epoch": 20.704545454545453,
      "grad_norm": 0.08679589629173279,
      "learning_rate": 0.0004890709168308861,
      "loss": 2.2332,
      "step": 9110
    },
    {
      "epoch": 20.727272727272727,
      "grad_norm": 0.08463206142187119,
      "learning_rate": 0.0004890445373793581,
      "loss": 2.2315,
      "step": 9120
    },
    {
      "epoch": 20.75,
      "grad_norm": 0.08385871350765228,
      "learning_rate": 0.0004890181268434056,
      "loss": 2.2268,
      "step": 9130
    },
    {
      "epoch": 20.772727272727273,
      "grad_norm": 0.08462727814912796,
      "learning_rate": 0.000488991685226463,
      "loss": 2.2342,
      "step": 9140
    },
    {
      "epoch": 20.795454545454547,
      "grad_norm": 0.07684153318405151,
      "learning_rate": 0.0004889652125319685,
      "loss": 2.2219,
      "step": 9150
    },
    {
      "epoch": 20.818181818181817,
      "grad_norm": 0.11138657480478287,
      "learning_rate": 0.0004889387087633647,
      "loss": 2.2318,
      "step": 9160
    },
    {
      "epoch": 20.84090909090909,
      "grad_norm": 0.08954238891601562,
      "learning_rate": 0.000488912173924098,
      "loss": 2.2273,
      "step": 9170
    },
    {
      "epoch": 20.863636363636363,
      "grad_norm": 0.08920202404260635,
      "learning_rate": 0.0004888856080176188,
      "loss": 2.2292,
      "step": 9180
    },
    {
      "epoch": 20.886363636363637,
      "grad_norm": 0.07875236868858337,
      "learning_rate": 0.0004888590110473817,
      "loss": 2.2398,
      "step": 9190
    },
    {
      "epoch": 20.90909090909091,
      "grad_norm": 0.0777110680937767,
      "learning_rate": 0.0004888323830168452,
      "loss": 2.2367,
      "step": 9200
    },
    {
      "epoch": 20.931818181818183,
      "grad_norm": 0.0764484629034996,
      "learning_rate": 0.000488805723929472,
      "loss": 2.2318,
      "step": 9210
    },
    {
      "epoch": 20.954545454545453,
      "grad_norm": 0.14695532619953156,
      "learning_rate": 0.0004887790337887287,
      "loss": 2.2298,
      "step": 9220
    },
    {
      "epoch": 20.977272727272727,
      "grad_norm": 0.07722395658493042,
      "learning_rate": 0.0004887523125980859,
      "loss": 2.2353,
      "step": 9230
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.18281187117099762,
      "learning_rate": 0.0004887255603610184,
      "loss": 2.2451,
      "step": 9240
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.1068603992462158,
      "eval_runtime": 8.7472,
      "eval_samples_per_second": 3478.946,
      "eval_steps_per_second": 13.604,
      "step": 9240
    },
    {
      "epoch": 21.022727272727273,
      "grad_norm": 0.0803704485297203,
      "learning_rate": 0.0004886987770810051,
      "loss": 2.223,
      "step": 9250
    },
    {
      "epoch": 21.045454545454547,
      "grad_norm": 0.10220374912023544,
      "learning_rate": 0.0004886719627615286,
      "loss": 2.2301,
      "step": 9260
    },
    {
      "epoch": 21.068181818181817,
      "grad_norm": 0.10169172286987305,
      "learning_rate": 0.0004886451174060758,
      "loss": 2.2356,
      "step": 9270
    },
    {
      "epoch": 21.09090909090909,
      "grad_norm": 0.09353764355182648,
      "learning_rate": 0.0004886182410181374,
      "loss": 2.2298,
      "step": 9280
    },
    {
      "epoch": 21.113636363636363,
      "grad_norm": 0.10382629930973053,
      "learning_rate": 0.0004885913336012085,
      "loss": 2.2367,
      "step": 9290
    },
    {
      "epoch": 21.136363636363637,
      "grad_norm": 0.09178461134433746,
      "learning_rate": 0.0004885643951587881,
      "loss": 2.2372,
      "step": 9300
    },
    {
      "epoch": 21.15909090909091,
      "grad_norm": 0.09582331031560898,
      "learning_rate": 0.000488537425694379,
      "loss": 2.2324,
      "step": 9310
    },
    {
      "epoch": 21.181818181818183,
      "grad_norm": 0.27996963262557983,
      "learning_rate": 0.0004885104252114882,
      "loss": 2.2262,
      "step": 9320
    },
    {
      "epoch": 21.204545454545453,
      "grad_norm": 0.07671400904655457,
      "learning_rate": 0.000488483393713627,
      "loss": 2.2201,
      "step": 9330
    },
    {
      "epoch": 21.227272727272727,
      "grad_norm": 0.0845562070608139,
      "learning_rate": 0.00048845633120431,
      "loss": 2.2296,
      "step": 9340
    },
    {
      "epoch": 21.25,
      "grad_norm": 0.09900890290737152,
      "learning_rate": 0.0004884292376870567,
      "loss": 2.2256,
      "step": 9350
    },
    {
      "epoch": 21.272727272727273,
      "grad_norm": 0.07663969695568085,
      "learning_rate": 0.0004884021131653902,
      "loss": 2.2369,
      "step": 9360
    },
    {
      "epoch": 21.295454545454547,
      "grad_norm": 0.08054520189762115,
      "learning_rate": 0.0004883749576428375,
      "loss": 2.2206,
      "step": 9370
    },
    {
      "epoch": 21.318181818181817,
      "grad_norm": 0.08113489300012589,
      "learning_rate": 0.0004883477711229298,
      "loss": 2.2384,
      "step": 9380
    },
    {
      "epoch": 21.34090909090909,
      "grad_norm": 0.1611010879278183,
      "learning_rate": 0.0004883205536092025,
      "loss": 2.2225,
      "step": 9390
    },
    {
      "epoch": 21.363636363636363,
      "grad_norm": 0.09094729274511337,
      "learning_rate": 0.0004882933051051947,
      "loss": 2.2257,
      "step": 9400
    },
    {
      "epoch": 21.386363636363637,
      "grad_norm": 0.0898008942604065,
      "learning_rate": 0.00048826602561444975,
      "loss": 2.2412,
      "step": 9410
    },
    {
      "epoch": 21.40909090909091,
      "grad_norm": 0.12405825406312943,
      "learning_rate": 0.000488238715140515,
      "loss": 2.2236,
      "step": 9420
    },
    {
      "epoch": 21.431818181818183,
      "grad_norm": 0.11003252863883972,
      "learning_rate": 0.0004882113736869418,
      "loss": 2.2203,
      "step": 9430
    },
    {
      "epoch": 21.454545454545453,
      "grad_norm": 0.08523564785718918,
      "learning_rate": 0.00048818400125728547,
      "loss": 2.2259,
      "step": 9440
    },
    {
      "epoch": 21.477272727272727,
      "grad_norm": 0.08550994098186493,
      "learning_rate": 0.00048815659785510544,
      "loss": 2.237,
      "step": 9450
    },
    {
      "epoch": 21.5,
      "grad_norm": 0.08009015023708344,
      "learning_rate": 0.00048812916348396515,
      "loss": 2.216,
      "step": 9460
    },
    {
      "epoch": 21.522727272727273,
      "grad_norm": 0.256119042634964,
      "learning_rate": 0.00048810169814743207,
      "loss": 2.2312,
      "step": 9470
    },
    {
      "epoch": 21.545454545454547,
      "grad_norm": 0.0864890068769455,
      "learning_rate": 0.0004880742018490776,
      "loss": 2.2415,
      "step": 9480
    },
    {
      "epoch": 21.568181818181817,
      "grad_norm": 0.08358235657215118,
      "learning_rate": 0.00048804667459247743,
      "loss": 2.236,
      "step": 9490
    },
    {
      "epoch": 21.59090909090909,
      "grad_norm": 0.07784584164619446,
      "learning_rate": 0.0004880191163812109,
      "loss": 2.2405,
      "step": 9500
    },
    {
      "epoch": 21.613636363636363,
      "grad_norm": 0.08140453696250916,
      "learning_rate": 0.00048799152721886173,
      "loss": 2.2385,
      "step": 9510
    },
    {
      "epoch": 21.636363636363637,
      "grad_norm": 0.07549624890089035,
      "learning_rate": 0.0004879639071090174,
      "loss": 2.2264,
      "step": 9520
    },
    {
      "epoch": 21.65909090909091,
      "grad_norm": 0.07720543444156647,
      "learning_rate": 0.0004879362560552696,
      "loss": 2.2367,
      "step": 9530
    },
    {
      "epoch": 21.681818181818183,
      "grad_norm": 0.08262733370065689,
      "learning_rate": 0.00048790857406121393,
      "loss": 2.2246,
      "step": 9540
    },
    {
      "epoch": 21.704545454545453,
      "grad_norm": 0.07702233642339706,
      "learning_rate": 0.00048788086113045,
      "loss": 2.2358,
      "step": 9550
    },
    {
      "epoch": 21.727272727272727,
      "grad_norm": 0.08010254800319672,
      "learning_rate": 0.00048785311726658154,
      "loss": 2.2353,
      "step": 9560
    },
    {
      "epoch": 21.75,
      "grad_norm": 0.07688463479280472,
      "learning_rate": 0.0004878253424732163,
      "loss": 2.2369,
      "step": 9570
    },
    {
      "epoch": 21.772727272727273,
      "grad_norm": 0.07549741119146347,
      "learning_rate": 0.0004877975367539659,
      "loss": 2.2383,
      "step": 9580
    },
    {
      "epoch": 21.795454545454547,
      "grad_norm": 0.08448027074337006,
      "learning_rate": 0.0004877697001124462,
      "loss": 2.2273,
      "step": 9590
    },
    {
      "epoch": 21.818181818181817,
      "grad_norm": 0.07309502363204956,
      "learning_rate": 0.0004877418325522769,
      "loss": 2.2156,
      "step": 9600
    },
    {
      "epoch": 21.84090909090909,
      "grad_norm": 0.08761975914239883,
      "learning_rate": 0.0004877139340770818,
      "loss": 2.2279,
      "step": 9610
    },
    {
      "epoch": 21.863636363636363,
      "grad_norm": 0.2944779694080353,
      "learning_rate": 0.00048768600469048875,
      "loss": 2.2305,
      "step": 9620
    },
    {
      "epoch": 21.886363636363637,
      "grad_norm": 0.08109233528375626,
      "learning_rate": 0.0004876580443961295,
      "loss": 2.2325,
      "step": 9630
    },
    {
      "epoch": 21.90909090909091,
      "grad_norm": 0.082707978785038,
      "learning_rate": 0.00048763005319764,
      "loss": 2.2306,
      "step": 9640
    },
    {
      "epoch": 21.931818181818183,
      "grad_norm": 0.08183695375919342,
      "learning_rate": 0.00048760203109865995,
      "loss": 2.2187,
      "step": 9650
    },
    {
      "epoch": 21.954545454545453,
      "grad_norm": 0.09880862385034561,
      "learning_rate": 0.0004875739781028334,
      "loss": 2.2318,
      "step": 9660
    },
    {
      "epoch": 21.977272727272727,
      "grad_norm": 0.09901753067970276,
      "learning_rate": 0.00048754589421380823,
      "loss": 2.2276,
      "step": 9670
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.11786255985498428,
      "learning_rate": 0.0004875177794352363,
      "loss": 2.224,
      "step": 9680
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.1058905124664307,
      "eval_runtime": 8.7224,
      "eval_samples_per_second": 3488.814,
      "eval_steps_per_second": 13.643,
      "step": 9680
    },
    {
      "epoch": 22.022727272727273,
      "grad_norm": 0.17605553567409515,
      "learning_rate": 0.0004874896337707737,
      "loss": 2.2174,
      "step": 9690
    },
    {
      "epoch": 22.045454545454547,
      "grad_norm": 0.08710287511348724,
      "learning_rate": 0.00048746145722408006,
      "loss": 2.2224,
      "step": 9700
    },
    {
      "epoch": 22.068181818181817,
      "grad_norm": 0.09457669407129288,
      "learning_rate": 0.00048743324979881966,
      "loss": 2.2395,
      "step": 9710
    },
    {
      "epoch": 22.09090909090909,
      "grad_norm": 0.1193903237581253,
      "learning_rate": 0.00048740501149866037,
      "loss": 2.2284,
      "step": 9720
    },
    {
      "epoch": 22.113636363636363,
      "grad_norm": 0.08670674264431,
      "learning_rate": 0.0004873767423272741,
      "loss": 2.2297,
      "step": 9730
    },
    {
      "epoch": 22.136363636363637,
      "grad_norm": 0.1282031387090683,
      "learning_rate": 0.00048734844228833705,
      "loss": 2.2302,
      "step": 9740
    },
    {
      "epoch": 22.15909090909091,
      "grad_norm": 0.10518331825733185,
      "learning_rate": 0.0004873201113855291,
      "loss": 2.2215,
      "step": 9750
    },
    {
      "epoch": 22.181818181818183,
      "grad_norm": 0.15960687398910522,
      "learning_rate": 0.0004872917496225344,
      "loss": 2.2301,
      "step": 9760
    },
    {
      "epoch": 22.204545454545453,
      "grad_norm": 0.09663443267345428,
      "learning_rate": 0.00048726335700304094,
      "loss": 2.2392,
      "step": 9770
    },
    {
      "epoch": 22.227272727272727,
      "grad_norm": 0.10904193669557571,
      "learning_rate": 0.00048723493353074074,
      "loss": 2.2224,
      "step": 9780
    },
    {
      "epoch": 22.25,
      "grad_norm": 0.10633968561887741,
      "learning_rate": 0.00048720647920932994,
      "loss": 2.2336,
      "step": 9790
    },
    {
      "epoch": 22.272727272727273,
      "grad_norm": 0.13382165133953094,
      "learning_rate": 0.0004871779940425086,
      "loss": 2.2273,
      "step": 9800
    },
    {
      "epoch": 22.295454545454547,
      "grad_norm": 0.09177996963262558,
      "learning_rate": 0.0004871494780339809,
      "loss": 2.2221,
      "step": 9810
    },
    {
      "epoch": 22.318181818181817,
      "grad_norm": 0.16989916563034058,
      "learning_rate": 0.0004871209311874548,
      "loss": 2.2285,
      "step": 9820
    },
    {
      "epoch": 22.34090909090909,
      "grad_norm": 0.09701255708932877,
      "learning_rate": 0.0004870923535066426,
      "loss": 2.2245,
      "step": 9830
    },
    {
      "epoch": 22.363636363636363,
      "grad_norm": 0.10525047779083252,
      "learning_rate": 0.0004870637449952603,
      "loss": 2.2367,
      "step": 9840
    },
    {
      "epoch": 22.386363636363637,
      "grad_norm": 0.1076558455824852,
      "learning_rate": 0.0004870351056570281,
      "loss": 2.2254,
      "step": 9850
    },
    {
      "epoch": 22.40909090909091,
      "grad_norm": 0.08985456824302673,
      "learning_rate": 0.00048700643549567013,
      "loss": 2.2334,
      "step": 9860
    },
    {
      "epoch": 22.431818181818183,
      "grad_norm": 0.33418509364128113,
      "learning_rate": 0.0004869777345149146,
      "loss": 2.2293,
      "step": 9870
    },
    {
      "epoch": 22.454545454545453,
      "grad_norm": 0.10423112660646439,
      "learning_rate": 0.00048694900271849354,
      "loss": 2.2242,
      "step": 9880
    },
    {
      "epoch": 22.477272727272727,
      "grad_norm": 0.27535703778266907,
      "learning_rate": 0.00048692024011014315,
      "loss": 2.2337,
      "step": 9890
    },
    {
      "epoch": 22.5,
      "grad_norm": 0.1301894187927246,
      "learning_rate": 0.00048689144669360375,
      "loss": 2.2323,
      "step": 9900
    },
    {
      "epoch": 22.522727272727273,
      "grad_norm": 0.09265821427106857,
      "learning_rate": 0.0004868626224726194,
      "loss": 2.2267,
      "step": 9910
    },
    {
      "epoch": 22.545454545454547,
      "grad_norm": 0.09223310649394989,
      "learning_rate": 0.0004868337674509384,
      "loss": 2.2332,
      "step": 9920
    },
    {
      "epoch": 22.568181818181817,
      "grad_norm": 0.09020692855119705,
      "learning_rate": 0.0004868048816323128,
      "loss": 2.2262,
      "step": 9930
    },
    {
      "epoch": 22.59090909090909,
      "grad_norm": 0.12077582627534866,
      "learning_rate": 0.00048677596502049884,
      "loss": 2.233,
      "step": 9940
    },
    {
      "epoch": 22.613636363636363,
      "grad_norm": 0.11015572398900986,
      "learning_rate": 0.00048674701761925685,
      "loss": 2.2419,
      "step": 9950
    },
    {
      "epoch": 22.636363636363637,
      "grad_norm": 0.20686949789524078,
      "learning_rate": 0.00048671803943235086,
      "loss": 2.224,
      "step": 9960
    },
    {
      "epoch": 22.65909090909091,
      "grad_norm": 0.08401498943567276,
      "learning_rate": 0.00048668903046354917,
      "loss": 2.22,
      "step": 9970
    },
    {
      "epoch": 22.681818181818183,
      "grad_norm": 0.09144483506679535,
      "learning_rate": 0.00048665999071662404,
      "loss": 2.2265,
      "step": 9980
    },
    {
      "epoch": 22.704545454545453,
      "grad_norm": 0.12280072271823883,
      "learning_rate": 0.0004866309201953516,
      "loss": 2.2281,
      "step": 9990
    },
    {
      "epoch": 22.727272727272727,
      "grad_norm": 0.10711529850959778,
      "learning_rate": 0.00048660181890351214,
      "loss": 2.2258,
      "step": 10000
    },
    {
      "epoch": 22.75,
      "grad_norm": 0.08355522155761719,
      "learning_rate": 0.0004865726868448898,
      "loss": 2.2259,
      "step": 10010
    },
    {
      "epoch": 22.772727272727273,
      "grad_norm": 0.08243484795093536,
      "learning_rate": 0.00048654352402327283,
      "loss": 2.2373,
      "step": 10020
    },
    {
      "epoch": 22.795454545454547,
      "grad_norm": 0.09773209691047668,
      "learning_rate": 0.0004865143304424535,
      "loss": 2.2275,
      "step": 10030
    },
    {
      "epoch": 22.818181818181817,
      "grad_norm": 0.11046810448169708,
      "learning_rate": 0.00048648510610622796,
      "loss": 2.2303,
      "step": 10040
    },
    {
      "epoch": 22.84090909090909,
      "grad_norm": 0.08672185242176056,
      "learning_rate": 0.00048645585101839644,
      "loss": 2.224,
      "step": 10050
    },
    {
      "epoch": 22.863636363636363,
      "grad_norm": 0.09860200434923172,
      "learning_rate": 0.0004864265651827632,
      "loss": 2.2282,
      "step": 10060
    },
    {
      "epoch": 22.886363636363637,
      "grad_norm": 0.08543960750102997,
      "learning_rate": 0.00048639724860313646,
      "loss": 2.2469,
      "step": 10070
    },
    {
      "epoch": 22.90909090909091,
      "grad_norm": 0.08095834404230118,
      "learning_rate": 0.0004863679012833284,
      "loss": 2.2425,
      "step": 10080
    },
    {
      "epoch": 22.931818181818183,
      "grad_norm": 0.15095154941082,
      "learning_rate": 0.00048633852322715524,
      "loss": 2.2347,
      "step": 10090
    },
    {
      "epoch": 22.954545454545453,
      "grad_norm": 0.08888719975948334,
      "learning_rate": 0.0004863091144384371,
      "loss": 2.2193,
      "step": 10100
    },
    {
      "epoch": 22.977272727272727,
      "grad_norm": 0.11340446770191193,
      "learning_rate": 0.0004862796749209983,
      "loss": 2.2254,
      "step": 10110
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.14250220358371735,
      "learning_rate": 0.0004862502046786671,
      "loss": 2.2169,
      "step": 10120
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.1062809228897095,
      "eval_runtime": 8.7391,
      "eval_samples_per_second": 3482.152,
      "eval_steps_per_second": 13.617,
      "step": 10120
    },
    {
      "epoch": 23.022727272727273,
      "grad_norm": 0.09529036283493042,
      "learning_rate": 0.0004862207037152756,
      "loss": 2.2264,
      "step": 10130
    },
    {
      "epoch": 23.045454545454547,
      "grad_norm": 0.08091682195663452,
      "learning_rate": 0.00048619117203466003,
      "loss": 2.2325,
      "step": 10140
    },
    {
      "epoch": 23.068181818181817,
      "grad_norm": 0.09373511373996735,
      "learning_rate": 0.0004861616096406604,
      "loss": 2.2265,
      "step": 10150
    },
    {
      "epoch": 23.09090909090909,
      "grad_norm": 0.08050865679979324,
      "learning_rate": 0.00048613201653712113,
      "loss": 2.2253,
      "step": 10160
    },
    {
      "epoch": 23.113636363636363,
      "grad_norm": 0.09487798064947128,
      "learning_rate": 0.00048610239272789033,
      "loss": 2.23,
      "step": 10170
    },
    {
      "epoch": 23.136363636363637,
      "grad_norm": 0.09343716502189636,
      "learning_rate": 0.0004860727382168201,
      "loss": 2.229,
      "step": 10180
    },
    {
      "epoch": 23.15909090909091,
      "grad_norm": 0.10702485591173172,
      "learning_rate": 0.0004860430530077666,
      "loss": 2.2196,
      "step": 10190
    },
    {
      "epoch": 23.181818181818183,
      "grad_norm": 0.10108401626348495,
      "learning_rate": 0.00048601333710459,
      "loss": 2.2207,
      "step": 10200
    },
    {
      "epoch": 23.204545454545453,
      "grad_norm": 0.09816509485244751,
      "learning_rate": 0.00048598359051115446,
      "loss": 2.2344,
      "step": 10210
    },
    {
      "epoch": 23.227272727272727,
      "grad_norm": 0.09483589977025986,
      "learning_rate": 0.00048595381323132816,
      "loss": 2.2188,
      "step": 10220
    },
    {
      "epoch": 23.25,
      "grad_norm": 0.08953539282083511,
      "learning_rate": 0.0004859240052689831,
      "loss": 2.2193,
      "step": 10230
    },
    {
      "epoch": 23.272727272727273,
      "grad_norm": 0.09707728773355484,
      "learning_rate": 0.00048589416662799553,
      "loss": 2.2181,
      "step": 10240
    },
    {
      "epoch": 23.295454545454547,
      "grad_norm": 0.17488020658493042,
      "learning_rate": 0.00048586429731224543,
      "loss": 2.2233,
      "step": 10250
    },
    {
      "epoch": 23.318181818181817,
      "grad_norm": 0.10840831696987152,
      "learning_rate": 0.0004858343973256169,
      "loss": 2.2194,
      "step": 10260
    },
    {
      "epoch": 23.34090909090909,
      "grad_norm": 0.5442454218864441,
      "learning_rate": 0.0004858044666719981,
      "loss": 2.2328,
      "step": 10270
    },
    {
      "epoch": 23.363636363636363,
      "grad_norm": 0.11416009813547134,
      "learning_rate": 0.000485774505355281,
      "loss": 2.2397,
      "step": 10280
    },
    {
      "epoch": 23.386363636363637,
      "grad_norm": 0.17772994935512543,
      "learning_rate": 0.0004857445133793618,
      "loss": 2.2309,
      "step": 10290
    },
    {
      "epoch": 23.40909090909091,
      "grad_norm": 0.14581021666526794,
      "learning_rate": 0.00048571449074814036,
      "loss": 2.2312,
      "step": 10300
    },
    {
      "epoch": 23.431818181818183,
      "grad_norm": 0.4637141525745392,
      "learning_rate": 0.00048568443746552085,
      "loss": 2.2307,
      "step": 10310
    },
    {
      "epoch": 23.454545454545453,
      "grad_norm": 0.5678106546401978,
      "learning_rate": 0.0004856543535354112,
      "loss": 2.2245,
      "step": 10320
    },
    {
      "epoch": 23.477272727272727,
      "grad_norm": 0.11698479950428009,
      "learning_rate": 0.00048562423896172344,
      "loss": 2.2248,
      "step": 10330
    },
    {
      "epoch": 23.5,
      "grad_norm": 0.10524953156709671,
      "learning_rate": 0.0004855940937483735,
      "loss": 2.2189,
      "step": 10340
    },
    {
      "epoch": 23.522727272727273,
      "grad_norm": 0.10883387923240662,
      "learning_rate": 0.00048556391789928143,
      "loss": 2.2332,
      "step": 10350
    },
    {
      "epoch": 23.545454545454547,
      "grad_norm": 1.0635910034179688,
      "learning_rate": 0.0004855337114183711,
      "loss": 2.2244,
      "step": 10360
    },
    {
      "epoch": 23.568181818181817,
      "grad_norm": 0.09879400581121445,
      "learning_rate": 0.00048550347430957055,
      "loss": 2.2257,
      "step": 10370
    },
    {
      "epoch": 23.59090909090909,
      "grad_norm": 0.15607160329818726,
      "learning_rate": 0.0004854732065768116,
      "loss": 2.222,
      "step": 10380
    },
    {
      "epoch": 23.613636363636363,
      "grad_norm": 0.10212261229753494,
      "learning_rate": 0.00048544290822403015,
      "loss": 2.2376,
      "step": 10390
    },
    {
      "epoch": 23.636363636363637,
      "grad_norm": 0.09810134768486023,
      "learning_rate": 0.00048541257925516614,
      "loss": 2.2232,
      "step": 10400
    },
    {
      "epoch": 23.65909090909091,
      "grad_norm": 0.1548956334590912,
      "learning_rate": 0.00048538221967416343,
      "loss": 2.2257,
      "step": 10410
    },
    {
      "epoch": 23.681818181818183,
      "grad_norm": 0.11658827215433121,
      "learning_rate": 0.00048535182948496983,
      "loss": 2.221,
      "step": 10420
    },
    {
      "epoch": 23.704545454545453,
      "grad_norm": 0.17894576489925385,
      "learning_rate": 0.0004853214086915371,
      "loss": 2.2206,
      "step": 10430
    },
    {
      "epoch": 23.727272727272727,
      "grad_norm": 0.11645394563674927,
      "learning_rate": 0.00048529095729782113,
      "loss": 2.2245,
      "step": 10440
    },
    {
      "epoch": 23.75,
      "grad_norm": 0.10034682601690292,
      "learning_rate": 0.00048526047530778174,
      "loss": 2.2333,
      "step": 10450
    },
    {
      "epoch": 23.772727272727273,
      "grad_norm": 0.11795400083065033,
      "learning_rate": 0.00048522996272538254,
      "loss": 2.2419,
      "step": 10460
    },
    {
      "epoch": 23.795454545454547,
      "grad_norm": 0.1987578123807907,
      "learning_rate": 0.0004851994195545914,
      "loss": 2.2316,
      "step": 10470
    },
    {
      "epoch": 23.818181818181817,
      "grad_norm": 0.09369253367185593,
      "learning_rate": 0.00048516884579938004,
      "loss": 2.2342,
      "step": 10480
    },
    {
      "epoch": 23.84090909090909,
      "grad_norm": 0.1167164072394371,
      "learning_rate": 0.000485138241463724,
      "loss": 2.2234,
      "step": 10490
    },
    {
      "epoch": 23.863636363636363,
      "grad_norm": 0.11084917932748795,
      "learning_rate": 0.00048510760655160314,
      "loss": 2.2323,
      "step": 10500
    },
    {
      "epoch": 23.886363636363637,
      "grad_norm": 0.11580365151166916,
      "learning_rate": 0.00048507694106700094,
      "loss": 2.218,
      "step": 10510
    },
    {
      "epoch": 23.90909090909091,
      "grad_norm": 0.10106244683265686,
      "learning_rate": 0.00048504624501390515,
      "loss": 2.2142,
      "step": 10520
    },
    {
      "epoch": 23.931818181818183,
      "grad_norm": 0.13010606169700623,
      "learning_rate": 0.0004850155183963073,
      "loss": 2.2235,
      "step": 10530
    },
    {
      "epoch": 23.954545454545453,
      "grad_norm": 0.1054287776350975,
      "learning_rate": 0.000484984761218203,
      "loss": 2.245,
      "step": 10540
    },
    {
      "epoch": 23.977272727272727,
      "grad_norm": 0.11304623633623123,
      "learning_rate": 0.0004849539734835918,
      "loss": 2.229,
      "step": 10550
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.14411993324756622,
      "learning_rate": 0.0004849231551964771,
      "loss": 2.2381,
      "step": 10560
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.1059211492538452,
      "eval_runtime": 8.9182,
      "eval_samples_per_second": 3412.233,
      "eval_steps_per_second": 13.343,
      "step": 10560
    },
    {
      "epoch": 24.022727272727273,
      "grad_norm": 0.09666217118501663,
      "learning_rate": 0.0004848923063608666,
      "loss": 2.2223,
      "step": 10570
    },
    {
      "epoch": 24.045454545454547,
      "grad_norm": 0.11396782100200653,
      "learning_rate": 0.0004848614269807715,
      "loss": 2.2185,
      "step": 10580
    },
    {
      "epoch": 24.068181818181817,
      "grad_norm": 0.11642775684595108,
      "learning_rate": 0.0004848305170602074,
      "loss": 2.2261,
      "step": 10590
    },
    {
      "epoch": 24.09090909090909,
      "grad_norm": 0.11500035226345062,
      "learning_rate": 0.00048479957660319374,
      "loss": 2.2194,
      "step": 10600
    },
    {
      "epoch": 24.113636363636363,
      "grad_norm": 0.10501071065664291,
      "learning_rate": 0.0004847686056137538,
      "loss": 2.2231,
      "step": 10610
    },
    {
      "epoch": 24.136363636363637,
      "grad_norm": 0.11274071037769318,
      "learning_rate": 0.00048473760409591495,
      "loss": 2.2308,
      "step": 10620
    },
    {
      "epoch": 24.15909090909091,
      "grad_norm": 0.25935158133506775,
      "learning_rate": 0.0004847065720537086,
      "loss": 2.2392,
      "step": 10630
    },
    {
      "epoch": 24.181818181818183,
      "grad_norm": 0.10811629891395569,
      "learning_rate": 0.0004846755094911699,
      "loss": 2.2163,
      "step": 10640
    },
    {
      "epoch": 24.204545454545453,
      "grad_norm": 0.09632738679647446,
      "learning_rate": 0.00048464441641233815,
      "loss": 2.2298,
      "step": 10650
    },
    {
      "epoch": 24.227272727272727,
      "grad_norm": 0.08607842028141022,
      "learning_rate": 0.00048461329282125667,
      "loss": 2.2336,
      "step": 10660
    },
    {
      "epoch": 24.25,
      "grad_norm": 0.13714811205863953,
      "learning_rate": 0.00048458213872197244,
      "loss": 2.2286,
      "step": 10670
    },
    {
      "epoch": 24.272727272727273,
      "grad_norm": 0.09774074703454971,
      "learning_rate": 0.0004845509541185369,
      "loss": 2.2211,
      "step": 10680
    },
    {
      "epoch": 24.295454545454547,
      "grad_norm": 0.0886155217885971,
      "learning_rate": 0.0004845197390150049,
      "loss": 2.2328,
      "step": 10690
    },
    {
      "epoch": 24.318181818181817,
      "grad_norm": 0.11137033253908157,
      "learning_rate": 0.00048448849341543567,
      "loss": 2.2311,
      "step": 10700
    },
    {
      "epoch": 24.34090909090909,
      "grad_norm": 0.10526613891124725,
      "learning_rate": 0.00048445721732389227,
      "loss": 2.2237,
      "step": 10710
    },
    {
      "epoch": 24.363636363636363,
      "grad_norm": 0.09507197141647339,
      "learning_rate": 0.00048442591074444175,
      "loss": 2.2326,
      "step": 10720
    },
    {
      "epoch": 24.386363636363637,
      "grad_norm": 0.16001178324222565,
      "learning_rate": 0.00048439457368115496,
      "loss": 2.2151,
      "step": 10730
    },
    {
      "epoch": 24.40909090909091,
      "grad_norm": 0.08378695696592331,
      "learning_rate": 0.00048436320613810704,
      "loss": 2.2391,
      "step": 10740
    },
    {
      "epoch": 24.431818181818183,
      "grad_norm": 0.10441464930772781,
      "learning_rate": 0.0004843318081193767,
      "loss": 2.2152,
      "step": 10750
    },
    {
      "epoch": 24.454545454545453,
      "grad_norm": 0.11313214898109436,
      "learning_rate": 0.00048430037962904696,
      "loss": 2.2304,
      "step": 10760
    },
    {
      "epoch": 24.477272727272727,
      "grad_norm": 0.14472071826457977,
      "learning_rate": 0.0004842689206712046,
      "loss": 2.2277,
      "step": 10770
    },
    {
      "epoch": 24.5,
      "grad_norm": 0.09853348881006241,
      "learning_rate": 0.0004842374312499405,
      "loss": 2.2203,
      "step": 10780
    },
    {
      "epoch": 24.522727272727273,
      "grad_norm": 0.08882208168506622,
      "learning_rate": 0.0004842059113693492,
      "loss": 2.2261,
      "step": 10790
    },
    {
      "epoch": 24.545454545454547,
      "grad_norm": 0.08294852823019028,
      "learning_rate": 0.00048417436103352976,
      "loss": 2.2274,
      "step": 10800
    },
    {
      "epoch": 24.568181818181817,
      "grad_norm": 0.12673474848270416,
      "learning_rate": 0.00048414278024658464,
      "loss": 2.2188,
      "step": 10810
    },
    {
      "epoch": 24.59090909090909,
      "grad_norm": 0.08299320936203003,
      "learning_rate": 0.0004841111690126205,
      "loss": 2.2181,
      "step": 10820
    },
    {
      "epoch": 24.613636363636363,
      "grad_norm": 0.10012271255254745,
      "learning_rate": 0.00048407952733574793,
      "loss": 2.2329,
      "step": 10830
    },
    {
      "epoch": 24.636363636363637,
      "grad_norm": 0.12354611605405807,
      "learning_rate": 0.0004840478552200816,
      "loss": 2.2188,
      "step": 10840
    },
    {
      "epoch": 24.65909090909091,
      "grad_norm": 0.08105120807886124,
      "learning_rate": 0.00048401615266973995,
      "loss": 2.2163,
      "step": 10850
    },
    {
      "epoch": 24.681818181818183,
      "grad_norm": 0.10741163045167923,
      "learning_rate": 0.00048398441968884544,
      "loss": 2.2232,
      "step": 10860
    },
    {
      "epoch": 24.704545454545453,
      "grad_norm": 0.09594663232564926,
      "learning_rate": 0.00048395265628152457,
      "loss": 2.2257,
      "step": 10870
    },
    {
      "epoch": 24.727272727272727,
      "grad_norm": 0.10927172750234604,
      "learning_rate": 0.0004839208624519077,
      "loss": 2.2204,
      "step": 10880
    },
    {
      "epoch": 24.75,
      "grad_norm": 0.09314916282892227,
      "learning_rate": 0.0004838890382041291,
      "loss": 2.2288,
      "step": 10890
    },
    {
      "epoch": 24.772727272727273,
      "grad_norm": 0.5046340823173523,
      "learning_rate": 0.0004838571835423272,
      "loss": 2.2276,
      "step": 10900
    },
    {
      "epoch": 24.795454545454547,
      "grad_norm": 0.08674915879964828,
      "learning_rate": 0.00048382529847064415,
      "loss": 2.2238,
      "step": 10910
    },
    {
      "epoch": 24.818181818181817,
      "grad_norm": 0.09458726644515991,
      "learning_rate": 0.00048379338299322627,
      "loss": 2.2142,
      "step": 10920
    },
    {
      "epoch": 24.84090909090909,
      "grad_norm": 0.08955948799848557,
      "learning_rate": 0.0004837614371142236,
      "loss": 2.2221,
      "step": 10930
    },
    {
      "epoch": 24.863636363636363,
      "grad_norm": 0.09081001579761505,
      "learning_rate": 0.00048372946083779036,
      "loss": 2.2146,
      "step": 10940
    },
    {
      "epoch": 24.886363636363637,
      "grad_norm": 0.0881192609667778,
      "learning_rate": 0.0004836974541680846,
      "loss": 2.225,
      "step": 10950
    },
    {
      "epoch": 24.90909090909091,
      "grad_norm": 0.11265940219163895,
      "learning_rate": 0.0004836654171092683,
      "loss": 2.2355,
      "step": 10960
    },
    {
      "epoch": 24.931818181818183,
      "grad_norm": 0.10169071704149246,
      "learning_rate": 0.0004836333496655074,
      "loss": 2.2332,
      "step": 10970
    },
    {
      "epoch": 24.954545454545453,
      "grad_norm": 0.09463910758495331,
      "learning_rate": 0.00048360125184097197,
      "loss": 2.2365,
      "step": 10980
    },
    {
      "epoch": 24.977272727272727,
      "grad_norm": 0.08215363323688507,
      "learning_rate": 0.0004835691236398358,
      "loss": 2.2302,
      "step": 10990
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.26991817355155945,
      "learning_rate": 0.0004835369650662767,
      "loss": 2.2151,
      "step": 11000
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.1049702167510986,
      "eval_runtime": 8.7044,
      "eval_samples_per_second": 3496.028,
      "eval_steps_per_second": 13.671,
      "step": 11000
    },
    {
      "epoch": 25.022727272727273,
      "grad_norm": 0.0919797345995903,
      "learning_rate": 0.0004835047761244765,
      "loss": 2.2142,
      "step": 11010
    },
    {
      "epoch": 25.045454545454547,
      "grad_norm": 0.1198660358786583,
      "learning_rate": 0.0004834725568186208,
      "loss": 2.2289,
      "step": 11020
    },
    {
      "epoch": 25.068181818181817,
      "grad_norm": 0.0846722275018692,
      "learning_rate": 0.00048344030715289943,
      "loss": 2.2281,
      "step": 11030
    },
    {
      "epoch": 25.09090909090909,
      "grad_norm": 0.10806845873594284,
      "learning_rate": 0.00048340802713150587,
      "loss": 2.2274,
      "step": 11040
    },
    {
      "epoch": 25.113636363636363,
      "grad_norm": 0.09815305471420288,
      "learning_rate": 0.0004833757167586379,
      "loss": 2.2212,
      "step": 11050
    },
    {
      "epoch": 25.136363636363637,
      "grad_norm": 0.08336103707551956,
      "learning_rate": 0.00048334337603849674,
      "loss": 2.2145,
      "step": 11060
    },
    {
      "epoch": 25.15909090909091,
      "grad_norm": 0.16870278120040894,
      "learning_rate": 0.00048331100497528813,
      "loss": 2.2206,
      "step": 11070
    },
    {
      "epoch": 25.181818181818183,
      "grad_norm": 0.08031703531742096,
      "learning_rate": 0.00048327860357322137,
      "loss": 2.2163,
      "step": 11080
    },
    {
      "epoch": 25.204545454545453,
      "grad_norm": 0.1367703080177307,
      "learning_rate": 0.00048324617183650976,
      "loss": 2.2159,
      "step": 11090
    },
    {
      "epoch": 25.227272727272727,
      "grad_norm": 0.15354664623737335,
      "learning_rate": 0.00048321370976937063,
      "loss": 2.2232,
      "step": 11100
    },
    {
      "epoch": 25.25,
      "grad_norm": 0.10788901895284653,
      "learning_rate": 0.00048318121737602523,
      "loss": 2.2402,
      "step": 11110
    },
    {
      "epoch": 25.272727272727273,
      "grad_norm": 0.08340446650981903,
      "learning_rate": 0.0004831486946606988,
      "loss": 2.2223,
      "step": 11120
    },
    {
      "epoch": 25.295454545454547,
      "grad_norm": 0.10647529363632202,
      "learning_rate": 0.0004831161416276203,
      "loss": 2.2243,
      "step": 11130
    },
    {
      "epoch": 25.318181818181817,
      "grad_norm": 0.13775481283664703,
      "learning_rate": 0.000483083558281023,
      "loss": 2.2178,
      "step": 11140
    },
    {
      "epoch": 25.34090909090909,
      "grad_norm": 0.12245887517929077,
      "learning_rate": 0.00048305094462514374,
      "loss": 2.2298,
      "step": 11150
    },
    {
      "epoch": 25.363636363636363,
      "grad_norm": 0.1299336850643158,
      "learning_rate": 0.0004830183006642237,
      "loss": 2.2239,
      "step": 11160
    },
    {
      "epoch": 25.386363636363637,
      "grad_norm": 0.14215341210365295,
      "learning_rate": 0.0004829856264025075,
      "loss": 2.2226,
      "step": 11170
    },
    {
      "epoch": 25.40909090909091,
      "grad_norm": 0.07439545542001724,
      "learning_rate": 0.0004829529218442442,
      "loss": 2.2108,
      "step": 11180
    },
    {
      "epoch": 25.431818181818183,
      "grad_norm": 0.08227454870939255,
      "learning_rate": 0.00048292018699368645,
      "loss": 2.2238,
      "step": 11190
    },
    {
      "epoch": 25.454545454545453,
      "grad_norm": 0.08637697994709015,
      "learning_rate": 0.00048288742185509094,
      "loss": 2.2362,
      "step": 11200
    },
    {
      "epoch": 25.477272727272727,
      "grad_norm": 0.07610106468200684,
      "learning_rate": 0.0004828546264327184,
      "loss": 2.2209,
      "step": 11210
    },
    {
      "epoch": 25.5,
      "grad_norm": 0.0915134996175766,
      "learning_rate": 0.0004828218007308335,
      "loss": 2.2224,
      "step": 11220
    },
    {
      "epoch": 25.522727272727273,
      "grad_norm": 0.11176348477602005,
      "learning_rate": 0.0004827889447537045,
      "loss": 2.2176,
      "step": 11230
    },
    {
      "epoch": 25.545454545454547,
      "grad_norm": 0.11185020208358765,
      "learning_rate": 0.0004827560585056041,
      "loss": 2.2208,
      "step": 11240
    },
    {
      "epoch": 25.568181818181817,
      "grad_norm": 0.09174428880214691,
      "learning_rate": 0.0004827231419908087,
      "loss": 2.2225,
      "step": 11250
    },
    {
      "epoch": 25.59090909090909,
      "grad_norm": 0.10205913335084915,
      "learning_rate": 0.00048269019521359846,
      "loss": 2.2138,
      "step": 11260
    },
    {
      "epoch": 25.613636363636363,
      "grad_norm": 0.08549337089061737,
      "learning_rate": 0.0004826572181782578,
      "loss": 2.2259,
      "step": 11270
    },
    {
      "epoch": 25.636363636363637,
      "grad_norm": 0.098369300365448,
      "learning_rate": 0.00048262421088907483,
      "loss": 2.22,
      "step": 11280
    },
    {
      "epoch": 25.65909090909091,
      "grad_norm": 0.08641871064901352,
      "learning_rate": 0.0004825911733503418,
      "loss": 2.2222,
      "step": 11290
    },
    {
      "epoch": 25.681818181818183,
      "grad_norm": 0.10674278438091278,
      "learning_rate": 0.00048255810556635466,
      "loss": 2.2298,
      "step": 11300
    },
    {
      "epoch": 25.704545454545453,
      "grad_norm": 0.1540704369544983,
      "learning_rate": 0.0004825250075414135,
      "loss": 2.2322,
      "step": 11310
    },
    {
      "epoch": 25.727272727272727,
      "grad_norm": 0.14924761652946472,
      "learning_rate": 0.0004824918792798222,
      "loss": 2.2286,
      "step": 11320
    },
    {
      "epoch": 25.75,
      "grad_norm": 0.11890876293182373,
      "learning_rate": 0.0004824587207858888,
      "loss": 2.2185,
      "step": 11330
    },
    {
      "epoch": 25.772727272727273,
      "grad_norm": 0.16041268408298492,
      "learning_rate": 0.0004824255320639248,
      "loss": 2.2164,
      "step": 11340
    },
    {
      "epoch": 25.795454545454547,
      "grad_norm": 0.34495213627815247,
      "learning_rate": 0.00048239231311824623,
      "loss": 2.2286,
      "step": 11350
    },
    {
      "epoch": 25.818181818181817,
      "grad_norm": 0.100338876247406,
      "learning_rate": 0.0004823590639531726,
      "loss": 2.2285,
      "step": 11360
    },
    {
      "epoch": 25.84090909090909,
      "grad_norm": 0.10647674649953842,
      "learning_rate": 0.00048232578457302745,
      "loss": 2.226,
      "step": 11370
    },
    {
      "epoch": 25.863636363636363,
      "grad_norm": 0.37073633074760437,
      "learning_rate": 0.00048229247498213847,
      "loss": 2.2194,
      "step": 11380
    },
    {
      "epoch": 25.886363636363637,
      "grad_norm": 0.10367618501186371,
      "learning_rate": 0.000482259135184837,
      "loss": 2.2159,
      "step": 11390
    },
    {
      "epoch": 25.90909090909091,
      "grad_norm": 0.1379409283399582,
      "learning_rate": 0.0004822257651854584,
      "loss": 2.2277,
      "step": 11400
    },
    {
      "epoch": 25.931818181818183,
      "grad_norm": 0.12656299769878387,
      "learning_rate": 0.00048219236498834206,
      "loss": 2.214,
      "step": 11410
    },
    {
      "epoch": 25.954545454545453,
      "grad_norm": 0.08724158257246017,
      "learning_rate": 0.0004821589345978311,
      "loss": 2.2241,
      "step": 11420
    },
    {
      "epoch": 25.977272727272727,
      "grad_norm": 0.20233626663684845,
      "learning_rate": 0.0004821254740182728,
      "loss": 2.2215,
      "step": 11430
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.22677844762802124,
      "learning_rate": 0.00048209198325401817,
      "loss": 2.2257,
      "step": 11440
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.1043039560317993,
      "eval_runtime": 8.8723,
      "eval_samples_per_second": 3429.898,
      "eval_steps_per_second": 13.413,
      "step": 11440
    },
    {
      "epoch": 26.022727272727273,
      "grad_norm": 0.15717650949954987,
      "learning_rate": 0.00048205846230942224,
      "loss": 2.2451,
      "step": 11450
    },
    {
      "epoch": 26.045454545454547,
      "grad_norm": 0.15008074045181274,
      "learning_rate": 0.00048202491118884395,
      "loss": 2.2224,
      "step": 11460
    },
    {
      "epoch": 26.068181818181817,
      "grad_norm": 0.16177015006542206,
      "learning_rate": 0.00048199132989664605,
      "loss": 2.2156,
      "step": 11470
    },
    {
      "epoch": 26.09090909090909,
      "grad_norm": 0.19708672165870667,
      "learning_rate": 0.00048195771843719557,
      "loss": 2.2201,
      "step": 11480
    },
    {
      "epoch": 26.113636363636363,
      "grad_norm": 0.14017702639102936,
      "learning_rate": 0.00048192407681486296,
      "loss": 2.2283,
      "step": 11490
    },
    {
      "epoch": 26.136363636363637,
      "grad_norm": 0.12477629631757736,
      "learning_rate": 0.000481890405034023,
      "loss": 2.2185,
      "step": 11500
    },
    {
      "epoch": 26.15909090909091,
      "grad_norm": 0.3434741199016571,
      "learning_rate": 0.00048185670309905415,
      "loss": 2.2079,
      "step": 11510
    },
    {
      "epoch": 26.181818181818183,
      "grad_norm": 0.12779687345027924,
      "learning_rate": 0.0004818229710143389,
      "loss": 2.2271,
      "step": 11520
    },
    {
      "epoch": 26.204545454545453,
      "grad_norm": 0.25194627046585083,
      "learning_rate": 0.0004817892087842637,
      "loss": 2.2086,
      "step": 11530
    },
    {
      "epoch": 26.227272727272727,
      "grad_norm": 0.16984809935092926,
      "learning_rate": 0.0004817554164132188,
      "loss": 2.2284,
      "step": 11540
    },
    {
      "epoch": 26.25,
      "grad_norm": 0.12021373957395554,
      "learning_rate": 0.0004817215939055984,
      "loss": 2.2173,
      "step": 11550
    },
    {
      "epoch": 26.272727272727273,
      "grad_norm": 0.1300232708454132,
      "learning_rate": 0.0004816877412658007,
      "loss": 2.2218,
      "step": 11560
    },
    {
      "epoch": 26.295454545454547,
      "grad_norm": 0.14074790477752686,
      "learning_rate": 0.0004816538584982279,
      "loss": 2.2275,
      "step": 11570
    },
    {
      "epoch": 26.318181818181817,
      "grad_norm": 0.1869553029537201,
      "learning_rate": 0.00048161994560728564,
      "loss": 2.2214,
      "step": 11580
    },
    {
      "epoch": 26.34090909090909,
      "grad_norm": 0.13667377829551697,
      "learning_rate": 0.0004815860025973841,
      "loss": 2.2354,
      "step": 11590
    },
    {
      "epoch": 26.363636363636363,
      "grad_norm": 0.11238150298595428,
      "learning_rate": 0.000481552029472937,
      "loss": 2.2162,
      "step": 11600
    },
    {
      "epoch": 26.386363636363637,
      "grad_norm": 0.14550326764583588,
      "learning_rate": 0.0004815180262383622,
      "loss": 2.228,
      "step": 11610
    },
    {
      "epoch": 26.40909090909091,
      "grad_norm": 0.14686572551727295,
      "learning_rate": 0.0004814839928980811,
      "loss": 2.2078,
      "step": 11620
    },
    {
      "epoch": 26.431818181818183,
      "grad_norm": 0.18983256816864014,
      "learning_rate": 0.00048144992945651946,
      "loss": 2.2296,
      "step": 11630
    },
    {
      "epoch": 26.454545454545453,
      "grad_norm": 0.16167792677879333,
      "learning_rate": 0.00048141583591810666,
      "loss": 2.224,
      "step": 11640
    },
    {
      "epoch": 26.477272727272727,
      "grad_norm": 0.13077537715435028,
      "learning_rate": 0.00048138171228727616,
      "loss": 2.2292,
      "step": 11650
    },
    {
      "epoch": 26.5,
      "grad_norm": 0.1352328658103943,
      "learning_rate": 0.0004813475585684652,
      "loss": 2.2259,
      "step": 11660
    },
    {
      "epoch": 26.522727272727273,
      "grad_norm": 0.1271347999572754,
      "learning_rate": 0.00048131337476611515,
      "loss": 2.2279,
      "step": 11670
    },
    {
      "epoch": 26.545454545454547,
      "grad_norm": 0.1349347084760666,
      "learning_rate": 0.00048127916088467097,
      "loss": 2.2103,
      "step": 11680
    },
    {
      "epoch": 26.568181818181817,
      "grad_norm": 0.1963837593793869,
      "learning_rate": 0.00048124491692858176,
      "loss": 2.2135,
      "step": 11690
    },
    {
      "epoch": 26.59090909090909,
      "grad_norm": 0.19481483101844788,
      "learning_rate": 0.00048121064290230044,
      "loss": 2.2204,
      "step": 11700
    },
    {
      "epoch": 26.613636363636363,
      "grad_norm": 0.13544423878192902,
      "learning_rate": 0.0004811763388102839,
      "loss": 2.2262,
      "step": 11710
    },
    {
      "epoch": 26.636363636363637,
      "grad_norm": 0.11586295068264008,
      "learning_rate": 0.0004811420046569929,
      "loss": 2.2151,
      "step": 11720
    },
    {
      "epoch": 26.65909090909091,
      "grad_norm": 0.16078251600265503,
      "learning_rate": 0.00048110764044689207,
      "loss": 2.2188,
      "step": 11730
    },
    {
      "epoch": 26.681818181818183,
      "grad_norm": 0.1401132345199585,
      "learning_rate": 0.00048107324618445015,
      "loss": 2.2263,
      "step": 11740
    },
    {
      "epoch": 26.704545454545453,
      "grad_norm": 0.17522230744361877,
      "learning_rate": 0.0004810388218741395,
      "loss": 2.2302,
      "step": 11750
    },
    {
      "epoch": 26.727272727272727,
      "grad_norm": 0.1559014618396759,
      "learning_rate": 0.0004810043675204366,
      "loss": 2.2208,
      "step": 11760
    },
    {
      "epoch": 26.75,
      "grad_norm": 0.17005793750286102,
      "learning_rate": 0.0004809698831278217,
      "loss": 2.2278,
      "step": 11770
    },
    {
      "epoch": 26.772727272727273,
      "grad_norm": 0.32799825072288513,
      "learning_rate": 0.00048093536870077904,
      "loss": 2.2326,
      "step": 11780
    },
    {
      "epoch": 26.795454545454547,
      "grad_norm": 0.1800692081451416,
      "learning_rate": 0.0004809008242437968,
      "loss": 2.2295,
      "step": 11790
    },
    {
      "epoch": 26.818181818181817,
      "grad_norm": 0.15286827087402344,
      "learning_rate": 0.0004808662497613669,
      "loss": 2.2079,
      "step": 11800
    },
    {
      "epoch": 26.84090909090909,
      "grad_norm": 0.13910843431949615,
      "learning_rate": 0.00048083164525798526,
      "loss": 2.2212,
      "step": 11810
    },
    {
      "epoch": 26.863636363636363,
      "grad_norm": 0.11691240221261978,
      "learning_rate": 0.0004807970107381519,
      "loss": 2.2159,
      "step": 11820
    },
    {
      "epoch": 26.886363636363637,
      "grad_norm": 0.1274835765361786,
      "learning_rate": 0.0004807623462063704,
      "loss": 2.2227,
      "step": 11830
    },
    {
      "epoch": 26.90909090909091,
      "grad_norm": 0.11780469119548798,
      "learning_rate": 0.00048072765166714843,
      "loss": 2.2279,
      "step": 11840
    },
    {
      "epoch": 26.931818181818183,
      "grad_norm": 0.1810874193906784,
      "learning_rate": 0.00048069292712499755,
      "loss": 2.2252,
      "step": 11850
    },
    {
      "epoch": 26.954545454545453,
      "grad_norm": 0.15063005685806274,
      "learning_rate": 0.00048065817258443325,
      "loss": 2.225,
      "step": 11860
    },
    {
      "epoch": 26.977272727272727,
      "grad_norm": 0.33087003231048584,
      "learning_rate": 0.00048062338804997476,
      "loss": 2.2269,
      "step": 11870
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.3072221577167511,
      "learning_rate": 0.00048058857352614536,
      "loss": 2.2258,
      "step": 11880
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.1040197610855103,
      "eval_runtime": 8.7265,
      "eval_samples_per_second": 3487.196,
      "eval_steps_per_second": 13.637,
      "step": 11880
    },
    {
      "epoch": 27.022727272727273,
      "grad_norm": 0.18042469024658203,
      "learning_rate": 0.0004805537290174723,
      "loss": 2.1991,
      "step": 11890
    },
    {
      "epoch": 27.045454545454547,
      "grad_norm": 0.14919787645339966,
      "learning_rate": 0.0004805188545284865,
      "loss": 2.2373,
      "step": 11900
    },
    {
      "epoch": 27.068181818181817,
      "grad_norm": 0.14465723931789398,
      "learning_rate": 0.0004804839500637229,
      "loss": 2.2293,
      "step": 11910
    },
    {
      "epoch": 27.09090909090909,
      "grad_norm": 0.29914242029190063,
      "learning_rate": 0.00048044901562772045,
      "loss": 2.2212,
      "step": 11920
    },
    {
      "epoch": 27.113636363636363,
      "grad_norm": 0.16650880873203278,
      "learning_rate": 0.0004804140512250219,
      "loss": 2.2166,
      "step": 11930
    },
    {
      "epoch": 27.136363636363637,
      "grad_norm": 0.15203045308589935,
      "learning_rate": 0.0004803790568601738,
      "loss": 2.2194,
      "step": 11940
    },
    {
      "epoch": 27.15909090909091,
      "grad_norm": 0.10800284892320633,
      "learning_rate": 0.00048034403253772664,
      "loss": 2.2112,
      "step": 11950
    },
    {
      "epoch": 27.181818181818183,
      "grad_norm": 0.21006304025650024,
      "learning_rate": 0.000480308978262235,
      "loss": 2.2185,
      "step": 11960
    },
    {
      "epoch": 27.204545454545453,
      "grad_norm": 0.17893002927303314,
      "learning_rate": 0.0004802738940382569,
      "loss": 2.2201,
      "step": 11970
    },
    {
      "epoch": 27.227272727272727,
      "grad_norm": 0.1608896702528,
      "learning_rate": 0.000480238779870355,
      "loss": 2.2152,
      "step": 11980
    },
    {
      "epoch": 27.25,
      "grad_norm": 0.17281852662563324,
      "learning_rate": 0.0004802036357630951,
      "loss": 2.2322,
      "step": 11990
    },
    {
      "epoch": 27.272727272727273,
      "grad_norm": 0.19832909107208252,
      "learning_rate": 0.00048016846172104725,
      "loss": 2.2234,
      "step": 12000
    },
    {
      "epoch": 27.295454545454547,
      "grad_norm": 0.28331583738327026,
      "learning_rate": 0.00048013325774878536,
      "loss": 2.2199,
      "step": 12010
    },
    {
      "epoch": 27.318181818181817,
      "grad_norm": 0.1548241823911667,
      "learning_rate": 0.0004800980238508873,
      "loss": 2.224,
      "step": 12020
    },
    {
      "epoch": 27.34090909090909,
      "grad_norm": 0.14297743141651154,
      "learning_rate": 0.0004800627600319346,
      "loss": 2.2167,
      "step": 12030
    },
    {
      "epoch": 27.363636363636363,
      "grad_norm": 0.11704336851835251,
      "learning_rate": 0.00048002746629651296,
      "loss": 2.227,
      "step": 12040
    },
    {
      "epoch": 27.386363636363637,
      "grad_norm": 0.17593595385551453,
      "learning_rate": 0.00047999214264921183,
      "loss": 2.2267,
      "step": 12050
    },
    {
      "epoch": 27.40909090909091,
      "grad_norm": 0.1535317301750183,
      "learning_rate": 0.00047995678909462446,
      "loss": 2.2077,
      "step": 12060
    },
    {
      "epoch": 27.431818181818183,
      "grad_norm": 0.1382141411304474,
      "learning_rate": 0.0004799214056373482,
      "loss": 2.2147,
      "step": 12070
    },
    {
      "epoch": 27.454545454545453,
      "grad_norm": 0.22253800928592682,
      "learning_rate": 0.00047988599228198406,
      "loss": 2.2292,
      "step": 12080
    },
    {
      "epoch": 27.477272727272727,
      "grad_norm": 0.15959742665290833,
      "learning_rate": 0.0004798505490331372,
      "loss": 2.225,
      "step": 12090
    },
    {
      "epoch": 27.5,
      "grad_norm": 0.09729167819023132,
      "learning_rate": 0.0004798150758954164,
      "loss": 2.2248,
      "step": 12100
    },
    {
      "epoch": 27.522727272727273,
      "grad_norm": 0.13611365854740143,
      "learning_rate": 0.00047977957287343456,
      "loss": 2.2284,
      "step": 12110
    },
    {
      "epoch": 27.545454545454547,
      "grad_norm": 0.13539555668830872,
      "learning_rate": 0.0004797440399718082,
      "loss": 2.2124,
      "step": 12120
    },
    {
      "epoch": 27.568181818181817,
      "grad_norm": 0.15569640696048737,
      "learning_rate": 0.00047970847719515803,
      "loss": 2.2328,
      "step": 12130
    },
    {
      "epoch": 27.59090909090909,
      "grad_norm": 0.1447673738002777,
      "learning_rate": 0.0004796728845481084,
      "loss": 2.2268,
      "step": 12140
    },
    {
      "epoch": 27.613636363636363,
      "grad_norm": 0.19746115803718567,
      "learning_rate": 0.00047963726203528777,
      "loss": 2.2178,
      "step": 12150
    },
    {
      "epoch": 27.636363636363637,
      "grad_norm": 0.11971171945333481,
      "learning_rate": 0.00047960160966132817,
      "loss": 2.2159,
      "step": 12160
    },
    {
      "epoch": 27.65909090909091,
      "grad_norm": 0.10948918759822845,
      "learning_rate": 0.0004795659274308658,
      "loss": 2.224,
      "step": 12170
    },
    {
      "epoch": 27.681818181818183,
      "grad_norm": 0.3062826097011566,
      "learning_rate": 0.0004795302153485407,
      "loss": 2.2155,
      "step": 12180
    },
    {
      "epoch": 27.704545454545453,
      "grad_norm": 0.10955965518951416,
      "learning_rate": 0.0004794944734189966,
      "loss": 2.2222,
      "step": 12190
    },
    {
      "epoch": 27.727272727272727,
      "grad_norm": 0.11861970275640488,
      "learning_rate": 0.0004794587016468813,
      "loss": 2.2198,
      "step": 12200
    },
    {
      "epoch": 27.75,
      "grad_norm": 0.13133284449577332,
      "learning_rate": 0.0004794229000368464,
      "loss": 2.2273,
      "step": 12210
    },
    {
      "epoch": 27.772727272727273,
      "grad_norm": 0.11628880351781845,
      "learning_rate": 0.00047938706859354746,
      "loss": 2.2239,
      "step": 12220
    },
    {
      "epoch": 27.795454545454547,
      "grad_norm": 0.21211262047290802,
      "learning_rate": 0.00047935120732164383,
      "loss": 2.2027,
      "step": 12230
    },
    {
      "epoch": 27.818181818181817,
      "grad_norm": 0.13472390174865723,
      "learning_rate": 0.00047931531622579876,
      "loss": 2.221,
      "step": 12240
    },
    {
      "epoch": 27.84090909090909,
      "grad_norm": 0.15057943761348724,
      "learning_rate": 0.0004792793953106794,
      "loss": 2.2243,
      "step": 12250
    },
    {
      "epoch": 27.863636363636363,
      "grad_norm": 0.12262997031211853,
      "learning_rate": 0.00047924344458095676,
      "loss": 2.2316,
      "step": 12260
    },
    {
      "epoch": 27.886363636363637,
      "grad_norm": 0.19213663041591644,
      "learning_rate": 0.0004792074640413057,
      "loss": 2.2264,
      "step": 12270
    },
    {
      "epoch": 27.90909090909091,
      "grad_norm": 0.17774005234241486,
      "learning_rate": 0.00047917145369640507,
      "loss": 2.2199,
      "step": 12280
    },
    {
      "epoch": 27.931818181818183,
      "grad_norm": 0.1374680995941162,
      "learning_rate": 0.00047913541355093747,
      "loss": 2.2241,
      "step": 12290
    },
    {
      "epoch": 27.954545454545453,
      "grad_norm": 0.17501194775104523,
      "learning_rate": 0.0004790993436095894,
      "loss": 2.2101,
      "step": 12300
    },
    {
      "epoch": 27.977272727272727,
      "grad_norm": 0.14803792536258698,
      "learning_rate": 0.0004790632438770513,
      "loss": 2.2236,
      "step": 12310
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.3821124732494354,
      "learning_rate": 0.0004790271143580174,
      "loss": 2.2281,
      "step": 12320
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.1040915250778198,
      "eval_runtime": 8.9399,
      "eval_samples_per_second": 3403.943,
      "eval_steps_per_second": 13.311,
      "step": 12320
    },
    {
      "epoch": 28.022727272727273,
      "grad_norm": 0.16410204768180847,
      "learning_rate": 0.0004789909550571859,
      "loss": 2.2244,
      "step": 12330
    },
    {
      "epoch": 28.045454545454547,
      "grad_norm": 0.1535426527261734,
      "learning_rate": 0.00047895476597925867,
      "loss": 2.2214,
      "step": 12340
    },
    {
      "epoch": 28.068181818181817,
      "grad_norm": 0.10995708405971527,
      "learning_rate": 0.00047891854712894176,
      "loss": 2.2203,
      "step": 12350
    },
    {
      "epoch": 28.09090909090909,
      "grad_norm": 0.11456923931837082,
      "learning_rate": 0.00047888229851094494,
      "loss": 2.2243,
      "step": 12360
    },
    {
      "epoch": 28.113636363636363,
      "grad_norm": 0.10890410095453262,
      "learning_rate": 0.0004788460201299817,
      "loss": 2.2102,
      "step": 12370
    },
    {
      "epoch": 28.136363636363637,
      "grad_norm": 0.12518884241580963,
      "learning_rate": 0.00047880971199076963,
      "loss": 2.221,
      "step": 12380
    },
    {
      "epoch": 28.15909090909091,
      "grad_norm": 0.13665029406547546,
      "learning_rate": 0.0004787733740980301,
      "loss": 2.2177,
      "step": 12390
    },
    {
      "epoch": 28.181818181818183,
      "grad_norm": 0.1785934567451477,
      "learning_rate": 0.0004787370064564883,
      "loss": 2.2095,
      "step": 12400
    },
    {
      "epoch": 28.204545454545453,
      "grad_norm": 0.1360820233821869,
      "learning_rate": 0.00047870060907087336,
      "loss": 2.2131,
      "step": 12410
    },
    {
      "epoch": 28.227272727272727,
      "grad_norm": 0.209110289812088,
      "learning_rate": 0.0004786641819459183,
      "loss": 2.2145,
      "step": 12420
    },
    {
      "epoch": 28.25,
      "grad_norm": 0.11372970044612885,
      "learning_rate": 0.0004786277250863599,
      "loss": 2.2186,
      "step": 12430
    },
    {
      "epoch": 28.272727272727273,
      "grad_norm": 0.15720340609550476,
      "learning_rate": 0.0004785912384969389,
      "loss": 2.223,
      "step": 12440
    },
    {
      "epoch": 28.295454545454547,
      "grad_norm": 0.1373753547668457,
      "learning_rate": 0.00047855472218239983,
      "loss": 2.2146,
      "step": 12450
    },
    {
      "epoch": 28.318181818181817,
      "grad_norm": 0.16538986563682556,
      "learning_rate": 0.00047851817614749117,
      "loss": 2.2236,
      "step": 12460
    },
    {
      "epoch": 28.34090909090909,
      "grad_norm": 0.15808576345443726,
      "learning_rate": 0.0004784816003969652,
      "loss": 2.2259,
      "step": 12470
    },
    {
      "epoch": 28.363636363636363,
      "grad_norm": 0.17540322244167328,
      "learning_rate": 0.0004784449949355781,
      "loss": 2.2183,
      "step": 12480
    },
    {
      "epoch": 28.386363636363637,
      "grad_norm": 0.14088532328605652,
      "learning_rate": 0.00047840835976809004,
      "loss": 2.2149,
      "step": 12490
    },
    {
      "epoch": 28.40909090909091,
      "grad_norm": 0.14532552659511566,
      "learning_rate": 0.0004783716948992647,
      "loss": 2.2091,
      "step": 12500
    },
    {
      "epoch": 28.431818181818183,
      "grad_norm": 0.12405043095350266,
      "learning_rate": 0.0004783350003338699,
      "loss": 2.2189,
      "step": 12510
    },
    {
      "epoch": 28.454545454545453,
      "grad_norm": 0.1081024557352066,
      "learning_rate": 0.0004782982760766773,
      "loss": 2.2331,
      "step": 12520
    },
    {
      "epoch": 28.477272727272727,
      "grad_norm": 0.1871241331100464,
      "learning_rate": 0.00047826152213246244,
      "loss": 2.2214,
      "step": 12530
    },
    {
      "epoch": 28.5,
      "grad_norm": 0.10699167102575302,
      "learning_rate": 0.00047822473850600447,
      "loss": 2.2119,
      "step": 12540
    },
    {
      "epoch": 28.522727272727273,
      "grad_norm": 0.12110885232686996,
      "learning_rate": 0.00047818792520208674,
      "loss": 2.2077,
      "step": 12550
    },
    {
      "epoch": 28.545454545454547,
      "grad_norm": 0.14281640946865082,
      "learning_rate": 0.0004781510822254963,
      "loss": 2.2213,
      "step": 12560
    },
    {
      "epoch": 28.568181818181817,
      "grad_norm": 0.14061367511749268,
      "learning_rate": 0.00047811420958102405,
      "loss": 2.2221,
      "step": 12570
    },
    {
      "epoch": 28.59090909090909,
      "grad_norm": 0.1254298985004425,
      "learning_rate": 0.00047807730727346463,
      "loss": 2.21,
      "step": 12580
    },
    {
      "epoch": 28.613636363636363,
      "grad_norm": 0.1634952574968338,
      "learning_rate": 0.0004780403753076169,
      "loss": 2.2257,
      "step": 12590
    },
    {
      "epoch": 28.636363636363637,
      "grad_norm": 0.1126706451177597,
      "learning_rate": 0.00047800341368828324,
      "loss": 2.2196,
      "step": 12600
    },
    {
      "epoch": 28.65909090909091,
      "grad_norm": 0.12141941487789154,
      "learning_rate": 0.00047796642242027007,
      "loss": 2.213,
      "step": 12610
    },
    {
      "epoch": 28.681818181818183,
      "grad_norm": 0.23037223517894745,
      "learning_rate": 0.0004779294015083875,
      "loss": 2.225,
      "step": 12620
    },
    {
      "epoch": 28.704545454545453,
      "grad_norm": 0.20958738029003143,
      "learning_rate": 0.0004778923509574495,
      "loss": 2.2192,
      "step": 12630
    },
    {
      "epoch": 28.727272727272727,
      "grad_norm": 0.10056295990943909,
      "learning_rate": 0.0004778552707722742,
      "loss": 2.2242,
      "step": 12640
    },
    {
      "epoch": 28.75,
      "grad_norm": 0.39794743061065674,
      "learning_rate": 0.00047781816095768313,
      "loss": 2.222,
      "step": 12650
    },
    {
      "epoch": 28.772727272727273,
      "grad_norm": 0.12766705453395844,
      "learning_rate": 0.00047778102151850224,
      "loss": 2.2144,
      "step": 12660
    },
    {
      "epoch": 28.795454545454547,
      "grad_norm": 0.18365375697612762,
      "learning_rate": 0.00047774385245956066,
      "loss": 2.2216,
      "step": 12670
    },
    {
      "epoch": 28.818181818181817,
      "grad_norm": 0.15912917256355286,
      "learning_rate": 0.00047770665378569176,
      "loss": 2.2293,
      "step": 12680
    },
    {
      "epoch": 28.84090909090909,
      "grad_norm": 0.1538427174091339,
      "learning_rate": 0.0004776694255017329,
      "loss": 2.2249,
      "step": 12690
    },
    {
      "epoch": 28.863636363636363,
      "grad_norm": 0.4737115204334259,
      "learning_rate": 0.00047763216761252496,
      "loss": 2.221,
      "step": 12700
    },
    {
      "epoch": 28.886363636363637,
      "grad_norm": 0.21874617040157318,
      "learning_rate": 0.00047759488012291283,
      "loss": 2.2155,
      "step": 12710
    },
    {
      "epoch": 28.90909090909091,
      "grad_norm": 0.14538639783859253,
      "learning_rate": 0.0004775575630377453,
      "loss": 2.2182,
      "step": 12720
    },
    {
      "epoch": 28.931818181818183,
      "grad_norm": 0.11369046568870544,
      "learning_rate": 0.00047752021636187486,
      "loss": 2.2149,
      "step": 12730
    },
    {
      "epoch": 28.954545454545453,
      "grad_norm": 0.16056951880455017,
      "learning_rate": 0.00047748284010015794,
      "loss": 2.2253,
      "step": 12740
    },
    {
      "epoch": 28.977272727272727,
      "grad_norm": 0.14923526346683502,
      "learning_rate": 0.0004774454342574548,
      "loss": 2.2184,
      "step": 12750
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.35039472579956055,
      "learning_rate": 0.0004774079988386296,
      "loss": 2.2382,
      "step": 12760
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.1039929389953613,
      "eval_runtime": 8.682,
      "eval_samples_per_second": 3505.049,
      "eval_steps_per_second": 13.706,
      "step": 12760
    },
    {
      "epoch": 29.022727272727273,
      "grad_norm": 0.18202169239521027,
      "learning_rate": 0.00047737053384855024,
      "loss": 2.2248,
      "step": 12770
    },
    {
      "epoch": 29.045454545454547,
      "grad_norm": 0.26575422286987305,
      "learning_rate": 0.00047733303929208855,
      "loss": 2.2261,
      "step": 12780
    },
    {
      "epoch": 29.068181818181817,
      "grad_norm": 0.1393173485994339,
      "learning_rate": 0.0004772955151741202,
      "loss": 2.224,
      "step": 12790
    },
    {
      "epoch": 29.09090909090909,
      "grad_norm": 0.37304091453552246,
      "learning_rate": 0.0004772579614995246,
      "loss": 2.232,
      "step": 12800
    },
    {
      "epoch": 29.113636363636363,
      "grad_norm": 0.16762080788612366,
      "learning_rate": 0.0004772203782731851,
      "loss": 2.2246,
      "step": 12810
    },
    {
      "epoch": 29.136363636363637,
      "grad_norm": 0.1400037556886673,
      "learning_rate": 0.00047718276549998896,
      "loss": 2.2179,
      "step": 12820
    },
    {
      "epoch": 29.15909090909091,
      "grad_norm": 0.1443175971508026,
      "learning_rate": 0.0004771451231848272,
      "loss": 2.2111,
      "step": 12830
    },
    {
      "epoch": 29.181818181818183,
      "grad_norm": 0.13936279714107513,
      "learning_rate": 0.0004771074513325946,
      "loss": 2.2191,
      "step": 12840
    },
    {
      "epoch": 29.204545454545453,
      "grad_norm": 0.19285425543785095,
      "learning_rate": 0.00047706974994818986,
      "loss": 2.2145,
      "step": 12850
    },
    {
      "epoch": 29.227272727272727,
      "grad_norm": 0.2982826828956604,
      "learning_rate": 0.0004770320190365156,
      "loss": 2.2178,
      "step": 12860
    },
    {
      "epoch": 29.25,
      "grad_norm": 0.16333572566509247,
      "learning_rate": 0.00047699425860247814,
      "loss": 2.2228,
      "step": 12870
    },
    {
      "epoch": 29.272727272727273,
      "grad_norm": 0.14452116191387177,
      "learning_rate": 0.0004769564686509877,
      "loss": 2.223,
      "step": 12880
    },
    {
      "epoch": 29.295454545454547,
      "grad_norm": 0.20233431458473206,
      "learning_rate": 0.00047691864918695837,
      "loss": 2.2333,
      "step": 12890
    },
    {
      "epoch": 29.318181818181817,
      "grad_norm": 0.22259001433849335,
      "learning_rate": 0.00047688080021530797,
      "loss": 2.2149,
      "step": 12900
    },
    {
      "epoch": 29.34090909090909,
      "grad_norm": 0.28106391429901123,
      "learning_rate": 0.0004768429217409584,
      "loss": 2.2062,
      "step": 12910
    },
    {
      "epoch": 29.363636363636363,
      "grad_norm": 0.2373850792646408,
      "learning_rate": 0.00047680501376883504,
      "loss": 2.2191,
      "step": 12920
    },
    {
      "epoch": 29.386363636363637,
      "grad_norm": 0.15818634629249573,
      "learning_rate": 0.0004767670763038675,
      "loss": 2.2199,
      "step": 12930
    },
    {
      "epoch": 29.40909090909091,
      "grad_norm": 0.2194061130285263,
      "learning_rate": 0.0004767291093509888,
      "loss": 2.2094,
      "step": 12940
    },
    {
      "epoch": 29.431818181818183,
      "grad_norm": 0.39439669251441956,
      "learning_rate": 0.0004766911129151361,
      "loss": 2.2143,
      "step": 12950
    },
    {
      "epoch": 29.454545454545453,
      "grad_norm": 0.14412471652030945,
      "learning_rate": 0.0004766530870012504,
      "loss": 2.2153,
      "step": 12960
    },
    {
      "epoch": 29.477272727272727,
      "grad_norm": 0.13012386858463287,
      "learning_rate": 0.0004766150316142763,
      "loss": 2.2179,
      "step": 12970
    },
    {
      "epoch": 29.5,
      "grad_norm": 0.4004356861114502,
      "learning_rate": 0.00047657694675916254,
      "loss": 2.207,
      "step": 12980
    },
    {
      "epoch": 29.522727272727273,
      "grad_norm": 0.41975900530815125,
      "learning_rate": 0.00047653883244086134,
      "loss": 2.2142,
      "step": 12990
    },
    {
      "epoch": 29.545454545454547,
      "grad_norm": 0.2067834734916687,
      "learning_rate": 0.00047650068866432905,
      "loss": 2.2266,
      "step": 13000
    },
    {
      "epoch": 29.568181818181817,
      "grad_norm": 0.22715245187282562,
      "learning_rate": 0.0004764625154345257,
      "loss": 2.2138,
      "step": 13010
    },
    {
      "epoch": 29.59090909090909,
      "grad_norm": 0.2119029462337494,
      "learning_rate": 0.0004764243127564153,
      "loss": 2.2114,
      "step": 13020
    },
    {
      "epoch": 29.613636363636363,
      "grad_norm": 0.2627415060997009,
      "learning_rate": 0.00047638608063496537,
      "loss": 2.2346,
      "step": 13030
    },
    {
      "epoch": 29.636363636363637,
      "grad_norm": 0.22464124858379364,
      "learning_rate": 0.0004763478190751476,
      "loss": 2.2204,
      "step": 13040
    },
    {
      "epoch": 29.65909090909091,
      "grad_norm": 0.17895208299160004,
      "learning_rate": 0.0004763095280819374,
      "loss": 2.2284,
      "step": 13050
    },
    {
      "epoch": 29.681818181818183,
      "grad_norm": 0.21016673743724823,
      "learning_rate": 0.000476271207660314,
      "loss": 2.2177,
      "step": 13060
    },
    {
      "epoch": 29.704545454545453,
      "grad_norm": 0.15086929500102997,
      "learning_rate": 0.0004762328578152603,
      "loss": 2.2257,
      "step": 13070
    },
    {
      "epoch": 29.727272727272727,
      "grad_norm": 0.1915290653705597,
      "learning_rate": 0.0004761944785517632,
      "loss": 2.2079,
      "step": 13080
    },
    {
      "epoch": 29.75,
      "grad_norm": 0.17392392456531525,
      "learning_rate": 0.0004761560698748135,
      "loss": 2.2324,
      "step": 13090
    },
    {
      "epoch": 29.772727272727273,
      "grad_norm": 0.18053413927555084,
      "learning_rate": 0.0004761176317894056,
      "loss": 2.2155,
      "step": 13100
    },
    {
      "epoch": 29.795454545454547,
      "grad_norm": 0.13692381978034973,
      "learning_rate": 0.0004760791643005379,
      "loss": 2.2052,
      "step": 13110
    },
    {
      "epoch": 29.818181818181817,
      "grad_norm": 0.2164851874113083,
      "learning_rate": 0.00047604066741321254,
      "loss": 2.2139,
      "step": 13120
    },
    {
      "epoch": 29.84090909090909,
      "grad_norm": 0.13809235394001007,
      "learning_rate": 0.00047600214113243555,
      "loss": 2.2139,
      "step": 13130
    },
    {
      "epoch": 29.863636363636363,
      "grad_norm": 0.21466630697250366,
      "learning_rate": 0.00047596358546321674,
      "loss": 2.2186,
      "step": 13140
    },
    {
      "epoch": 29.886363636363637,
      "grad_norm": 0.16864201426506042,
      "learning_rate": 0.0004759250004105696,
      "loss": 2.2129,
      "step": 13150
    },
    {
      "epoch": 29.90909090909091,
      "grad_norm": 0.16516026854515076,
      "learning_rate": 0.00047588638597951177,
      "loss": 2.2178,
      "step": 13160
    },
    {
      "epoch": 29.931818181818183,
      "grad_norm": 0.16294069588184357,
      "learning_rate": 0.00047584774217506446,
      "loss": 2.2212,
      "step": 13170
    },
    {
      "epoch": 29.954545454545453,
      "grad_norm": 0.12855073809623718,
      "learning_rate": 0.00047580906900225264,
      "loss": 2.2176,
      "step": 13180
    },
    {
      "epoch": 29.977272727272727,
      "grad_norm": 0.13802383840084076,
      "learning_rate": 0.0004757703664661054,
      "loss": 2.2078,
      "step": 13190
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.6877514719963074,
      "learning_rate": 0.0004757316345716554,
      "loss": 2.2184,
      "step": 13200
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.103177547454834,
      "eval_runtime": 8.9582,
      "eval_samples_per_second": 3396.994,
      "eval_steps_per_second": 13.284,
      "step": 13200
    },
    {
      "epoch": 30.022727272727273,
      "grad_norm": 0.20547284185886383,
      "learning_rate": 0.00047569287332393904,
      "loss": 2.2327,
      "step": 13210
    },
    {
      "epoch": 30.045454545454547,
      "grad_norm": 0.19293667376041412,
      "learning_rate": 0.00047565408272799687,
      "loss": 2.2299,
      "step": 13220
    },
    {
      "epoch": 30.068181818181817,
      "grad_norm": 1.8708668947219849,
      "learning_rate": 0.000475615262788873,
      "loss": 2.2202,
      "step": 13230
    },
    {
      "epoch": 30.09090909090909,
      "grad_norm": 0.16752848029136658,
      "learning_rate": 0.00047557641351161554,
      "loss": 2.2108,
      "step": 13240
    },
    {
      "epoch": 30.113636363636363,
      "grad_norm": 0.25783422589302063,
      "learning_rate": 0.00047553753490127605,
      "loss": 2.2169,
      "step": 13250
    },
    {
      "epoch": 30.136363636363637,
      "grad_norm": 0.22396424412727356,
      "learning_rate": 0.00047549862696291033,
      "loss": 2.2227,
      "step": 13260
    },
    {
      "epoch": 30.15909090909091,
      "grad_norm": 0.18389976024627686,
      "learning_rate": 0.0004754596897015779,
      "loss": 2.201,
      "step": 13270
    },
    {
      "epoch": 30.181818181818183,
      "grad_norm": 0.11895816773176193,
      "learning_rate": 0.00047542072312234174,
      "loss": 2.224,
      "step": 13280
    },
    {
      "epoch": 30.204545454545453,
      "grad_norm": 0.1146247535943985,
      "learning_rate": 0.0004753817272302692,
      "loss": 2.2151,
      "step": 13290
    },
    {
      "epoch": 30.227272727272727,
      "grad_norm": 0.20690427720546722,
      "learning_rate": 0.00047534270203043093,
      "loss": 2.2134,
      "step": 13300
    },
    {
      "epoch": 30.25,
      "grad_norm": 0.17908841371536255,
      "learning_rate": 0.0004753036475279018,
      "loss": 2.2203,
      "step": 13310
    },
    {
      "epoch": 30.272727272727273,
      "grad_norm": 0.13963304460048676,
      "learning_rate": 0.00047526456372776004,
      "loss": 2.2166,
      "step": 13320
    },
    {
      "epoch": 30.295454545454547,
      "grad_norm": 0.17696765065193176,
      "learning_rate": 0.00047522545063508834,
      "loss": 2.2054,
      "step": 13330
    },
    {
      "epoch": 30.318181818181817,
      "grad_norm": 0.1935465931892395,
      "learning_rate": 0.00047518630825497244,
      "loss": 2.2195,
      "step": 13340
    },
    {
      "epoch": 30.34090909090909,
      "grad_norm": 0.1606493443250656,
      "learning_rate": 0.00047514713659250254,
      "loss": 2.2182,
      "step": 13350
    },
    {
      "epoch": 30.363636363636363,
      "grad_norm": 0.260057270526886,
      "learning_rate": 0.0004751079356527722,
      "loss": 2.2078,
      "step": 13360
    },
    {
      "epoch": 30.386363636363637,
      "grad_norm": 0.21993431448936462,
      "learning_rate": 0.000475068705440879,
      "loss": 2.2092,
      "step": 13370
    },
    {
      "epoch": 30.40909090909091,
      "grad_norm": 0.47538137435913086,
      "learning_rate": 0.00047502944596192437,
      "loss": 2.2163,
      "step": 13380
    },
    {
      "epoch": 30.431818181818183,
      "grad_norm": 0.18909071385860443,
      "learning_rate": 0.00047499015722101334,
      "loss": 2.2208,
      "step": 13390
    },
    {
      "epoch": 30.454545454545453,
      "grad_norm": 0.3161681294441223,
      "learning_rate": 0.00047495083922325493,
      "loss": 2.2116,
      "step": 13400
    },
    {
      "epoch": 30.477272727272727,
      "grad_norm": 0.2567230761051178,
      "learning_rate": 0.0004749114919737619,
      "loss": 2.2169,
      "step": 13410
    },
    {
      "epoch": 30.5,
      "grad_norm": 0.606983482837677,
      "learning_rate": 0.0004748721154776508,
      "loss": 2.2193,
      "step": 13420
    },
    {
      "epoch": 30.522727272727273,
      "grad_norm": 0.33614876866340637,
      "learning_rate": 0.000474832709740042,
      "loss": 2.2196,
      "step": 13430
    },
    {
      "epoch": 30.545454545454547,
      "grad_norm": 0.17250461876392365,
      "learning_rate": 0.00047479327476605967,
      "loss": 2.2212,
      "step": 13440
    },
    {
      "epoch": 30.568181818181817,
      "grad_norm": 0.14022181928157806,
      "learning_rate": 0.0004747538105608318,
      "loss": 2.2224,
      "step": 13450
    },
    {
      "epoch": 30.59090909090909,
      "grad_norm": 0.35380756855010986,
      "learning_rate": 0.0004747143171294901,
      "loss": 2.2074,
      "step": 13460
    },
    {
      "epoch": 30.613636363636363,
      "grad_norm": 0.4105065166950226,
      "learning_rate": 0.00047467479447717016,
      "loss": 2.2313,
      "step": 13470
    },
    {
      "epoch": 30.636363636363637,
      "grad_norm": 0.19129885733127594,
      "learning_rate": 0.0004746352426090114,
      "loss": 2.214,
      "step": 13480
    },
    {
      "epoch": 30.65909090909091,
      "grad_norm": 0.17551060020923615,
      "learning_rate": 0.000474595661530157,
      "loss": 2.2101,
      "step": 13490
    },
    {
      "epoch": 30.681818181818183,
      "grad_norm": 0.5442124605178833,
      "learning_rate": 0.00047455605124575384,
      "loss": 2.2255,
      "step": 13500
    },
    {
      "epoch": 30.704545454545453,
      "grad_norm": 0.20130540430545807,
      "learning_rate": 0.0004745164117609528,
      "loss": 2.2127,
      "step": 13510
    },
    {
      "epoch": 30.727272727272727,
      "grad_norm": 0.14750352501869202,
      "learning_rate": 0.0004744767430809083,
      "loss": 2.2154,
      "step": 13520
    },
    {
      "epoch": 30.75,
      "grad_norm": 0.13880345225334167,
      "learning_rate": 0.0004744370452107789,
      "loss": 2.2138,
      "step": 13530
    },
    {
      "epoch": 30.772727272727273,
      "grad_norm": 0.11304409801959991,
      "learning_rate": 0.00047439731815572664,
      "loss": 2.217,
      "step": 13540
    },
    {
      "epoch": 30.795454545454547,
      "grad_norm": 0.1526113748550415,
      "learning_rate": 0.0004743575619209174,
      "loss": 2.2092,
      "step": 13550
    },
    {
      "epoch": 30.818181818181817,
      "grad_norm": 0.2100977897644043,
      "learning_rate": 0.0004743177765115211,
      "loss": 2.2191,
      "step": 13560
    },
    {
      "epoch": 30.84090909090909,
      "grad_norm": 0.14387813210487366,
      "learning_rate": 0.0004742779619327111,
      "loss": 2.2205,
      "step": 13570
    },
    {
      "epoch": 30.863636363636363,
      "grad_norm": 0.26382002234458923,
      "learning_rate": 0.0004742381181896649,
      "loss": 2.2053,
      "step": 13580
    },
    {
      "epoch": 30.886363636363637,
      "grad_norm": 0.12974850833415985,
      "learning_rate": 0.0004741982452875635,
      "loss": 2.2207,
      "step": 13590
    },
    {
      "epoch": 30.90909090909091,
      "grad_norm": 0.13780991733074188,
      "learning_rate": 0.00047415834323159177,
      "loss": 2.221,
      "step": 13600
    },
    {
      "epoch": 30.931818181818183,
      "grad_norm": 0.21967139840126038,
      "learning_rate": 0.00047411841202693865,
      "loss": 2.2221,
      "step": 13610
    },
    {
      "epoch": 30.954545454545453,
      "grad_norm": 0.10284651815891266,
      "learning_rate": 0.0004740784516787965,
      "loss": 2.2103,
      "step": 13620
    },
    {
      "epoch": 30.977272727272727,
      "grad_norm": 0.09966116398572922,
      "learning_rate": 0.00047403846219236156,
      "loss": 2.2147,
      "step": 13630
    },
    {
      "epoch": 31.0,
      "grad_norm": 0.34873872995376587,
      "learning_rate": 0.00047399844357283395,
      "loss": 2.204,
      "step": 13640
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.1031310558319092,
      "eval_runtime": 8.7084,
      "eval_samples_per_second": 3494.447,
      "eval_steps_per_second": 13.665,
      "step": 13640
    },
    {
      "epoch": 31.022727272727273,
      "grad_norm": 0.23283465206623077,
      "learning_rate": 0.00047395839582541755,
      "loss": 2.2086,
      "step": 13650
    },
    {
      "epoch": 31.045454545454547,
      "grad_norm": 0.20036831498146057,
      "learning_rate": 0.00047391831895532004,
      "loss": 2.208,
      "step": 13660
    },
    {
      "epoch": 31.068181818181817,
      "grad_norm": 0.1326926350593567,
      "learning_rate": 0.00047387821296775287,
      "loss": 2.2227,
      "step": 13670
    },
    {
      "epoch": 31.09090909090909,
      "grad_norm": 0.1787528693675995,
      "learning_rate": 0.00047383807786793115,
      "loss": 2.2174,
      "step": 13680
    },
    {
      "epoch": 31.113636363636363,
      "grad_norm": 0.11294510960578918,
      "learning_rate": 0.00047379791366107397,
      "loss": 2.2134,
      "step": 13690
    },
    {
      "epoch": 31.136363636363637,
      "grad_norm": 0.12894552946090698,
      "learning_rate": 0.0004737577203524042,
      "loss": 2.2165,
      "step": 13700
    },
    {
      "epoch": 31.15909090909091,
      "grad_norm": 0.12725771963596344,
      "learning_rate": 0.00047371749794714827,
      "loss": 2.2208,
      "step": 13710
    },
    {
      "epoch": 31.181818181818183,
      "grad_norm": 0.14532437920570374,
      "learning_rate": 0.0004736772464505366,
      "loss": 2.2166,
      "step": 13720
    },
    {
      "epoch": 31.204545454545453,
      "grad_norm": 0.16538576781749725,
      "learning_rate": 0.0004736369658678034,
      "loss": 2.2154,
      "step": 13730
    },
    {
      "epoch": 31.227272727272727,
      "grad_norm": 0.12339431047439575,
      "learning_rate": 0.0004735966562041866,
      "loss": 2.2194,
      "step": 13740
    },
    {
      "epoch": 31.25,
      "grad_norm": 0.13688771426677704,
      "learning_rate": 0.00047355631746492786,
      "loss": 2.2192,
      "step": 13750
    },
    {
      "epoch": 31.272727272727273,
      "grad_norm": 0.1225208267569542,
      "learning_rate": 0.0004735159496552727,
      "loss": 2.2117,
      "step": 13760
    },
    {
      "epoch": 31.295454545454547,
      "grad_norm": 0.1436198204755783,
      "learning_rate": 0.00047347555278047027,
      "loss": 2.2161,
      "step": 13770
    },
    {
      "epoch": 31.318181818181817,
      "grad_norm": 0.1123238056898117,
      "learning_rate": 0.0004734351268457738,
      "loss": 2.2128,
      "step": 13780
    },
    {
      "epoch": 31.34090909090909,
      "grad_norm": 0.4958436191082001,
      "learning_rate": 0.00047339467185644014,
      "loss": 2.219,
      "step": 13790
    },
    {
      "epoch": 31.363636363636363,
      "grad_norm": 0.1613837331533432,
      "learning_rate": 0.0004733541878177298,
      "loss": 2.2129,
      "step": 13800
    },
    {
      "epoch": 31.386363636363637,
      "grad_norm": 0.17416803538799286,
      "learning_rate": 0.00047331367473490705,
      "loss": 2.2136,
      "step": 13810
    },
    {
      "epoch": 31.40909090909091,
      "grad_norm": 0.14108015596866608,
      "learning_rate": 0.0004732731326132403,
      "loss": 2.212,
      "step": 13820
    },
    {
      "epoch": 31.431818181818183,
      "grad_norm": 0.15943869948387146,
      "learning_rate": 0.0004732325614580013,
      "loss": 2.2063,
      "step": 13830
    },
    {
      "epoch": 31.454545454545453,
      "grad_norm": 0.11807502061128616,
      "learning_rate": 0.00047319196127446593,
      "loss": 2.2182,
      "step": 13840
    },
    {
      "epoch": 31.477272727272727,
      "grad_norm": 0.14528082311153412,
      "learning_rate": 0.00047315133206791353,
      "loss": 2.2185,
      "step": 13850
    },
    {
      "epoch": 31.5,
      "grad_norm": 0.13927066326141357,
      "learning_rate": 0.0004731106738436275,
      "loss": 2.2195,
      "step": 13860
    },
    {
      "epoch": 31.522727272727273,
      "grad_norm": 0.12771566212177277,
      "learning_rate": 0.0004730699866068947,
      "loss": 2.2152,
      "step": 13870
    },
    {
      "epoch": 31.545454545454547,
      "grad_norm": 0.14147068560123444,
      "learning_rate": 0.00047302927036300615,
      "loss": 2.2234,
      "step": 13880
    },
    {
      "epoch": 31.568181818181817,
      "grad_norm": 0.18253658711910248,
      "learning_rate": 0.0004729885251172563,
      "loss": 2.2153,
      "step": 13890
    },
    {
      "epoch": 31.59090909090909,
      "grad_norm": 0.15069392323493958,
      "learning_rate": 0.00047294775087494353,
      "loss": 2.2286,
      "step": 13900
    },
    {
      "epoch": 31.613636363636363,
      "grad_norm": 0.1519145965576172,
      "learning_rate": 0.00047290694764136993,
      "loss": 2.213,
      "step": 13910
    },
    {
      "epoch": 31.636363636363637,
      "grad_norm": 0.3210088610649109,
      "learning_rate": 0.0004728661154218414,
      "loss": 2.2177,
      "step": 13920
    },
    {
      "epoch": 31.65909090909091,
      "grad_norm": 0.13562527298927307,
      "learning_rate": 0.00047282525422166777,
      "loss": 2.2307,
      "step": 13930
    },
    {
      "epoch": 31.681818181818183,
      "grad_norm": 0.11495522409677505,
      "learning_rate": 0.0004727843640461622,
      "loss": 2.2195,
      "step": 13940
    },
    {
      "epoch": 31.704545454545453,
      "grad_norm": 0.15243013203144073,
      "learning_rate": 0.0004727434449006421,
      "loss": 2.2174,
      "step": 13950
    },
    {
      "epoch": 31.727272727272727,
      "grad_norm": 0.16331589221954346,
      "learning_rate": 0.00047270249679042843,
      "loss": 2.2196,
      "step": 13960
    },
    {
      "epoch": 31.75,
      "grad_norm": 0.21261461079120636,
      "learning_rate": 0.0004726615197208457,
      "loss": 2.2192,
      "step": 13970
    },
    {
      "epoch": 31.772727272727273,
      "grad_norm": 0.1428900808095932,
      "learning_rate": 0.0004726205136972227,
      "loss": 2.2028,
      "step": 13980
    },
    {
      "epoch": 31.795454545454547,
      "grad_norm": 0.1704503446817398,
      "learning_rate": 0.0004725794787248915,
      "loss": 2.2114,
      "step": 13990
    },
    {
      "epoch": 31.818181818181817,
      "grad_norm": 0.22073408961296082,
      "learning_rate": 0.0004725384148091882,
      "loss": 2.2044,
      "step": 14000
    },
    {
      "epoch": 31.84090909090909,
      "grad_norm": 0.18536357581615448,
      "learning_rate": 0.0004724973219554526,
      "loss": 2.2216,
      "step": 14010
    },
    {
      "epoch": 31.863636363636363,
      "grad_norm": 0.131465882062912,
      "learning_rate": 0.00047245620016902817,
      "loss": 2.2175,
      "step": 14020
    },
    {
      "epoch": 31.886363636363637,
      "grad_norm": 0.15683041512966156,
      "learning_rate": 0.0004724150494552624,
      "loss": 2.2087,
      "step": 14030
    },
    {
      "epoch": 31.90909090909091,
      "grad_norm": 0.19853156805038452,
      "learning_rate": 0.00047237386981950613,
      "loss": 2.2179,
      "step": 14040
    },
    {
      "epoch": 31.931818181818183,
      "grad_norm": 0.15043309330940247,
      "learning_rate": 0.00047233266126711437,
      "loss": 2.2202,
      "step": 14050
    },
    {
      "epoch": 31.954545454545453,
      "grad_norm": 0.16776102781295776,
      "learning_rate": 0.00047229142380344574,
      "loss": 2.2249,
      "step": 14060
    },
    {
      "epoch": 31.977272727272727,
      "grad_norm": 0.16249647736549377,
      "learning_rate": 0.00047225015743386256,
      "loss": 2.2212,
      "step": 14070
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.35558274388313293,
      "learning_rate": 0.0004722088621637309,
      "loss": 2.2122,
      "step": 14080
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.1031501293182373,
      "eval_runtime": 8.6967,
      "eval_samples_per_second": 3499.151,
      "eval_steps_per_second": 13.683,
      "step": 14080
    },
    {
      "epoch": 32.02272727272727,
      "grad_norm": 0.18606272339820862,
      "learning_rate": 0.0004721675379984206,
      "loss": 2.2232,
      "step": 14090
    },
    {
      "epoch": 32.04545454545455,
      "grad_norm": 0.17227043211460114,
      "learning_rate": 0.00047212618494330553,
      "loss": 2.2138,
      "step": 14100
    },
    {
      "epoch": 32.06818181818182,
      "grad_norm": 0.17295026779174805,
      "learning_rate": 0.00047208480300376277,
      "loss": 2.2155,
      "step": 14110
    },
    {
      "epoch": 32.09090909090909,
      "grad_norm": 0.23218987882137299,
      "learning_rate": 0.00047204339218517367,
      "loss": 2.2157,
      "step": 14120
    },
    {
      "epoch": 32.11363636363637,
      "grad_norm": 0.16852891445159912,
      "learning_rate": 0.000472001952492923,
      "loss": 2.2147,
      "step": 14130
    },
    {
      "epoch": 32.13636363636363,
      "grad_norm": 0.3758193552494049,
      "learning_rate": 0.0004719604839323995,
      "loss": 2.2111,
      "step": 14140
    },
    {
      "epoch": 32.15909090909091,
      "grad_norm": 0.22735260426998138,
      "learning_rate": 0.0004719189865089957,
      "loss": 2.2184,
      "step": 14150
    },
    {
      "epoch": 32.18181818181818,
      "grad_norm": 0.19830070436000824,
      "learning_rate": 0.0004718774602281075,
      "loss": 2.2095,
      "step": 14160
    },
    {
      "epoch": 32.20454545454545,
      "grad_norm": 0.22377149760723114,
      "learning_rate": 0.00047183590509513505,
      "loss": 2.2154,
      "step": 14170
    },
    {
      "epoch": 32.22727272727273,
      "grad_norm": 0.15740966796875,
      "learning_rate": 0.00047179432111548185,
      "loss": 2.21,
      "step": 14180
    },
    {
      "epoch": 32.25,
      "grad_norm": 0.17874222993850708,
      "learning_rate": 0.0004717527082945554,
      "loss": 2.2074,
      "step": 14190
    },
    {
      "epoch": 32.27272727272727,
      "grad_norm": 0.15390823781490326,
      "learning_rate": 0.0004717110666377669,
      "loss": 2.2127,
      "step": 14200
    },
    {
      "epoch": 32.29545454545455,
      "grad_norm": 0.1331285536289215,
      "learning_rate": 0.00047166939615053124,
      "loss": 2.2233,
      "step": 14210
    },
    {
      "epoch": 32.31818181818182,
      "grad_norm": 0.16641663014888763,
      "learning_rate": 0.000471627696838267,
      "loss": 2.2182,
      "step": 14220
    },
    {
      "epoch": 32.34090909090909,
      "grad_norm": 0.15880697965621948,
      "learning_rate": 0.0004715859687063967,
      "loss": 2.2258,
      "step": 14230
    },
    {
      "epoch": 32.36363636363637,
      "grad_norm": 0.13236646354198456,
      "learning_rate": 0.0004715442117603465,
      "loss": 2.2164,
      "step": 14240
    },
    {
      "epoch": 32.38636363636363,
      "grad_norm": 0.7150028347969055,
      "learning_rate": 0.00047150242600554627,
      "loss": 2.2191,
      "step": 14250
    },
    {
      "epoch": 32.40909090909091,
      "grad_norm": 0.1834172159433365,
      "learning_rate": 0.00047146061144742973,
      "loss": 2.2118,
      "step": 14260
    },
    {
      "epoch": 32.43181818181818,
      "grad_norm": 0.11778160184621811,
      "learning_rate": 0.00047141876809143424,
      "loss": 2.2108,
      "step": 14270
    },
    {
      "epoch": 32.45454545454545,
      "grad_norm": 0.16426481306552887,
      "learning_rate": 0.00047137689594300085,
      "loss": 2.2154,
      "step": 14280
    },
    {
      "epoch": 32.47727272727273,
      "grad_norm": 0.1649577021598816,
      "learning_rate": 0.00047133499500757466,
      "loss": 2.217,
      "step": 14290
    },
    {
      "epoch": 32.5,
      "grad_norm": 0.300115168094635,
      "learning_rate": 0.0004712930652906041,
      "loss": 2.2184,
      "step": 14300
    },
    {
      "epoch": 32.52272727272727,
      "grad_norm": 0.26167240738868713,
      "learning_rate": 0.00047125110679754174,
      "loss": 2.2054,
      "step": 14310
    },
    {
      "epoch": 32.54545454545455,
      "grad_norm": 0.13274988532066345,
      "learning_rate": 0.0004712091195338436,
      "loss": 2.2203,
      "step": 14320
    },
    {
      "epoch": 32.56818181818182,
      "grad_norm": 0.2900995910167694,
      "learning_rate": 0.0004711671035049695,
      "loss": 2.2145,
      "step": 14330
    },
    {
      "epoch": 32.59090909090909,
      "grad_norm": 0.13855130970478058,
      "learning_rate": 0.0004711250587163831,
      "loss": 2.2205,
      "step": 14340
    },
    {
      "epoch": 32.61363636363637,
      "grad_norm": 0.32575342059135437,
      "learning_rate": 0.0004710829851735516,
      "loss": 2.2202,
      "step": 14350
    },
    {
      "epoch": 32.63636363636363,
      "grad_norm": 0.2527691125869751,
      "learning_rate": 0.00047104088288194636,
      "loss": 2.2198,
      "step": 14360
    },
    {
      "epoch": 32.65909090909091,
      "grad_norm": 0.14580097794532776,
      "learning_rate": 0.000470998751847042,
      "loss": 2.2208,
      "step": 14370
    },
    {
      "epoch": 32.68181818181818,
      "grad_norm": 0.14472295343875885,
      "learning_rate": 0.00047095659207431717,
      "loss": 2.215,
      "step": 14380
    },
    {
      "epoch": 32.70454545454545,
      "grad_norm": 0.15173257887363434,
      "learning_rate": 0.0004709144035692541,
      "loss": 2.2051,
      "step": 14390
    },
    {
      "epoch": 32.72727272727273,
      "grad_norm": 0.20081433653831482,
      "learning_rate": 0.00047087218633733873,
      "loss": 2.2128,
      "step": 14400
    },
    {
      "epoch": 32.75,
      "grad_norm": 0.16158388555049896,
      "learning_rate": 0.000470829940384061,
      "loss": 2.2184,
      "step": 14410
    },
    {
      "epoch": 32.77272727272727,
      "grad_norm": 0.2030060738325119,
      "learning_rate": 0.00047078766571491436,
      "loss": 2.207,
      "step": 14420
    },
    {
      "epoch": 32.79545454545455,
      "grad_norm": 0.3306938409805298,
      "learning_rate": 0.000470745362335396,
      "loss": 2.218,
      "step": 14430
    },
    {
      "epoch": 32.81818181818182,
      "grad_norm": 0.2234095185995102,
      "learning_rate": 0.00047070303025100694,
      "loss": 2.2135,
      "step": 14440
    },
    {
      "epoch": 32.84090909090909,
      "grad_norm": 0.18536829948425293,
      "learning_rate": 0.00047066066946725184,
      "loss": 2.2154,
      "step": 14450
    },
    {
      "epoch": 32.86363636363637,
      "grad_norm": 0.33389389514923096,
      "learning_rate": 0.0004706182799896391,
      "loss": 2.2142,
      "step": 14460
    },
    {
      "epoch": 32.88636363636363,
      "grad_norm": 0.23296962678432465,
      "learning_rate": 0.00047057586182368093,
      "loss": 2.2177,
      "step": 14470
    },
    {
      "epoch": 32.90909090909091,
      "grad_norm": 0.21865008771419525,
      "learning_rate": 0.00047053341497489324,
      "loss": 2.2155,
      "step": 14480
    },
    {
      "epoch": 32.93181818181818,
      "grad_norm": 0.20344778895378113,
      "learning_rate": 0.00047049093944879567,
      "loss": 2.2121,
      "step": 14490
    },
    {
      "epoch": 32.95454545454545,
      "grad_norm": 0.18890854716300964,
      "learning_rate": 0.00047044843525091153,
      "loss": 2.216,
      "step": 14500
    },
    {
      "epoch": 32.97727272727273,
      "grad_norm": 0.19389274716377258,
      "learning_rate": 0.0004704059023867678,
      "loss": 2.2202,
      "step": 14510
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.7247194051742554,
      "learning_rate": 0.0004703633408618955,
      "loss": 2.2197,
      "step": 14520
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.1026790142059326,
      "eval_runtime": 8.8098,
      "eval_samples_per_second": 3454.234,
      "eval_steps_per_second": 13.508,
      "step": 14520
    },
    {
      "epoch": 33.02272727272727,
      "grad_norm": 0.18611839413642883,
      "learning_rate": 0.00047032075068182907,
      "loss": 2.2087,
      "step": 14530
    },
    {
      "epoch": 33.04545454545455,
      "grad_norm": 0.23105131089687347,
      "learning_rate": 0.00047027813185210673,
      "loss": 2.2096,
      "step": 14540
    },
    {
      "epoch": 33.06818181818182,
      "grad_norm": 0.16671210527420044,
      "learning_rate": 0.00047023548437827046,
      "loss": 2.2155,
      "step": 14550
    },
    {
      "epoch": 33.09090909090909,
      "grad_norm": 0.2401028871536255,
      "learning_rate": 0.0004701928082658661,
      "loss": 2.2005,
      "step": 14560
    },
    {
      "epoch": 33.11363636363637,
      "grad_norm": 0.18935760855674744,
      "learning_rate": 0.00047015010352044294,
      "loss": 2.2076,
      "step": 14570
    },
    {
      "epoch": 33.13636363636363,
      "grad_norm": 0.4077489376068115,
      "learning_rate": 0.0004701073701475542,
      "loss": 2.2163,
      "step": 14580
    },
    {
      "epoch": 33.15909090909091,
      "grad_norm": 0.26694220304489136,
      "learning_rate": 0.00047006460815275674,
      "loss": 2.2169,
      "step": 14590
    },
    {
      "epoch": 33.18181818181818,
      "grad_norm": 0.18615487217903137,
      "learning_rate": 0.0004700218175416112,
      "loss": 2.2098,
      "step": 14600
    },
    {
      "epoch": 33.20454545454545,
      "grad_norm": 0.2695606052875519,
      "learning_rate": 0.00046997899831968193,
      "loss": 2.2245,
      "step": 14610
    },
    {
      "epoch": 33.22727272727273,
      "grad_norm": 0.21807315945625305,
      "learning_rate": 0.0004699361504925369,
      "loss": 2.2082,
      "step": 14620
    },
    {
      "epoch": 33.25,
      "grad_norm": 0.27820008993148804,
      "learning_rate": 0.00046989327406574794,
      "loss": 2.2219,
      "step": 14630
    },
    {
      "epoch": 33.27272727272727,
      "grad_norm": 0.18567459285259247,
      "learning_rate": 0.0004698503690448905,
      "loss": 2.2208,
      "step": 14640
    },
    {
      "epoch": 33.29545454545455,
      "grad_norm": 0.26828309893608093,
      "learning_rate": 0.0004698074354355438,
      "loss": 2.2194,
      "step": 14650
    },
    {
      "epoch": 33.31818181818182,
      "grad_norm": 0.24772657454013824,
      "learning_rate": 0.0004697644732432907,
      "loss": 2.2136,
      "step": 14660
    },
    {
      "epoch": 33.34090909090909,
      "grad_norm": 0.30723798274993896,
      "learning_rate": 0.00046972148247371786,
      "loss": 2.2082,
      "step": 14670
    },
    {
      "epoch": 33.36363636363637,
      "grad_norm": 0.3402961492538452,
      "learning_rate": 0.0004696784631324157,
      "loss": 2.2175,
      "step": 14680
    },
    {
      "epoch": 33.38636363636363,
      "grad_norm": 0.20662473142147064,
      "learning_rate": 0.00046963541522497825,
      "loss": 2.2211,
      "step": 14690
    },
    {
      "epoch": 33.40909090909091,
      "grad_norm": 0.2564364969730377,
      "learning_rate": 0.0004695923387570033,
      "loss": 2.2241,
      "step": 14700
    },
    {
      "epoch": 33.43181818181818,
      "grad_norm": 0.41780418157577515,
      "learning_rate": 0.0004695492337340923,
      "loss": 2.2256,
      "step": 14710
    },
    {
      "epoch": 33.45454545454545,
      "grad_norm": 0.2770135700702667,
      "learning_rate": 0.00046950610016185045,
      "loss": 2.2126,
      "step": 14720
    },
    {
      "epoch": 33.47727272727273,
      "grad_norm": 0.17547360062599182,
      "learning_rate": 0.0004694629380458868,
      "loss": 2.2149,
      "step": 14730
    },
    {
      "epoch": 33.5,
      "grad_norm": 0.313616544008255,
      "learning_rate": 0.0004694197473918139,
      "loss": 2.213,
      "step": 14740
    },
    {
      "epoch": 33.52272727272727,
      "grad_norm": 0.21877717971801758,
      "learning_rate": 0.0004693765282052481,
      "loss": 2.2187,
      "step": 14750
    },
    {
      "epoch": 33.54545454545455,
      "grad_norm": 0.25174108147621155,
      "learning_rate": 0.0004693332804918094,
      "loss": 2.2157,
      "step": 14760
    },
    {
      "epoch": 33.56818181818182,
      "grad_norm": 0.1856977790594101,
      "learning_rate": 0.00046929000425712165,
      "loss": 2.2145,
      "step": 14770
    },
    {
      "epoch": 33.59090909090909,
      "grad_norm": 0.29213812947273254,
      "learning_rate": 0.0004692466995068123,
      "loss": 2.2148,
      "step": 14780
    },
    {
      "epoch": 33.61363636363637,
      "grad_norm": 0.2332053780555725,
      "learning_rate": 0.0004692033662465125,
      "loss": 2.2136,
      "step": 14790
    },
    {
      "epoch": 33.63636363636363,
      "grad_norm": 0.21602435410022736,
      "learning_rate": 0.00046916000448185715,
      "loss": 2.2224,
      "step": 14800
    },
    {
      "epoch": 33.65909090909091,
      "grad_norm": 0.12709006667137146,
      "learning_rate": 0.00046911661421848483,
      "loss": 2.2201,
      "step": 14810
    },
    {
      "epoch": 33.68181818181818,
      "grad_norm": 0.1627042144536972,
      "learning_rate": 0.00046907319546203797,
      "loss": 2.2154,
      "step": 14820
    },
    {
      "epoch": 33.70454545454545,
      "grad_norm": 0.2553113102912903,
      "learning_rate": 0.00046902974821816243,
      "loss": 2.2196,
      "step": 14830
    },
    {
      "epoch": 33.72727272727273,
      "grad_norm": 0.1613999456167221,
      "learning_rate": 0.0004689862724925079,
      "loss": 2.2169,
      "step": 14840
    },
    {
      "epoch": 33.75,
      "grad_norm": 0.27766379714012146,
      "learning_rate": 0.0004689427682907279,
      "loss": 2.2009,
      "step": 14850
    },
    {
      "epoch": 33.77272727272727,
      "grad_norm": 0.22442105412483215,
      "learning_rate": 0.0004688992356184796,
      "loss": 2.2141,
      "step": 14860
    },
    {
      "epoch": 33.79545454545455,
      "grad_norm": 0.1466640830039978,
      "learning_rate": 0.00046885567448142363,
      "loss": 2.2056,
      "step": 14870
    },
    {
      "epoch": 33.81818181818182,
      "grad_norm": 0.22245408594608307,
      "learning_rate": 0.00046881208488522465,
      "loss": 2.2155,
      "step": 14880
    },
    {
      "epoch": 33.84090909090909,
      "grad_norm": 0.45995277166366577,
      "learning_rate": 0.00046876846683555073,
      "loss": 2.2302,
      "step": 14890
    },
    {
      "epoch": 33.86363636363637,
      "grad_norm": 0.2008158564567566,
      "learning_rate": 0.00046872482033807406,
      "loss": 2.207,
      "step": 14900
    },
    {
      "epoch": 33.88636363636363,
      "grad_norm": 0.2425488531589508,
      "learning_rate": 0.00046868114539847,
      "loss": 2.2075,
      "step": 14910
    },
    {
      "epoch": 33.90909090909091,
      "grad_norm": 0.14495503902435303,
      "learning_rate": 0.00046863744202241806,
      "loss": 2.2123,
      "step": 14920
    },
    {
      "epoch": 33.93181818181818,
      "grad_norm": 0.1631208062171936,
      "learning_rate": 0.0004685937102156012,
      "loss": 2.2149,
      "step": 14930
    },
    {
      "epoch": 33.95454545454545,
      "grad_norm": 0.26048144698143005,
      "learning_rate": 0.0004685499499837059,
      "loss": 2.2056,
      "step": 14940
    },
    {
      "epoch": 33.97727272727273,
      "grad_norm": 0.212151437997818,
      "learning_rate": 0.00046850616133242294,
      "loss": 2.2197,
      "step": 14950
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.4622730612754822,
      "learning_rate": 0.00046846234426744626,
      "loss": 2.2078,
      "step": 14960
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.1023319959640503,
      "eval_runtime": 8.7072,
      "eval_samples_per_second": 3494.906,
      "eval_steps_per_second": 13.667,
      "step": 14960
    },
    {
      "epoch": 34.02272727272727,
      "grad_norm": 0.20090976357460022,
      "learning_rate": 0.0004684184987944736,
      "loss": 2.2188,
      "step": 14970
    },
    {
      "epoch": 34.04545454545455,
      "grad_norm": 0.2175203412771225,
      "learning_rate": 0.0004683746249192066,
      "loss": 2.2053,
      "step": 14980
    },
    {
      "epoch": 34.06818181818182,
      "grad_norm": 0.3637177646160126,
      "learning_rate": 0.0004683307226473503,
      "loss": 2.2109,
      "step": 14990
    },
    {
      "epoch": 34.09090909090909,
      "grad_norm": 0.21590688824653625,
      "learning_rate": 0.00046828679198461374,
      "loss": 2.2124,
      "step": 15000
    },
    {
      "epoch": 34.11363636363637,
      "grad_norm": 0.19615896046161652,
      "learning_rate": 0.0004682428329367093,
      "loss": 2.2209,
      "step": 15010
    },
    {
      "epoch": 34.13636363636363,
      "grad_norm": 0.8352071046829224,
      "learning_rate": 0.0004681988455093534,
      "loss": 2.2032,
      "step": 15020
    },
    {
      "epoch": 34.15909090909091,
      "grad_norm": 0.37744420766830444,
      "learning_rate": 0.00046815482970826594,
      "loss": 2.2186,
      "step": 15030
    },
    {
      "epoch": 34.18181818181818,
      "grad_norm": 0.32216906547546387,
      "learning_rate": 0.00046811078553917054,
      "loss": 2.2057,
      "step": 15040
    },
    {
      "epoch": 34.20454545454545,
      "grad_norm": 0.1991139054298401,
      "learning_rate": 0.00046806671300779457,
      "loss": 2.2033,
      "step": 15050
    },
    {
      "epoch": 34.22727272727273,
      "grad_norm": 0.5479840040206909,
      "learning_rate": 0.0004680226121198691,
      "loss": 2.2119,
      "step": 15060
    },
    {
      "epoch": 34.25,
      "grad_norm": 0.20046348869800568,
      "learning_rate": 0.00046797848288112866,
      "loss": 2.2211,
      "step": 15070
    },
    {
      "epoch": 34.27272727272727,
      "grad_norm": 1.169533371925354,
      "learning_rate": 0.00046793432529731186,
      "loss": 2.2115,
      "step": 15080
    },
    {
      "epoch": 34.29545454545455,
      "grad_norm": 0.3156577944755554,
      "learning_rate": 0.00046789013937416064,
      "loss": 2.2142,
      "step": 15090
    },
    {
      "epoch": 34.31818181818182,
      "grad_norm": 0.19478993117809296,
      "learning_rate": 0.00046784592511742086,
      "loss": 2.2116,
      "step": 15100
    },
    {
      "epoch": 34.34090909090909,
      "grad_norm": 0.18005745112895966,
      "learning_rate": 0.0004678016825328418,
      "loss": 2.2112,
      "step": 15110
    },
    {
      "epoch": 34.36363636363637,
      "grad_norm": 0.6482585072517395,
      "learning_rate": 0.00046775741162617684,
      "loss": 2.2,
      "step": 15120
    },
    {
      "epoch": 34.38636363636363,
      "grad_norm": 0.28884801268577576,
      "learning_rate": 0.00046771311240318265,
      "loss": 2.2072,
      "step": 15130
    },
    {
      "epoch": 34.40909090909091,
      "grad_norm": 0.2486225813627243,
      "learning_rate": 0.00046766878486961973,
      "loss": 2.2195,
      "step": 15140
    },
    {
      "epoch": 34.43181818181818,
      "grad_norm": 0.4281182289123535,
      "learning_rate": 0.00046762442903125224,
      "loss": 2.2056,
      "step": 15150
    },
    {
      "epoch": 34.45454545454545,
      "grad_norm": 0.37741661071777344,
      "learning_rate": 0.0004675800448938482,
      "loss": 2.216,
      "step": 15160
    },
    {
      "epoch": 34.47727272727273,
      "grad_norm": 0.3538401126861572,
      "learning_rate": 0.0004675356324631789,
      "loss": 2.2136,
      "step": 15170
    },
    {
      "epoch": 34.5,
      "grad_norm": 0.4194432497024536,
      "learning_rate": 0.00046749119174501976,
      "loss": 2.2155,
      "step": 15180
    },
    {
      "epoch": 34.52272727272727,
      "grad_norm": 0.26807600259780884,
      "learning_rate": 0.0004674467227451496,
      "loss": 2.2212,
      "step": 15190
    },
    {
      "epoch": 34.54545454545455,
      "grad_norm": 0.22366288304328918,
      "learning_rate": 0.00046740222546935104,
      "loss": 2.2152,
      "step": 15200
    },
    {
      "epoch": 34.56818181818182,
      "grad_norm": 0.2518990933895111,
      "learning_rate": 0.00046735769992341025,
      "loss": 2.2224,
      "step": 15210
    },
    {
      "epoch": 34.59090909090909,
      "grad_norm": 0.16151873767375946,
      "learning_rate": 0.0004673131461131173,
      "loss": 2.2208,
      "step": 15220
    },
    {
      "epoch": 34.61363636363637,
      "grad_norm": 0.21008151769638062,
      "learning_rate": 0.00046726856404426557,
      "loss": 2.1982,
      "step": 15230
    },
    {
      "epoch": 34.63636363636363,
      "grad_norm": 0.40069660544395447,
      "learning_rate": 0.00046722395372265256,
      "loss": 2.2199,
      "step": 15240
    },
    {
      "epoch": 34.65909090909091,
      "grad_norm": 0.25611576437950134,
      "learning_rate": 0.0004671793151540791,
      "loss": 2.232,
      "step": 15250
    },
    {
      "epoch": 34.68181818181818,
      "grad_norm": 0.19542914628982544,
      "learning_rate": 0.00046713464834434996,
      "loss": 2.222,
      "step": 15260
    },
    {
      "epoch": 34.70454545454545,
      "grad_norm": 0.2426823526620865,
      "learning_rate": 0.0004670899532992732,
      "loss": 2.219,
      "step": 15270
    },
    {
      "epoch": 34.72727272727273,
      "grad_norm": 1.074371099472046,
      "learning_rate": 0.00046704523002466097,
      "loss": 2.2025,
      "step": 15280
    },
    {
      "epoch": 34.75,
      "grad_norm": 0.17309026420116425,
      "learning_rate": 0.0004670004785263289,
      "loss": 2.2146,
      "step": 15290
    },
    {
      "epoch": 34.77272727272727,
      "grad_norm": 0.22446230053901672,
      "learning_rate": 0.00046695569881009623,
      "loss": 2.2155,
      "step": 15300
    },
    {
      "epoch": 34.79545454545455,
      "grad_norm": 0.37315964698791504,
      "learning_rate": 0.00046691089088178594,
      "loss": 2.2201,
      "step": 15310
    },
    {
      "epoch": 34.81818181818182,
      "grad_norm": 0.2328203320503235,
      "learning_rate": 0.0004668660547472248,
      "loss": 2.2301,
      "step": 15320
    },
    {
      "epoch": 34.84090909090909,
      "grad_norm": 0.4882979393005371,
      "learning_rate": 0.000466821190412243,
      "loss": 2.2099,
      "step": 15330
    },
    {
      "epoch": 34.86363636363637,
      "grad_norm": 0.4043964445590973,
      "learning_rate": 0.00046677629788267455,
      "loss": 2.2058,
      "step": 15340
    },
    {
      "epoch": 34.88636363636363,
      "grad_norm": 0.33313605189323425,
      "learning_rate": 0.00046673137716435716,
      "loss": 2.2132,
      "step": 15350
    },
    {
      "epoch": 34.90909090909091,
      "grad_norm": 0.4878183901309967,
      "learning_rate": 0.0004666864282631321,
      "loss": 2.2228,
      "step": 15360
    },
    {
      "epoch": 34.93181818181818,
      "grad_norm": 0.17643460631370544,
      "learning_rate": 0.0004666414511848443,
      "loss": 2.2189,
      "step": 15370
    },
    {
      "epoch": 34.95454545454545,
      "grad_norm": 0.26428693532943726,
      "learning_rate": 0.00046659644593534253,
      "loss": 2.2069,
      "step": 15380
    },
    {
      "epoch": 34.97727272727273,
      "grad_norm": 0.2109251320362091,
      "learning_rate": 0.00046655141252047893,
      "loss": 2.2222,
      "step": 15390
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.28746894001960754,
      "learning_rate": 0.00046650635094610973,
      "loss": 2.2138,
      "step": 15400
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.1020764112472534,
      "eval_runtime": 8.919,
      "eval_samples_per_second": 3411.938,
      "eval_steps_per_second": 13.342,
      "step": 15400
    },
    {
      "epoch": 35.02272727272727,
      "grad_norm": 0.23559604585170746,
      "learning_rate": 0.00046646126121809424,
      "loss": 2.213,
      "step": 15410
    },
    {
      "epoch": 35.04545454545455,
      "grad_norm": 0.14420899748802185,
      "learning_rate": 0.000466416143342296,
      "loss": 2.223,
      "step": 15420
    },
    {
      "epoch": 35.06818181818182,
      "grad_norm": 0.3623404800891876,
      "learning_rate": 0.00046637099732458183,
      "loss": 2.2157,
      "step": 15430
    },
    {
      "epoch": 35.09090909090909,
      "grad_norm": 0.23472347855567932,
      "learning_rate": 0.0004663258231708224,
      "loss": 2.2122,
      "step": 15440
    },
    {
      "epoch": 35.11363636363637,
      "grad_norm": 0.34109655022621155,
      "learning_rate": 0.000466280620886892,
      "loss": 2.2177,
      "step": 15450
    },
    {
      "epoch": 35.13636363636363,
      "grad_norm": 0.2734765112400055,
      "learning_rate": 0.00046623539047866845,
      "loss": 2.2167,
      "step": 15460
    },
    {
      "epoch": 35.15909090909091,
      "grad_norm": 1.1049768924713135,
      "learning_rate": 0.0004661901319520335,
      "loss": 2.2248,
      "step": 15470
    },
    {
      "epoch": 35.18181818181818,
      "grad_norm": 1.1516143083572388,
      "learning_rate": 0.0004661448453128723,
      "loss": 2.2109,
      "step": 15480
    },
    {
      "epoch": 35.20454545454545,
      "grad_norm": 0.20152102410793304,
      "learning_rate": 0.00046609953056707377,
      "loss": 2.2092,
      "step": 15490
    },
    {
      "epoch": 35.22727272727273,
      "grad_norm": 0.5819067358970642,
      "learning_rate": 0.00046605418772053045,
      "loss": 2.2165,
      "step": 15500
    },
    {
      "epoch": 35.25,
      "grad_norm": 0.2614186406135559,
      "learning_rate": 0.00046600881677913853,
      "loss": 2.2191,
      "step": 15510
    },
    {
      "epoch": 35.27272727272727,
      "grad_norm": 0.25278979539871216,
      "learning_rate": 0.0004659634177487979,
      "loss": 2.234,
      "step": 15520
    },
    {
      "epoch": 35.29545454545455,
      "grad_norm": 0.18496796488761902,
      "learning_rate": 0.00046591799063541207,
      "loss": 2.2221,
      "step": 15530
    },
    {
      "epoch": 35.31818181818182,
      "grad_norm": 0.17308229207992554,
      "learning_rate": 0.00046587253544488823,
      "loss": 2.2131,
      "step": 15540
    },
    {
      "epoch": 35.34090909090909,
      "grad_norm": 0.2781003415584564,
      "learning_rate": 0.00046582705218313714,
      "loss": 2.2007,
      "step": 15550
    },
    {
      "epoch": 35.36363636363637,
      "grad_norm": 0.19149895012378693,
      "learning_rate": 0.00046578154085607327,
      "loss": 2.2122,
      "step": 15560
    },
    {
      "epoch": 35.38636363636363,
      "grad_norm": 0.25501325726509094,
      "learning_rate": 0.0004657360014696148,
      "loss": 2.2125,
      "step": 15570
    },
    {
      "epoch": 35.40909090909091,
      "grad_norm": 0.16921234130859375,
      "learning_rate": 0.0004656904340296835,
      "loss": 2.209,
      "step": 15580
    },
    {
      "epoch": 35.43181818181818,
      "grad_norm": 0.17705483734607697,
      "learning_rate": 0.00046564483854220467,
      "loss": 2.2231,
      "step": 15590
    },
    {
      "epoch": 35.45454545454545,
      "grad_norm": 0.2208966761827469,
      "learning_rate": 0.0004655992150131075,
      "loss": 2.2198,
      "step": 15600
    },
    {
      "epoch": 35.47727272727273,
      "grad_norm": 0.13603205978870392,
      "learning_rate": 0.00046555356344832457,
      "loss": 2.2147,
      "step": 15610
    },
    {
      "epoch": 35.5,
      "grad_norm": 0.14398561418056488,
      "learning_rate": 0.00046550788385379233,
      "loss": 2.2007,
      "step": 15620
    },
    {
      "epoch": 35.52272727272727,
      "grad_norm": 0.21058614552021027,
      "learning_rate": 0.00046546217623545075,
      "loss": 2.2053,
      "step": 15630
    },
    {
      "epoch": 35.54545454545455,
      "grad_norm": 0.3300526440143585,
      "learning_rate": 0.00046541644059924347,
      "loss": 2.208,
      "step": 15640
    },
    {
      "epoch": 35.56818181818182,
      "grad_norm": 0.26779109239578247,
      "learning_rate": 0.00046537067695111777,
      "loss": 2.2246,
      "step": 15650
    },
    {
      "epoch": 35.59090909090909,
      "grad_norm": 0.14143086969852448,
      "learning_rate": 0.0004653248852970246,
      "loss": 2.2073,
      "step": 15660
    },
    {
      "epoch": 35.61363636363637,
      "grad_norm": 0.1763908565044403,
      "learning_rate": 0.00046527906564291854,
      "loss": 2.2078,
      "step": 15670
    },
    {
      "epoch": 35.63636363636363,
      "grad_norm": 0.2791643440723419,
      "learning_rate": 0.0004652332179947577,
      "loss": 2.2127,
      "step": 15680
    },
    {
      "epoch": 35.65909090909091,
      "grad_norm": 0.34171414375305176,
      "learning_rate": 0.000465187342358504,
      "loss": 2.2215,
      "step": 15690
    },
    {
      "epoch": 35.68181818181818,
      "grad_norm": 0.1948898881673813,
      "learning_rate": 0.000465141438740123,
      "loss": 2.2151,
      "step": 15700
    },
    {
      "epoch": 35.70454545454545,
      "grad_norm": 0.1950116753578186,
      "learning_rate": 0.00046509550714558367,
      "loss": 2.2047,
      "step": 15710
    },
    {
      "epoch": 35.72727272727273,
      "grad_norm": 0.20166772603988647,
      "learning_rate": 0.00046504954758085895,
      "loss": 2.2013,
      "step": 15720
    },
    {
      "epoch": 35.75,
      "grad_norm": 0.36397385597229004,
      "learning_rate": 0.0004650035600519251,
      "loss": 2.2224,
      "step": 15730
    },
    {
      "epoch": 35.77272727272727,
      "grad_norm": 0.23030701279640198,
      "learning_rate": 0.0004649575445647623,
      "loss": 2.2084,
      "step": 15740
    },
    {
      "epoch": 35.79545454545455,
      "grad_norm": 0.23006798326969147,
      "learning_rate": 0.0004649115011253541,
      "loss": 2.2123,
      "step": 15750
    },
    {
      "epoch": 35.81818181818182,
      "grad_norm": 0.274358868598938,
      "learning_rate": 0.0004648654297396878,
      "loss": 2.2193,
      "step": 15760
    },
    {
      "epoch": 35.84090909090909,
      "grad_norm": 0.7483787536621094,
      "learning_rate": 0.00046481933041375446,
      "loss": 2.2208,
      "step": 15770
    },
    {
      "epoch": 35.86363636363637,
      "grad_norm": 0.16648390889167786,
      "learning_rate": 0.0004647732031535486,
      "loss": 2.2188,
      "step": 15780
    },
    {
      "epoch": 35.88636363636363,
      "grad_norm": 0.25985825061798096,
      "learning_rate": 0.0004647270479650684,
      "loss": 2.2189,
      "step": 15790
    },
    {
      "epoch": 35.90909090909091,
      "grad_norm": 0.1490548998117447,
      "learning_rate": 0.0004646808648543157,
      "loss": 2.2057,
      "step": 15800
    },
    {
      "epoch": 35.93181818181818,
      "grad_norm": 1.8079733848571777,
      "learning_rate": 0.0004646346538272961,
      "loss": 2.2064,
      "step": 15810
    },
    {
      "epoch": 35.95454545454545,
      "grad_norm": 0.18390829861164093,
      "learning_rate": 0.00046458841489001854,
      "loss": 2.2167,
      "step": 15820
    },
    {
      "epoch": 35.97727272727273,
      "grad_norm": 0.22799086570739746,
      "learning_rate": 0.0004645421480484958,
      "loss": 2.2089,
      "step": 15830
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.6083824038505554,
      "learning_rate": 0.0004644958533087443,
      "loss": 2.2117,
      "step": 15840
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.1021965742111206,
      "eval_runtime": 8.669,
      "eval_samples_per_second": 3510.306,
      "eval_steps_per_second": 13.727,
      "step": 15840
    },
    {
      "epoch": 36.02272727272727,
      "grad_norm": 0.19285942614078522,
      "learning_rate": 0.00046444953067678395,
      "loss": 2.2062,
      "step": 15850
    },
    {
      "epoch": 36.04545454545455,
      "grad_norm": 0.19723129272460938,
      "learning_rate": 0.00046440318015863834,
      "loss": 2.2122,
      "step": 15860
    },
    {
      "epoch": 36.06818181818182,
      "grad_norm": 0.1991197168827057,
      "learning_rate": 0.0004643568017603349,
      "loss": 2.1955,
      "step": 15870
    },
    {
      "epoch": 36.09090909090909,
      "grad_norm": 0.1835886687040329,
      "learning_rate": 0.0004643103954879043,
      "loss": 2.2003,
      "step": 15880
    },
    {
      "epoch": 36.11363636363637,
      "grad_norm": 0.2461843639612198,
      "learning_rate": 0.0004642639613473811,
      "loss": 2.2154,
      "step": 15890
    },
    {
      "epoch": 36.13636363636363,
      "grad_norm": 0.17572611570358276,
      "learning_rate": 0.0004642174993448034,
      "loss": 2.2173,
      "step": 15900
    },
    {
      "epoch": 36.15909090909091,
      "grad_norm": 0.2207997739315033,
      "learning_rate": 0.00046417100948621307,
      "loss": 2.2149,
      "step": 15910
    },
    {
      "epoch": 36.18181818181818,
      "grad_norm": 0.25992339849472046,
      "learning_rate": 0.0004641244917776553,
      "loss": 2.2082,
      "step": 15920
    },
    {
      "epoch": 36.20454545454545,
      "grad_norm": 0.29337817430496216,
      "learning_rate": 0.000464077946225179,
      "loss": 2.208,
      "step": 15930
    },
    {
      "epoch": 36.22727272727273,
      "grad_norm": 0.24051839113235474,
      "learning_rate": 0.000464031372834837,
      "loss": 2.2119,
      "step": 15940
    },
    {
      "epoch": 36.25,
      "grad_norm": 0.1498754769563675,
      "learning_rate": 0.00046398477161268544,
      "loss": 2.2226,
      "step": 15950
    },
    {
      "epoch": 36.27272727272727,
      "grad_norm": 0.2146417200565338,
      "learning_rate": 0.0004639381425647841,
      "loss": 2.2132,
      "step": 15960
    },
    {
      "epoch": 36.29545454545455,
      "grad_norm": 0.22675876319408417,
      "learning_rate": 0.0004638914856971965,
      "loss": 2.2202,
      "step": 15970
    },
    {
      "epoch": 36.31818181818182,
      "grad_norm": 0.1594683974981308,
      "learning_rate": 0.0004638448010159898,
      "loss": 2.2131,
      "step": 15980
    },
    {
      "epoch": 36.34090909090909,
      "grad_norm": 0.9578937888145447,
      "learning_rate": 0.0004637980885272345,
      "loss": 2.2078,
      "step": 15990
    },
    {
      "epoch": 36.36363636363637,
      "grad_norm": 0.13983459770679474,
      "learning_rate": 0.000463751348237005,
      "loss": 2.2089,
      "step": 16000
    },
    {
      "epoch": 36.38636363636363,
      "grad_norm": 0.16478568315505981,
      "learning_rate": 0.0004637045801513793,
      "loss": 2.2076,
      "step": 16010
    },
    {
      "epoch": 36.40909090909091,
      "grad_norm": 0.37581396102905273,
      "learning_rate": 0.0004636577842764388,
      "loss": 2.2258,
      "step": 16020
    },
    {
      "epoch": 36.43181818181818,
      "grad_norm": 0.20353811979293823,
      "learning_rate": 0.0004636109606182688,
      "loss": 2.2167,
      "step": 16030
    },
    {
      "epoch": 36.45454545454545,
      "grad_norm": 0.14812660217285156,
      "learning_rate": 0.000463564109182958,
      "loss": 2.1986,
      "step": 16040
    },
    {
      "epoch": 36.47727272727273,
      "grad_norm": 0.18721409142017365,
      "learning_rate": 0.00046351722997659873,
      "loss": 2.2146,
      "step": 16050
    },
    {
      "epoch": 36.5,
      "grad_norm": 0.17711521685123444,
      "learning_rate": 0.000463470323005287,
      "loss": 2.2148,
      "step": 16060
    },
    {
      "epoch": 36.52272727272727,
      "grad_norm": 0.15218593180179596,
      "learning_rate": 0.00046342338827512253,
      "loss": 2.214,
      "step": 16070
    },
    {
      "epoch": 36.54545454545455,
      "grad_norm": 0.1602632701396942,
      "learning_rate": 0.0004633764257922084,
      "loss": 2.2215,
      "step": 16080
    },
    {
      "epoch": 36.56818181818182,
      "grad_norm": 0.5798942446708679,
      "learning_rate": 0.00046332943556265144,
      "loss": 2.2122,
      "step": 16090
    },
    {
      "epoch": 36.59090909090909,
      "grad_norm": 0.22030988335609436,
      "learning_rate": 0.0004632824175925622,
      "loss": 2.2098,
      "step": 16100
    },
    {
      "epoch": 36.61363636363637,
      "grad_norm": 0.21208515763282776,
      "learning_rate": 0.00046323537188805447,
      "loss": 2.2203,
      "step": 16110
    },
    {
      "epoch": 36.63636363636363,
      "grad_norm": 0.19857414066791534,
      "learning_rate": 0.00046318829845524615,
      "loss": 2.2115,
      "step": 16120
    },
    {
      "epoch": 36.65909090909091,
      "grad_norm": 0.1644279956817627,
      "learning_rate": 0.00046314119730025836,
      "loss": 2.2181,
      "step": 16130
    },
    {
      "epoch": 36.68181818181818,
      "grad_norm": 0.21739277243614197,
      "learning_rate": 0.0004630940684292159,
      "loss": 2.2037,
      "step": 16140
    },
    {
      "epoch": 36.70454545454545,
      "grad_norm": 0.21206919848918915,
      "learning_rate": 0.0004630469118482473,
      "loss": 2.2057,
      "step": 16150
    },
    {
      "epoch": 36.72727272727273,
      "grad_norm": 0.4301071763038635,
      "learning_rate": 0.00046299972756348474,
      "loss": 2.2187,
      "step": 16160
    },
    {
      "epoch": 36.75,
      "grad_norm": 0.24794507026672363,
      "learning_rate": 0.00046295251558106366,
      "loss": 2.2148,
      "step": 16170
    },
    {
      "epoch": 36.77272727272727,
      "grad_norm": 0.17835530638694763,
      "learning_rate": 0.0004629052759071234,
      "loss": 2.2214,
      "step": 16180
    },
    {
      "epoch": 36.79545454545455,
      "grad_norm": 0.21300527453422546,
      "learning_rate": 0.0004628580085478069,
      "loss": 2.209,
      "step": 16190
    },
    {
      "epoch": 36.81818181818182,
      "grad_norm": 0.15086579322814941,
      "learning_rate": 0.00046281071350926054,
      "loss": 2.2045,
      "step": 16200
    },
    {
      "epoch": 36.84090909090909,
      "grad_norm": 0.23207855224609375,
      "learning_rate": 0.0004627633907976344,
      "loss": 2.2162,
      "step": 16210
    },
    {
      "epoch": 36.86363636363637,
      "grad_norm": 0.5760999321937561,
      "learning_rate": 0.0004627160404190821,
      "loss": 2.1901,
      "step": 16220
    },
    {
      "epoch": 36.88636363636363,
      "grad_norm": 0.35104772448539734,
      "learning_rate": 0.000462668662379761,
      "loss": 2.2183,
      "step": 16230
    },
    {
      "epoch": 36.90909090909091,
      "grad_norm": 0.3476775586605072,
      "learning_rate": 0.00046262125668583196,
      "loss": 2.2096,
      "step": 16240
    },
    {
      "epoch": 36.93181818181818,
      "grad_norm": 0.1869756281375885,
      "learning_rate": 0.0004625738233434593,
      "loss": 2.2129,
      "step": 16250
    },
    {
      "epoch": 36.95454545454545,
      "grad_norm": 0.14019456505775452,
      "learning_rate": 0.0004625263623588112,
      "loss": 2.2289,
      "step": 16260
    },
    {
      "epoch": 36.97727272727273,
      "grad_norm": 0.19700096547603607,
      "learning_rate": 0.00046247887373805917,
      "loss": 2.2147,
      "step": 16270
    },
    {
      "epoch": 37.0,
      "grad_norm": 0.3441576063632965,
      "learning_rate": 0.00046243135748737864,
      "loss": 2.2153,
      "step": 16280
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.102203607559204,
      "eval_runtime": 8.9126,
      "eval_samples_per_second": 3414.37,
      "eval_steps_per_second": 13.352,
      "step": 16280
    },
    {
      "epoch": 37.02272727272727,
      "grad_norm": 0.18277110159397125,
      "learning_rate": 0.00046238381361294825,
      "loss": 2.2089,
      "step": 16290
    },
    {
      "epoch": 37.04545454545455,
      "grad_norm": 0.18634144961833954,
      "learning_rate": 0.0004623362421209505,
      "loss": 2.2035,
      "step": 16300
    },
    {
      "epoch": 37.06818181818182,
      "grad_norm": 0.17179390788078308,
      "learning_rate": 0.0004622886430175714,
      "loss": 2.2132,
      "step": 16310
    },
    {
      "epoch": 37.09090909090909,
      "grad_norm": 0.2807597815990448,
      "learning_rate": 0.00046224101630900054,
      "loss": 2.2102,
      "step": 16320
    },
    {
      "epoch": 37.11363636363637,
      "grad_norm": 0.2765173017978668,
      "learning_rate": 0.0004621933620014311,
      "loss": 2.2104,
      "step": 16330
    },
    {
      "epoch": 37.13636363636363,
      "grad_norm": 0.21184401214122772,
      "learning_rate": 0.00046214568010105996,
      "loss": 2.2194,
      "step": 16340
    },
    {
      "epoch": 37.15909090909091,
      "grad_norm": 0.3529566526412964,
      "learning_rate": 0.0004620979706140874,
      "loss": 2.1979,
      "step": 16350
    },
    {
      "epoch": 37.18181818181818,
      "grad_norm": 0.20614971220493317,
      "learning_rate": 0.0004620502335467174,
      "loss": 2.2253,
      "step": 16360
    },
    {
      "epoch": 37.20454545454545,
      "grad_norm": 0.16743768751621246,
      "learning_rate": 0.0004620024689051574,
      "loss": 2.2119,
      "step": 16370
    },
    {
      "epoch": 37.22727272727273,
      "grad_norm": 0.2976920008659363,
      "learning_rate": 0.0004619546766956187,
      "loss": 2.2077,
      "step": 16380
    },
    {
      "epoch": 37.25,
      "grad_norm": 0.22609414160251617,
      "learning_rate": 0.0004619068569243159,
      "loss": 2.2115,
      "step": 16390
    },
    {
      "epoch": 37.27272727272727,
      "grad_norm": 0.12750288844108582,
      "learning_rate": 0.00046185900959746736,
      "loss": 2.2101,
      "step": 16400
    },
    {
      "epoch": 37.29545454545455,
      "grad_norm": 0.16770268976688385,
      "learning_rate": 0.00046181113472129495,
      "loss": 2.2031,
      "step": 16410
    },
    {
      "epoch": 37.31818181818182,
      "grad_norm": 0.2658407390117645,
      "learning_rate": 0.00046176323230202404,
      "loss": 2.1995,
      "step": 16420
    },
    {
      "epoch": 37.34090909090909,
      "grad_norm": 0.367624968290329,
      "learning_rate": 0.0004617153023458838,
      "loss": 2.222,
      "step": 16430
    },
    {
      "epoch": 37.36363636363637,
      "grad_norm": 0.18946947157382965,
      "learning_rate": 0.00046166734485910684,
      "loss": 2.2107,
      "step": 16440
    },
    {
      "epoch": 37.38636363636363,
      "grad_norm": 0.3101542294025421,
      "learning_rate": 0.0004616193598479293,
      "loss": 2.227,
      "step": 16450
    },
    {
      "epoch": 37.40909090909091,
      "grad_norm": 0.19245745241641998,
      "learning_rate": 0.00046157134731859096,
      "loss": 2.2078,
      "step": 16460
    },
    {
      "epoch": 37.43181818181818,
      "grad_norm": 0.31554433703422546,
      "learning_rate": 0.0004615233072773353,
      "loss": 2.2117,
      "step": 16470
    },
    {
      "epoch": 37.45454545454545,
      "grad_norm": 0.4288814961910248,
      "learning_rate": 0.0004614752397304091,
      "loss": 2.2259,
      "step": 16480
    },
    {
      "epoch": 37.47727272727273,
      "grad_norm": 0.2289298176765442,
      "learning_rate": 0.000461427144684063,
      "loss": 2.2075,
      "step": 16490
    },
    {
      "epoch": 37.5,
      "grad_norm": 0.21975502371788025,
      "learning_rate": 0.0004613790221445511,
      "loss": 2.2135,
      "step": 16500
    },
    {
      "epoch": 37.52272727272727,
      "grad_norm": 0.3011370301246643,
      "learning_rate": 0.00046133087211813094,
      "loss": 2.2018,
      "step": 16510
    },
    {
      "epoch": 37.54545454545455,
      "grad_norm": 0.6816310286521912,
      "learning_rate": 0.00046128269461106385,
      "loss": 2.2123,
      "step": 16520
    },
    {
      "epoch": 37.56818181818182,
      "grad_norm": 0.21176528930664062,
      "learning_rate": 0.00046123448962961467,
      "loss": 2.221,
      "step": 16530
    },
    {
      "epoch": 37.59090909090909,
      "grad_norm": 1.086704969406128,
      "learning_rate": 0.0004611862571800518,
      "loss": 2.2051,
      "step": 16540
    },
    {
      "epoch": 37.61363636363637,
      "grad_norm": 0.23296891152858734,
      "learning_rate": 0.00046113799726864714,
      "loss": 2.213,
      "step": 16550
    },
    {
      "epoch": 37.63636363636363,
      "grad_norm": 0.28569185733795166,
      "learning_rate": 0.0004610897099016762,
      "loss": 2.2122,
      "step": 16560
    },
    {
      "epoch": 37.65909090909091,
      "grad_norm": 0.8173956274986267,
      "learning_rate": 0.0004610413950854181,
      "loss": 2.2176,
      "step": 16570
    },
    {
      "epoch": 37.68181818181818,
      "grad_norm": 0.18664538860321045,
      "learning_rate": 0.00046099305282615554,
      "loss": 2.217,
      "step": 16580
    },
    {
      "epoch": 37.70454545454545,
      "grad_norm": 0.24472390115261078,
      "learning_rate": 0.0004609446831301748,
      "loss": 2.2129,
      "step": 16590
    },
    {
      "epoch": 37.72727272727273,
      "grad_norm": 0.2536464333534241,
      "learning_rate": 0.0004608962860037657,
      "loss": 2.2073,
      "step": 16600
    },
    {
      "epoch": 37.75,
      "grad_norm": 0.19145077466964722,
      "learning_rate": 0.00046084786145322143,
      "loss": 2.2078,
      "step": 16610
    },
    {
      "epoch": 37.77272727272727,
      "grad_norm": 0.20485937595367432,
      "learning_rate": 0.0004607994094848391,
      "loss": 2.2061,
      "step": 16620
    },
    {
      "epoch": 37.79545454545455,
      "grad_norm": 0.27038073539733887,
      "learning_rate": 0.0004607509301049192,
      "loss": 2.2228,
      "step": 16630
    },
    {
      "epoch": 37.81818181818182,
      "grad_norm": 2.612992286682129,
      "learning_rate": 0.00046070242331976575,
      "loss": 2.2161,
      "step": 16640
    },
    {
      "epoch": 37.84090909090909,
      "grad_norm": 0.5462954044342041,
      "learning_rate": 0.0004606538891356864,
      "loss": 2.2102,
      "step": 16650
    },
    {
      "epoch": 37.86363636363637,
      "grad_norm": 0.6271877884864807,
      "learning_rate": 0.00046060532755899235,
      "loss": 2.2188,
      "step": 16660
    },
    {
      "epoch": 37.88636363636363,
      "grad_norm": 0.3933078944683075,
      "learning_rate": 0.00046055673859599837,
      "loss": 2.227,
      "step": 16670
    },
    {
      "epoch": 37.90909090909091,
      "grad_norm": 0.26794400811195374,
      "learning_rate": 0.00046050812225302285,
      "loss": 2.21,
      "step": 16680
    },
    {
      "epoch": 37.93181818181818,
      "grad_norm": 0.20574694871902466,
      "learning_rate": 0.00046045947853638747,
      "loss": 2.2193,
      "step": 16690
    },
    {
      "epoch": 37.95454545454545,
      "grad_norm": 0.25764235854148865,
      "learning_rate": 0.0004604108074524179,
      "loss": 2.2121,
      "step": 16700
    },
    {
      "epoch": 37.97727272727273,
      "grad_norm": 0.28998002409935,
      "learning_rate": 0.00046036210900744296,
      "loss": 2.2063,
      "step": 16710
    },
    {
      "epoch": 38.0,
      "grad_norm": 0.46148455142974854,
      "learning_rate": 0.0004603133832077953,
      "loss": 2.2127,
      "step": 16720
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.101950764656067,
      "eval_runtime": 8.8318,
      "eval_samples_per_second": 3445.614,
      "eval_steps_per_second": 13.474,
      "step": 16720
    },
    {
      "epoch": 38.02272727272727,
      "grad_norm": 0.17666272819042206,
      "learning_rate": 0.000460264630059811,
      "loss": 2.2138,
      "step": 16730
    },
    {
      "epoch": 38.04545454545455,
      "grad_norm": 0.25462833046913147,
      "learning_rate": 0.0004602158495698298,
      "loss": 2.2054,
      "step": 16740
    },
    {
      "epoch": 38.06818181818182,
      "grad_norm": 0.3320777416229248,
      "learning_rate": 0.00046016704174419484,
      "loss": 2.2216,
      "step": 16750
    },
    {
      "epoch": 38.09090909090909,
      "grad_norm": 0.21590068936347961,
      "learning_rate": 0.0004601182065892529,
      "loss": 2.2176,
      "step": 16760
    },
    {
      "epoch": 38.11363636363637,
      "grad_norm": 0.21943627297878265,
      "learning_rate": 0.0004600693441113544,
      "loss": 2.2122,
      "step": 16770
    },
    {
      "epoch": 38.13636363636363,
      "grad_norm": 0.2757071852684021,
      "learning_rate": 0.00046002045431685313,
      "loss": 2.2089,
      "step": 16780
    },
    {
      "epoch": 38.15909090909091,
      "grad_norm": 0.25022128224372864,
      "learning_rate": 0.0004599715372121066,
      "loss": 2.2185,
      "step": 16790
    },
    {
      "epoch": 38.18181818181818,
      "grad_norm": 0.39496853947639465,
      "learning_rate": 0.0004599225928034757,
      "loss": 2.2208,
      "step": 16800
    },
    {
      "epoch": 38.20454545454545,
      "grad_norm": 0.2475319355726242,
      "learning_rate": 0.0004598736210973251,
      "loss": 2.2161,
      "step": 16810
    },
    {
      "epoch": 38.22727272727273,
      "grad_norm": 0.8581168055534363,
      "learning_rate": 0.00045982462210002287,
      "loss": 2.2008,
      "step": 16820
    },
    {
      "epoch": 38.25,
      "grad_norm": 0.2257145494222641,
      "learning_rate": 0.0004597755958179406,
      "loss": 2.2113,
      "step": 16830
    },
    {
      "epoch": 38.27272727272727,
      "grad_norm": 0.15921713411808014,
      "learning_rate": 0.0004597265422574535,
      "loss": 2.21,
      "step": 16840
    },
    {
      "epoch": 38.29545454545455,
      "grad_norm": 0.8631983995437622,
      "learning_rate": 0.0004596774614249402,
      "loss": 2.2182,
      "step": 16850
    },
    {
      "epoch": 38.31818181818182,
      "grad_norm": 0.17648009955883026,
      "learning_rate": 0.00045962835332678315,
      "loss": 2.2029,
      "step": 16860
    },
    {
      "epoch": 38.34090909090909,
      "grad_norm": 1.3464056253433228,
      "learning_rate": 0.0004595792179693681,
      "loss": 2.202,
      "step": 16870
    },
    {
      "epoch": 38.36363636363637,
      "grad_norm": 0.8981665372848511,
      "learning_rate": 0.00045953005535908444,
      "loss": 2.2094,
      "step": 16880
    },
    {
      "epoch": 38.38636363636363,
      "grad_norm": 0.25551098585128784,
      "learning_rate": 0.00045948086550232504,
      "loss": 2.2102,
      "step": 16890
    },
    {
      "epoch": 38.40909090909091,
      "grad_norm": 0.2817538380622864,
      "learning_rate": 0.0004594316484054864,
      "loss": 2.2124,
      "step": 16900
    },
    {
      "epoch": 38.43181818181818,
      "grad_norm": 0.2442668080329895,
      "learning_rate": 0.0004593824040749685,
      "loss": 2.2149,
      "step": 16910
    },
    {
      "epoch": 38.45454545454545,
      "grad_norm": 0.20040301978588104,
      "learning_rate": 0.0004593331325171749,
      "loss": 2.2116,
      "step": 16920
    },
    {
      "epoch": 38.47727272727273,
      "grad_norm": 0.7900938391685486,
      "learning_rate": 0.0004592838337385127,
      "loss": 2.2171,
      "step": 16930
    },
    {
      "epoch": 38.5,
      "grad_norm": 0.21279986202716827,
      "learning_rate": 0.00045923450774539243,
      "loss": 2.2186,
      "step": 16940
    },
    {
      "epoch": 38.52272727272727,
      "grad_norm": 0.21201860904693604,
      "learning_rate": 0.00045918515454422836,
      "loss": 2.2202,
      "step": 16950
    },
    {
      "epoch": 38.54545454545455,
      "grad_norm": 0.1416928619146347,
      "learning_rate": 0.00045913577414143813,
      "loss": 2.2102,
      "step": 16960
    },
    {
      "epoch": 38.56818181818182,
      "grad_norm": 0.4578469693660736,
      "learning_rate": 0.00045908636654344294,
      "loss": 2.2097,
      "step": 16970
    },
    {
      "epoch": 38.59090909090909,
      "grad_norm": 0.20843683183193207,
      "learning_rate": 0.0004590369317566677,
      "loss": 2.2073,
      "step": 16980
    },
    {
      "epoch": 38.61363636363637,
      "grad_norm": 2.2289621829986572,
      "learning_rate": 0.0004589874697875406,
      "loss": 2.2041,
      "step": 16990
    },
    {
      "epoch": 38.63636363636363,
      "grad_norm": 0.18468300998210907,
      "learning_rate": 0.0004589379806424935,
      "loss": 2.1999,
      "step": 17000
    },
    {
      "epoch": 38.65909090909091,
      "grad_norm": 0.3284044563770294,
      "learning_rate": 0.00045888846432796184,
      "loss": 2.2075,
      "step": 17010
    },
    {
      "epoch": 38.68181818181818,
      "grad_norm": 0.3840773403644562,
      "learning_rate": 0.0004588389208503844,
      "loss": 2.225,
      "step": 17020
    },
    {
      "epoch": 38.70454545454545,
      "grad_norm": 0.3081732988357544,
      "learning_rate": 0.0004587893502162037,
      "loss": 2.205,
      "step": 17030
    },
    {
      "epoch": 38.72727272727273,
      "grad_norm": 0.42401230335235596,
      "learning_rate": 0.0004587397524318658,
      "loss": 2.2152,
      "step": 17040
    },
    {
      "epoch": 38.75,
      "grad_norm": 0.2608456611633301,
      "learning_rate": 0.00045869012750382004,
      "loss": 2.1942,
      "step": 17050
    },
    {
      "epoch": 38.77272727272727,
      "grad_norm": 0.27962759137153625,
      "learning_rate": 0.0004586404754385196,
      "loss": 2.2017,
      "step": 17060
    },
    {
      "epoch": 38.79545454545455,
      "grad_norm": 0.15927410125732422,
      "learning_rate": 0.0004585907962424209,
      "loss": 2.2226,
      "step": 17070
    },
    {
      "epoch": 38.81818181818182,
      "grad_norm": 0.18706421554088593,
      "learning_rate": 0.00045854108992198417,
      "loss": 2.2097,
      "step": 17080
    },
    {
      "epoch": 38.84090909090909,
      "grad_norm": 0.23769162595272064,
      "learning_rate": 0.00045849135648367293,
      "loss": 2.216,
      "step": 17090
    },
    {
      "epoch": 38.86363636363637,
      "grad_norm": 0.2786858081817627,
      "learning_rate": 0.0004584415959339543,
      "loss": 2.2017,
      "step": 17100
    },
    {
      "epoch": 38.88636363636363,
      "grad_norm": 0.5677947998046875,
      "learning_rate": 0.00045839180827929906,
      "loss": 2.2097,
      "step": 17110
    },
    {
      "epoch": 38.90909090909091,
      "grad_norm": 1.7460122108459473,
      "learning_rate": 0.0004583419935261813,
      "loss": 2.2127,
      "step": 17120
    },
    {
      "epoch": 38.93181818181818,
      "grad_norm": 0.21046912670135498,
      "learning_rate": 0.00045829215168107884,
      "loss": 2.215,
      "step": 17130
    },
    {
      "epoch": 38.95454545454545,
      "grad_norm": 0.20355907082557678,
      "learning_rate": 0.00045824228275047286,
      "loss": 2.2064,
      "step": 17140
    },
    {
      "epoch": 38.97727272727273,
      "grad_norm": 0.2105654925107956,
      "learning_rate": 0.00045819238674084807,
      "loss": 2.2235,
      "step": 17150
    },
    {
      "epoch": 39.0,
      "grad_norm": 0.3173898756504059,
      "learning_rate": 0.00045814246365869285,
      "loss": 2.209,
      "step": 17160
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.1018844842910767,
      "eval_runtime": 8.9302,
      "eval_samples_per_second": 3407.637,
      "eval_steps_per_second": 13.326,
      "step": 17160
    },
    {
      "epoch": 39.02272727272727,
      "grad_norm": 0.3470836579799652,
      "learning_rate": 0.00045809251351049894,
      "loss": 2.211,
      "step": 17170
    },
    {
      "epoch": 39.04545454545455,
      "grad_norm": 0.9571199417114258,
      "learning_rate": 0.0004580425363027617,
      "loss": 2.2079,
      "step": 17180
    },
    {
      "epoch": 39.06818181818182,
      "grad_norm": 0.1541796326637268,
      "learning_rate": 0.00045799253204198,
      "loss": 2.2052,
      "step": 17190
    },
    {
      "epoch": 39.09090909090909,
      "grad_norm": 0.24789808690547943,
      "learning_rate": 0.0004579425007346561,
      "loss": 2.203,
      "step": 17200
    },
    {
      "epoch": 39.11363636363637,
      "grad_norm": 0.19937889277935028,
      "learning_rate": 0.00045789244238729596,
      "loss": 2.216,
      "step": 17210
    },
    {
      "epoch": 39.13636363636363,
      "grad_norm": 0.19673113524913788,
      "learning_rate": 0.000457842357006409,
      "loss": 2.219,
      "step": 17220
    },
    {
      "epoch": 39.15909090909091,
      "grad_norm": 0.2105095386505127,
      "learning_rate": 0.00045779224459850795,
      "loss": 2.2073,
      "step": 17230
    },
    {
      "epoch": 39.18181818181818,
      "grad_norm": 0.13841299712657928,
      "learning_rate": 0.0004577421051701094,
      "loss": 2.2043,
      "step": 17240
    },
    {
      "epoch": 39.20454545454545,
      "grad_norm": 0.3761883080005646,
      "learning_rate": 0.0004576919387277333,
      "loss": 2.2086,
      "step": 17250
    },
    {
      "epoch": 39.22727272727273,
      "grad_norm": 0.22576259076595306,
      "learning_rate": 0.00045764174527790304,
      "loss": 2.2086,
      "step": 17260
    },
    {
      "epoch": 39.25,
      "grad_norm": 0.17245781421661377,
      "learning_rate": 0.0004575915248271456,
      "loss": 2.2056,
      "step": 17270
    },
    {
      "epoch": 39.27272727272727,
      "grad_norm": 0.21076864004135132,
      "learning_rate": 0.00045754127738199135,
      "loss": 2.2078,
      "step": 17280
    },
    {
      "epoch": 39.29545454545455,
      "grad_norm": 0.8611497282981873,
      "learning_rate": 0.0004574910029489744,
      "loss": 2.2146,
      "step": 17290
    },
    {
      "epoch": 39.31818181818182,
      "grad_norm": 0.12848599255084991,
      "learning_rate": 0.0004574407015346323,
      "loss": 2.2068,
      "step": 17300
    },
    {
      "epoch": 39.34090909090909,
      "grad_norm": 0.2969507873058319,
      "learning_rate": 0.00045739037314550593,
      "loss": 2.2159,
      "step": 17310
    },
    {
      "epoch": 39.36363636363637,
      "grad_norm": 0.19332049787044525,
      "learning_rate": 0.0004573400177881397,
      "loss": 2.213,
      "step": 17320
    },
    {
      "epoch": 39.38636363636363,
      "grad_norm": 0.29963597655296326,
      "learning_rate": 0.00045728963546908186,
      "loss": 2.2115,
      "step": 17330
    },
    {
      "epoch": 39.40909090909091,
      "grad_norm": 0.4412853717803955,
      "learning_rate": 0.00045723922619488386,
      "loss": 2.2011,
      "step": 17340
    },
    {
      "epoch": 39.43181818181818,
      "grad_norm": 0.21431154012680054,
      "learning_rate": 0.00045718878997210057,
      "loss": 2.22,
      "step": 17350
    },
    {
      "epoch": 39.45454545454545,
      "grad_norm": 0.2097720205783844,
      "learning_rate": 0.0004571383268072907,
      "loss": 2.2034,
      "step": 17360
    },
    {
      "epoch": 39.47727272727273,
      "grad_norm": 0.23222418129444122,
      "learning_rate": 0.00045708783670701624,
      "loss": 2.2182,
      "step": 17370
    },
    {
      "epoch": 39.5,
      "grad_norm": 0.5431506037712097,
      "learning_rate": 0.00045703731967784266,
      "loss": 2.2258,
      "step": 17380
    },
    {
      "epoch": 39.52272727272727,
      "grad_norm": 0.18678006529808044,
      "learning_rate": 0.00045698677572633917,
      "loss": 2.2146,
      "step": 17390
    },
    {
      "epoch": 39.54545454545455,
      "grad_norm": 0.31451308727264404,
      "learning_rate": 0.00045693620485907807,
      "loss": 2.224,
      "step": 17400
    },
    {
      "epoch": 39.56818181818182,
      "grad_norm": 0.2113015353679657,
      "learning_rate": 0.0004568856070826356,
      "loss": 2.212,
      "step": 17410
    },
    {
      "epoch": 39.59090909090909,
      "grad_norm": 0.1605997383594513,
      "learning_rate": 0.00045683498240359113,
      "loss": 2.2114,
      "step": 17420
    },
    {
      "epoch": 39.61363636363637,
      "grad_norm": 0.3057977259159088,
      "learning_rate": 0.00045678433082852783,
      "loss": 2.2041,
      "step": 17430
    },
    {
      "epoch": 39.63636363636363,
      "grad_norm": 0.13013388216495514,
      "learning_rate": 0.00045673365236403217,
      "loss": 2.2059,
      "step": 17440
    },
    {
      "epoch": 39.65909090909091,
      "grad_norm": 0.15196245908737183,
      "learning_rate": 0.0004566829470166942,
      "loss": 2.2171,
      "step": 17450
    },
    {
      "epoch": 39.68181818181818,
      "grad_norm": 0.2221432477235794,
      "learning_rate": 0.00045663221479310746,
      "loss": 2.2103,
      "step": 17460
    },
    {
      "epoch": 39.70454545454545,
      "grad_norm": 0.4149906635284424,
      "learning_rate": 0.00045658145569986896,
      "loss": 2.2145,
      "step": 17470
    },
    {
      "epoch": 39.72727272727273,
      "grad_norm": 0.13047589361667633,
      "learning_rate": 0.00045653066974357925,
      "loss": 2.2102,
      "step": 17480
    },
    {
      "epoch": 39.75,
      "grad_norm": 0.6053430438041687,
      "learning_rate": 0.0004564798569308423,
      "loss": 2.2219,
      "step": 17490
    },
    {
      "epoch": 39.77272727272727,
      "grad_norm": 0.1278693974018097,
      "learning_rate": 0.0004564290172682655,
      "loss": 2.2023,
      "step": 17500
    },
    {
      "epoch": 39.79545454545455,
      "grad_norm": 0.3989163935184479,
      "learning_rate": 0.00045637815076246004,
      "loss": 2.2052,
      "step": 17510
    },
    {
      "epoch": 39.81818181818182,
      "grad_norm": 0.1277800351381302,
      "learning_rate": 0.00045632725742004033,
      "loss": 2.2218,
      "step": 17520
    },
    {
      "epoch": 39.84090909090909,
      "grad_norm": 0.16694670915603638,
      "learning_rate": 0.0004562763372476243,
      "loss": 2.207,
      "step": 17530
    },
    {
      "epoch": 39.86363636363637,
      "grad_norm": 0.24737103283405304,
      "learning_rate": 0.0004562253902518334,
      "loss": 2.2161,
      "step": 17540
    },
    {
      "epoch": 39.88636363636363,
      "grad_norm": 0.16730758547782898,
      "learning_rate": 0.0004561744164392927,
      "loss": 2.2169,
      "step": 17550
    },
    {
      "epoch": 39.90909090909091,
      "grad_norm": 0.16435499489307404,
      "learning_rate": 0.00045612341581663054,
      "loss": 2.2032,
      "step": 17560
    },
    {
      "epoch": 39.93181818181818,
      "grad_norm": 0.21660368144512177,
      "learning_rate": 0.0004560723883904788,
      "loss": 2.2108,
      "step": 17570
    },
    {
      "epoch": 39.95454545454545,
      "grad_norm": 0.17671142518520355,
      "learning_rate": 0.000456021334167473,
      "loss": 2.2077,
      "step": 17580
    },
    {
      "epoch": 39.97727272727273,
      "grad_norm": 0.21932001411914825,
      "learning_rate": 0.000455970253154252,
      "loss": 2.2075,
      "step": 17590
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.3912985026836395,
      "learning_rate": 0.0004559191453574582,
      "loss": 2.2193,
      "step": 17600
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.1019638776779175,
      "eval_runtime": 8.7628,
      "eval_samples_per_second": 3472.763,
      "eval_steps_per_second": 13.58,
      "step": 17600
    },
    {
      "epoch": 40.02272727272727,
      "grad_norm": 0.14574837684631348,
      "learning_rate": 0.0004558680107837374,
      "loss": 2.2032,
      "step": 17610
    },
    {
      "epoch": 40.04545454545455,
      "grad_norm": 0.18124496936798096,
      "learning_rate": 0.00045581684943973887,
      "loss": 2.2035,
      "step": 17620
    },
    {
      "epoch": 40.06818181818182,
      "grad_norm": 0.748960554599762,
      "learning_rate": 0.0004557656613321156,
      "loss": 2.2044,
      "step": 17630
    },
    {
      "epoch": 40.09090909090909,
      "grad_norm": 0.1319996416568756,
      "learning_rate": 0.00045571444646752383,
      "loss": 2.1992,
      "step": 17640
    },
    {
      "epoch": 40.11363636363637,
      "grad_norm": 0.15788529813289642,
      "learning_rate": 0.00045566320485262334,
      "loss": 2.2178,
      "step": 17650
    },
    {
      "epoch": 40.13636363636363,
      "grad_norm": 0.4442569613456726,
      "learning_rate": 0.00045561193649407735,
      "loss": 2.2099,
      "step": 17660
    },
    {
      "epoch": 40.15909090909091,
      "grad_norm": 0.1490955799818039,
      "learning_rate": 0.00045556064139855263,
      "loss": 2.2128,
      "step": 17670
    },
    {
      "epoch": 40.18181818181818,
      "grad_norm": 0.6054454445838928,
      "learning_rate": 0.0004555093195727194,
      "loss": 2.2162,
      "step": 17680
    },
    {
      "epoch": 40.20454545454545,
      "grad_norm": 0.4019271433353424,
      "learning_rate": 0.0004554579710232514,
      "loss": 2.2212,
      "step": 17690
    },
    {
      "epoch": 40.22727272727273,
      "grad_norm": 0.17517951130867004,
      "learning_rate": 0.00045540659575682565,
      "loss": 2.2148,
      "step": 17700
    },
    {
      "epoch": 40.25,
      "grad_norm": 0.151460662484169,
      "learning_rate": 0.0004553551937801229,
      "loss": 2.2116,
      "step": 17710
    },
    {
      "epoch": 40.27272727272727,
      "grad_norm": 0.17684969305992126,
      "learning_rate": 0.0004553037650998273,
      "loss": 2.2179,
      "step": 17720
    },
    {
      "epoch": 40.29545454545455,
      "grad_norm": 0.13639453053474426,
      "learning_rate": 0.0004552523097226263,
      "loss": 2.2214,
      "step": 17730
    },
    {
      "epoch": 40.31818181818182,
      "grad_norm": 0.17155808210372925,
      "learning_rate": 0.0004552008276552111,
      "loss": 2.2096,
      "step": 17740
    },
    {
      "epoch": 40.34090909090909,
      "grad_norm": 0.2530479431152344,
      "learning_rate": 0.0004551493189042762,
      "loss": 2.2072,
      "step": 17750
    },
    {
      "epoch": 40.36363636363637,
      "grad_norm": 0.30628418922424316,
      "learning_rate": 0.0004550977834765195,
      "loss": 2.2068,
      "step": 17760
    },
    {
      "epoch": 40.38636363636363,
      "grad_norm": 0.3639591336250305,
      "learning_rate": 0.0004550462213786426,
      "loss": 2.1948,
      "step": 17770
    },
    {
      "epoch": 40.40909090909091,
      "grad_norm": 0.49505922198295593,
      "learning_rate": 0.0004549946326173503,
      "loss": 2.2128,
      "step": 17780
    },
    {
      "epoch": 40.43181818181818,
      "grad_norm": 0.5672838091850281,
      "learning_rate": 0.0004549430171993511,
      "loss": 2.2082,
      "step": 17790
    },
    {
      "epoch": 40.45454545454545,
      "grad_norm": 0.1716424524784088,
      "learning_rate": 0.0004548913751313568,
      "loss": 2.2017,
      "step": 17800
    },
    {
      "epoch": 40.47727272727273,
      "grad_norm": 0.25901123881340027,
      "learning_rate": 0.00045483970642008287,
      "loss": 2.2221,
      "step": 17810
    },
    {
      "epoch": 40.5,
      "grad_norm": 0.2426338642835617,
      "learning_rate": 0.00045478801107224796,
      "loss": 2.213,
      "step": 17820
    },
    {
      "epoch": 40.52272727272727,
      "grad_norm": 0.15265582501888275,
      "learning_rate": 0.00045473628909457436,
      "loss": 2.2135,
      "step": 17830
    },
    {
      "epoch": 40.54545454545455,
      "grad_norm": 0.184892475605011,
      "learning_rate": 0.00045468454049378787,
      "loss": 2.2177,
      "step": 17840
    },
    {
      "epoch": 40.56818181818182,
      "grad_norm": 0.2273162454366684,
      "learning_rate": 0.00045463276527661764,
      "loss": 2.2105,
      "step": 17850
    },
    {
      "epoch": 40.59090909090909,
      "grad_norm": 0.19172538816928864,
      "learning_rate": 0.0004545809634497963,
      "loss": 2.201,
      "step": 17860
    },
    {
      "epoch": 40.61363636363637,
      "grad_norm": 0.1394749879837036,
      "learning_rate": 0.00045452913502005987,
      "loss": 2.2136,
      "step": 17870
    },
    {
      "epoch": 40.63636363636363,
      "grad_norm": 0.23193615674972534,
      "learning_rate": 0.0004544772799941481,
      "loss": 2.2155,
      "step": 17880
    },
    {
      "epoch": 40.65909090909091,
      "grad_norm": 0.1540483832359314,
      "learning_rate": 0.0004544253983788039,
      "loss": 2.1985,
      "step": 17890
    },
    {
      "epoch": 40.68181818181818,
      "grad_norm": 0.19334477186203003,
      "learning_rate": 0.00045437349018077384,
      "loss": 2.1996,
      "step": 17900
    },
    {
      "epoch": 40.70454545454545,
      "grad_norm": 0.19577434659004211,
      "learning_rate": 0.0004543215554068078,
      "loss": 2.2074,
      "step": 17910
    },
    {
      "epoch": 40.72727272727273,
      "grad_norm": 0.26254352927207947,
      "learning_rate": 0.0004542695940636591,
      "loss": 2.207,
      "step": 17920
    },
    {
      "epoch": 40.75,
      "grad_norm": 0.1932678073644638,
      "learning_rate": 0.0004542176061580847,
      "loss": 2.2101,
      "step": 17930
    },
    {
      "epoch": 40.77272727272727,
      "grad_norm": 0.534609854221344,
      "learning_rate": 0.0004541655916968449,
      "loss": 2.2158,
      "step": 17940
    },
    {
      "epoch": 40.79545454545455,
      "grad_norm": 0.2499013990163803,
      "learning_rate": 0.00045411355068670345,
      "loss": 2.2036,
      "step": 17950
    },
    {
      "epoch": 40.81818181818182,
      "grad_norm": 0.5727577805519104,
      "learning_rate": 0.00045406148313442755,
      "loss": 2.2112,
      "step": 17960
    },
    {
      "epoch": 40.84090909090909,
      "grad_norm": 0.5283634662628174,
      "learning_rate": 0.0004540093890467879,
      "loss": 2.2197,
      "step": 17970
    },
    {
      "epoch": 40.86363636363637,
      "grad_norm": 0.19727148115634918,
      "learning_rate": 0.0004539572684305585,
      "loss": 2.2077,
      "step": 17980
    },
    {
      "epoch": 40.88636363636363,
      "grad_norm": 0.33030909299850464,
      "learning_rate": 0.00045390512129251704,
      "loss": 2.2131,
      "step": 17990
    },
    {
      "epoch": 40.90909090909091,
      "grad_norm": 0.2176087647676468,
      "learning_rate": 0.0004538529476394444,
      "loss": 2.2204,
      "step": 18000
    },
    {
      "epoch": 40.93181818181818,
      "grad_norm": 0.29350414872169495,
      "learning_rate": 0.00045380074747812516,
      "loss": 2.2175,
      "step": 18010
    },
    {
      "epoch": 40.95454545454545,
      "grad_norm": 0.6428929567337036,
      "learning_rate": 0.00045374852081534725,
      "loss": 2.2104,
      "step": 18020
    },
    {
      "epoch": 40.97727272727273,
      "grad_norm": 0.15320439636707306,
      "learning_rate": 0.0004536962676579018,
      "loss": 2.2076,
      "step": 18030
    },
    {
      "epoch": 41.0,
      "grad_norm": 0.8906396627426147,
      "learning_rate": 0.00045364398801258396,
      "loss": 2.2078,
      "step": 18040
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.101967692375183,
      "eval_runtime": 8.714,
      "eval_samples_per_second": 3492.211,
      "eval_steps_per_second": 13.656,
      "step": 18040
    },
    {
      "epoch": 41.02272727272727,
      "grad_norm": 0.5720186829566956,
      "learning_rate": 0.00045359168188619165,
      "loss": 2.2132,
      "step": 18050
    },
    {
      "epoch": 41.04545454545455,
      "grad_norm": 0.3030216097831726,
      "learning_rate": 0.00045353934928552675,
      "loss": 2.2019,
      "step": 18060
    },
    {
      "epoch": 41.06818181818182,
      "grad_norm": 0.5172249674797058,
      "learning_rate": 0.00045348699021739426,
      "loss": 2.2099,
      "step": 18070
    },
    {
      "epoch": 41.09090909090909,
      "grad_norm": 0.31409284472465515,
      "learning_rate": 0.00045343460468860286,
      "loss": 2.2217,
      "step": 18080
    },
    {
      "epoch": 41.11363636363637,
      "grad_norm": 0.35572561621665955,
      "learning_rate": 0.00045338219270596447,
      "loss": 2.2136,
      "step": 18090
    },
    {
      "epoch": 41.13636363636363,
      "grad_norm": 0.3583606481552124,
      "learning_rate": 0.0004533297542762946,
      "loss": 2.2134,
      "step": 18100
    },
    {
      "epoch": 41.15909090909091,
      "grad_norm": 0.5338587760925293,
      "learning_rate": 0.00045327728940641217,
      "loss": 2.2208,
      "step": 18110
    },
    {
      "epoch": 41.18181818181818,
      "grad_norm": 0.48830610513687134,
      "learning_rate": 0.0004532247981031394,
      "loss": 2.2095,
      "step": 18120
    },
    {
      "epoch": 41.20454545454545,
      "grad_norm": 0.25244826078414917,
      "learning_rate": 0.00045317228037330216,
      "loss": 2.2092,
      "step": 18130
    },
    {
      "epoch": 41.22727272727273,
      "grad_norm": 0.1943693310022354,
      "learning_rate": 0.0004531197362237296,
      "loss": 2.2189,
      "step": 18140
    },
    {
      "epoch": 41.25,
      "grad_norm": 0.19134840369224548,
      "learning_rate": 0.00045306716566125433,
      "loss": 2.207,
      "step": 18150
    },
    {
      "epoch": 41.27272727272727,
      "grad_norm": 0.23419469594955444,
      "learning_rate": 0.0004530145686927125,
      "loss": 2.205,
      "step": 18160
    },
    {
      "epoch": 41.29545454545455,
      "grad_norm": 0.28233852982521057,
      "learning_rate": 0.0004529619453249436,
      "loss": 2.208,
      "step": 18170
    },
    {
      "epoch": 41.31818181818182,
      "grad_norm": 0.3466818332672119,
      "learning_rate": 0.0004529092955647905,
      "loss": 2.2035,
      "step": 18180
    },
    {
      "epoch": 41.34090909090909,
      "grad_norm": 0.3285422623157501,
      "learning_rate": 0.00045285661941909963,
      "loss": 2.2188,
      "step": 18190
    },
    {
      "epoch": 41.36363636363637,
      "grad_norm": 0.29885634779930115,
      "learning_rate": 0.00045280391689472073,
      "loss": 2.2122,
      "step": 18200
    },
    {
      "epoch": 41.38636363636363,
      "grad_norm": 1.2211027145385742,
      "learning_rate": 0.0004527511879985071,
      "loss": 2.2238,
      "step": 18210
    },
    {
      "epoch": 41.40909090909091,
      "grad_norm": 0.21347349882125854,
      "learning_rate": 0.00045269843273731535,
      "loss": 2.2158,
      "step": 18220
    },
    {
      "epoch": 41.43181818181818,
      "grad_norm": 0.23287419974803925,
      "learning_rate": 0.0004526456511180057,
      "loss": 2.203,
      "step": 18230
    },
    {
      "epoch": 41.45454545454545,
      "grad_norm": 2.2895121574401855,
      "learning_rate": 0.00045259284314744155,
      "loss": 2.2165,
      "step": 18240
    },
    {
      "epoch": 41.47727272727273,
      "grad_norm": 0.2788012623786926,
      "learning_rate": 0.0004525400088324898,
      "loss": 2.2022,
      "step": 18250
    },
    {
      "epoch": 41.5,
      "grad_norm": 0.2082124799489975,
      "learning_rate": 0.0004524871481800209,
      "loss": 2.212,
      "step": 18260
    },
    {
      "epoch": 41.52272727272727,
      "grad_norm": 1.9129101037979126,
      "learning_rate": 0.00045243426119690854,
      "loss": 2.1989,
      "step": 18270
    },
    {
      "epoch": 41.54545454545455,
      "grad_norm": 0.190190851688385,
      "learning_rate": 0.00045238134789003017,
      "loss": 2.205,
      "step": 18280
    },
    {
      "epoch": 41.56818181818182,
      "grad_norm": 0.34471940994262695,
      "learning_rate": 0.0004523284082662662,
      "loss": 2.2074,
      "step": 18290
    },
    {
      "epoch": 41.59090909090909,
      "grad_norm": 0.2926630675792694,
      "learning_rate": 0.00045227544233250074,
      "loss": 2.2,
      "step": 18300
    },
    {
      "epoch": 41.61363636363637,
      "grad_norm": 0.38116616010665894,
      "learning_rate": 0.00045222245009562137,
      "loss": 2.2068,
      "step": 18310
    },
    {
      "epoch": 41.63636363636363,
      "grad_norm": 0.2714279890060425,
      "learning_rate": 0.00045216943156251894,
      "loss": 2.2222,
      "step": 18320
    },
    {
      "epoch": 41.65909090909091,
      "grad_norm": 0.32326847314834595,
      "learning_rate": 0.00045211638674008773,
      "loss": 2.2111,
      "step": 18330
    },
    {
      "epoch": 41.68181818181818,
      "grad_norm": 0.29187527298927307,
      "learning_rate": 0.00045206331563522553,
      "loss": 2.2016,
      "step": 18340
    },
    {
      "epoch": 41.70454545454545,
      "grad_norm": 0.20599621534347534,
      "learning_rate": 0.0004520102182548335,
      "loss": 2.2038,
      "step": 18350
    },
    {
      "epoch": 41.72727272727273,
      "grad_norm": 0.22167474031448364,
      "learning_rate": 0.00045195709460581625,
      "loss": 2.1991,
      "step": 18360
    },
    {
      "epoch": 41.75,
      "grad_norm": 0.18498282134532928,
      "learning_rate": 0.0004519039446950817,
      "loss": 2.2017,
      "step": 18370
    },
    {
      "epoch": 41.77272727272727,
      "grad_norm": 0.5098310708999634,
      "learning_rate": 0.0004518507685295413,
      "loss": 2.2032,
      "step": 18380
    },
    {
      "epoch": 41.79545454545455,
      "grad_norm": 0.1750514656305313,
      "learning_rate": 0.00045179756611610976,
      "loss": 2.2176,
      "step": 18390
    },
    {
      "epoch": 41.81818181818182,
      "grad_norm": 0.5566344261169434,
      "learning_rate": 0.00045174433746170557,
      "loss": 2.2147,
      "step": 18400
    },
    {
      "epoch": 41.84090909090909,
      "grad_norm": 0.24752067029476166,
      "learning_rate": 0.0004516910825732502,
      "loss": 2.2125,
      "step": 18410
    },
    {
      "epoch": 41.86363636363637,
      "grad_norm": 0.33937856554985046,
      "learning_rate": 0.00045163780145766864,
      "loss": 2.21,
      "step": 18420
    },
    {
      "epoch": 41.88636363636363,
      "grad_norm": 0.3644915819168091,
      "learning_rate": 0.0004515844941218895,
      "loss": 2.2087,
      "step": 18430
    },
    {
      "epoch": 41.90909090909091,
      "grad_norm": 0.25786709785461426,
      "learning_rate": 0.0004515311605728447,
      "loss": 2.2166,
      "step": 18440
    },
    {
      "epoch": 41.93181818181818,
      "grad_norm": 0.18999743461608887,
      "learning_rate": 0.00045147780081746935,
      "loss": 2.2201,
      "step": 18450
    },
    {
      "epoch": 41.95454545454545,
      "grad_norm": 0.19697986543178558,
      "learning_rate": 0.0004514244148627023,
      "loss": 2.2007,
      "step": 18460
    },
    {
      "epoch": 41.97727272727273,
      "grad_norm": 0.20719236135482788,
      "learning_rate": 0.0004513710027154856,
      "loss": 2.2008,
      "step": 18470
    },
    {
      "epoch": 42.0,
      "grad_norm": 0.22360943257808685,
      "learning_rate": 0.00045131756438276466,
      "loss": 2.2184,
      "step": 18480
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.1016103029251099,
      "eval_runtime": 8.7109,
      "eval_samples_per_second": 3493.438,
      "eval_steps_per_second": 13.661,
      "step": 18480
    },
    {
      "epoch": 42.02272727272727,
      "grad_norm": 0.16524659097194672,
      "learning_rate": 0.0004512640998714885,
      "loss": 2.2122,
      "step": 18490
    },
    {
      "epoch": 42.04545454545455,
      "grad_norm": 0.2817862033843994,
      "learning_rate": 0.00045121060918860945,
      "loss": 2.2006,
      "step": 18500
    },
    {
      "epoch": 42.06818181818182,
      "grad_norm": 0.34497690200805664,
      "learning_rate": 0.0004511570923410833,
      "loss": 2.2052,
      "step": 18510
    },
    {
      "epoch": 42.09090909090909,
      "grad_norm": 0.20868220925331116,
      "learning_rate": 0.00045110354933586896,
      "loss": 2.2128,
      "step": 18520
    },
    {
      "epoch": 42.11363636363637,
      "grad_norm": 0.3173079192638397,
      "learning_rate": 0.00045104998017992905,
      "loss": 2.2009,
      "step": 18530
    },
    {
      "epoch": 42.13636363636363,
      "grad_norm": 0.5681138634681702,
      "learning_rate": 0.0004509963848802295,
      "loss": 2.2037,
      "step": 18540
    },
    {
      "epoch": 42.15909090909091,
      "grad_norm": 0.3121231198310852,
      "learning_rate": 0.0004509427634437397,
      "loss": 2.2123,
      "step": 18550
    },
    {
      "epoch": 42.18181818181818,
      "grad_norm": 0.1870271861553192,
      "learning_rate": 0.00045088911587743243,
      "loss": 2.1979,
      "step": 18560
    },
    {
      "epoch": 42.20454545454545,
      "grad_norm": 0.20689013600349426,
      "learning_rate": 0.00045083544218828356,
      "loss": 2.2108,
      "step": 18570
    },
    {
      "epoch": 42.22727272727273,
      "grad_norm": 0.33778512477874756,
      "learning_rate": 0.0004507817423832728,
      "loss": 2.2084,
      "step": 18580
    },
    {
      "epoch": 42.25,
      "grad_norm": 0.2592008113861084,
      "learning_rate": 0.000450728016469383,
      "loss": 2.2046,
      "step": 18590
    },
    {
      "epoch": 42.27272727272727,
      "grad_norm": 0.1707744002342224,
      "learning_rate": 0.0004506742644536005,
      "loss": 2.2146,
      "step": 18600
    },
    {
      "epoch": 42.29545454545455,
      "grad_norm": 0.1642487794160843,
      "learning_rate": 0.00045062048634291504,
      "loss": 2.195,
      "step": 18610
    },
    {
      "epoch": 42.31818181818182,
      "grad_norm": 0.21794234216213226,
      "learning_rate": 0.0004505666821443196,
      "loss": 2.2087,
      "step": 18620
    },
    {
      "epoch": 42.34090909090909,
      "grad_norm": 3.034459114074707,
      "learning_rate": 0.0004505128518648107,
      "loss": 2.2087,
      "step": 18630
    },
    {
      "epoch": 42.36363636363637,
      "grad_norm": 0.6883299350738525,
      "learning_rate": 0.0004504589955113884,
      "loss": 2.2156,
      "step": 18640
    },
    {
      "epoch": 42.38636363636363,
      "grad_norm": 0.2955729067325592,
      "learning_rate": 0.0004504051130910557,
      "loss": 2.2196,
      "step": 18650
    },
    {
      "epoch": 42.40909090909091,
      "grad_norm": 0.17609918117523193,
      "learning_rate": 0.0004503512046108195,
      "loss": 2.2121,
      "step": 18660
    },
    {
      "epoch": 42.43181818181818,
      "grad_norm": 0.2952614426612854,
      "learning_rate": 0.0004502972700776896,
      "loss": 2.219,
      "step": 18670
    },
    {
      "epoch": 42.45454545454545,
      "grad_norm": 0.30200931429862976,
      "learning_rate": 0.0004502433094986797,
      "loss": 2.213,
      "step": 18680
    },
    {
      "epoch": 42.47727272727273,
      "grad_norm": 0.19337546825408936,
      "learning_rate": 0.00045018932288080644,
      "loss": 2.2062,
      "step": 18690
    },
    {
      "epoch": 42.5,
      "grad_norm": 0.22399085760116577,
      "learning_rate": 0.0004501353102310901,
      "loss": 2.218,
      "step": 18700
    },
    {
      "epoch": 42.52272727272727,
      "grad_norm": 2.326030731201172,
      "learning_rate": 0.00045008127155655433,
      "loss": 2.2084,
      "step": 18710
    },
    {
      "epoch": 42.54545454545455,
      "grad_norm": 0.21639816462993622,
      "learning_rate": 0.0004500272068642259,
      "loss": 2.2159,
      "step": 18720
    },
    {
      "epoch": 42.56818181818182,
      "grad_norm": 0.47552603483200073,
      "learning_rate": 0.00044997311616113545,
      "loss": 2.2213,
      "step": 18730
    },
    {
      "epoch": 42.59090909090909,
      "grad_norm": 0.2140756994485855,
      "learning_rate": 0.00044991899945431656,
      "loss": 2.2092,
      "step": 18740
    },
    {
      "epoch": 42.61363636363637,
      "grad_norm": 0.7208385467529297,
      "learning_rate": 0.00044986485675080633,
      "loss": 2.2074,
      "step": 18750
    },
    {
      "epoch": 42.63636363636363,
      "grad_norm": 0.2647027373313904,
      "learning_rate": 0.00044981068805764545,
      "loss": 2.2078,
      "step": 18760
    },
    {
      "epoch": 42.65909090909091,
      "grad_norm": 0.21204116940498352,
      "learning_rate": 0.0004497564933818776,
      "loss": 2.2092,
      "step": 18770
    },
    {
      "epoch": 42.68181818181818,
      "grad_norm": 1.7030690908432007,
      "learning_rate": 0.0004497022727305502,
      "loss": 2.2061,
      "step": 18780
    },
    {
      "epoch": 42.70454545454545,
      "grad_norm": 1.594502329826355,
      "learning_rate": 0.00044964802611071377,
      "loss": 2.2051,
      "step": 18790
    },
    {
      "epoch": 42.72727272727273,
      "grad_norm": 1.725441575050354,
      "learning_rate": 0.0004495937535294224,
      "loss": 2.2246,
      "step": 18800
    },
    {
      "epoch": 42.75,
      "grad_norm": 0.1904222071170807,
      "learning_rate": 0.0004495394549937335,
      "loss": 2.2,
      "step": 18810
    },
    {
      "epoch": 42.77272727272727,
      "grad_norm": 0.8079461455345154,
      "learning_rate": 0.0004494851305107077,
      "loss": 2.2183,
      "step": 18820
    },
    {
      "epoch": 42.79545454545455,
      "grad_norm": 0.29492780566215515,
      "learning_rate": 0.00044943078008740934,
      "loss": 2.1927,
      "step": 18830
    },
    {
      "epoch": 42.81818181818182,
      "grad_norm": 0.309556782245636,
      "learning_rate": 0.0004493764037309059,
      "loss": 2.2062,
      "step": 18840
    },
    {
      "epoch": 42.84090909090909,
      "grad_norm": 0.3386155664920807,
      "learning_rate": 0.0004493220014482682,
      "loss": 2.2169,
      "step": 18850
    },
    {
      "epoch": 42.86363636363637,
      "grad_norm": 0.19010677933692932,
      "learning_rate": 0.00044926757324657043,
      "loss": 2.2042,
      "step": 18860
    },
    {
      "epoch": 42.88636363636363,
      "grad_norm": 1.1296720504760742,
      "learning_rate": 0.0004492131191328904,
      "loss": 2.2058,
      "step": 18870
    },
    {
      "epoch": 42.90909090909091,
      "grad_norm": 0.19177445769309998,
      "learning_rate": 0.000449158639114309,
      "loss": 2.208,
      "step": 18880
    },
    {
      "epoch": 42.93181818181818,
      "grad_norm": 0.20200952887535095,
      "learning_rate": 0.0004491041331979107,
      "loss": 2.2145,
      "step": 18890
    },
    {
      "epoch": 42.95454545454545,
      "grad_norm": 0.2555498480796814,
      "learning_rate": 0.0004490496013907831,
      "loss": 2.2042,
      "step": 18900
    },
    {
      "epoch": 42.97727272727273,
      "grad_norm": 2.596527338027954,
      "learning_rate": 0.00044899504370001745,
      "loss": 2.2153,
      "step": 18910
    },
    {
      "epoch": 43.0,
      "grad_norm": 0.3286433219909668,
      "learning_rate": 0.0004489404601327081,
      "loss": 2.2125,
      "step": 18920
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.101584553718567,
      "eval_runtime": 8.7221,
      "eval_samples_per_second": 3488.961,
      "eval_steps_per_second": 13.644,
      "step": 18920
    },
    {
      "epoch": 43.02272727272727,
      "grad_norm": 0.2124025523662567,
      "learning_rate": 0.0004488858506959529,
      "loss": 2.2094,
      "step": 18930
    },
    {
      "epoch": 43.04545454545455,
      "grad_norm": 0.19798845052719116,
      "learning_rate": 0.0004488312153968531,
      "loss": 2.2128,
      "step": 18940
    },
    {
      "epoch": 43.06818181818182,
      "grad_norm": 1.0778062343597412,
      "learning_rate": 0.00044877655424251317,
      "loss": 2.2072,
      "step": 18950
    },
    {
      "epoch": 43.09090909090909,
      "grad_norm": 0.1923610419034958,
      "learning_rate": 0.0004487218672400412,
      "loss": 2.217,
      "step": 18960
    },
    {
      "epoch": 43.11363636363637,
      "grad_norm": 0.20788919925689697,
      "learning_rate": 0.00044866715439654837,
      "loss": 2.2017,
      "step": 18970
    },
    {
      "epoch": 43.13636363636363,
      "grad_norm": 0.3448203206062317,
      "learning_rate": 0.00044861241571914935,
      "loss": 2.2052,
      "step": 18980
    },
    {
      "epoch": 43.15909090909091,
      "grad_norm": 0.17829565703868866,
      "learning_rate": 0.00044855765121496204,
      "loss": 2.2015,
      "step": 18990
    },
    {
      "epoch": 43.18181818181818,
      "grad_norm": 0.18142710626125336,
      "learning_rate": 0.00044850286089110806,
      "loss": 2.2145,
      "step": 19000
    },
    {
      "epoch": 43.20454545454545,
      "grad_norm": 0.5941200852394104,
      "learning_rate": 0.00044844804475471187,
      "loss": 2.2083,
      "step": 19010
    },
    {
      "epoch": 43.22727272727273,
      "grad_norm": 0.1970399022102356,
      "learning_rate": 0.0004483932028129016,
      "loss": 2.2083,
      "step": 19020
    },
    {
      "epoch": 43.25,
      "grad_norm": 0.2681015729904175,
      "learning_rate": 0.0004483383350728088,
      "loss": 2.2032,
      "step": 19030
    },
    {
      "epoch": 43.27272727272727,
      "grad_norm": 0.2014351189136505,
      "learning_rate": 0.0004482834415415682,
      "loss": 2.2105,
      "step": 19040
    },
    {
      "epoch": 43.29545454545455,
      "grad_norm": 0.4339183568954468,
      "learning_rate": 0.00044822852222631786,
      "loss": 2.2041,
      "step": 19050
    },
    {
      "epoch": 43.31818181818182,
      "grad_norm": 0.1951480358839035,
      "learning_rate": 0.0004481735771341994,
      "loss": 2.198,
      "step": 19060
    },
    {
      "epoch": 43.34090909090909,
      "grad_norm": 0.3939746022224426,
      "learning_rate": 0.0004481186062723576,
      "loss": 2.2116,
      "step": 19070
    },
    {
      "epoch": 43.36363636363637,
      "grad_norm": 0.5256261825561523,
      "learning_rate": 0.0004480636096479407,
      "loss": 2.2157,
      "step": 19080
    },
    {
      "epoch": 43.38636363636363,
      "grad_norm": 0.1439868062734604,
      "learning_rate": 0.0004480085872681001,
      "loss": 2.1989,
      "step": 19090
    },
    {
      "epoch": 43.40909090909091,
      "grad_norm": 0.2876644730567932,
      "learning_rate": 0.0004479535391399909,
      "loss": 2.2091,
      "step": 19100
    },
    {
      "epoch": 43.43181818181818,
      "grad_norm": 0.5885746479034424,
      "learning_rate": 0.00044789846527077125,
      "loss": 2.2047,
      "step": 19110
    },
    {
      "epoch": 43.45454545454545,
      "grad_norm": 1.0049316883087158,
      "learning_rate": 0.00044784336566760277,
      "loss": 2.2133,
      "step": 19120
    },
    {
      "epoch": 43.47727272727273,
      "grad_norm": 0.20714348554611206,
      "learning_rate": 0.0004477882403376503,
      "loss": 2.2161,
      "step": 19130
    },
    {
      "epoch": 43.5,
      "grad_norm": 0.38785988092422485,
      "learning_rate": 0.0004477330892880823,
      "loss": 2.216,
      "step": 19140
    },
    {
      "epoch": 43.52272727272727,
      "grad_norm": 0.2384566068649292,
      "learning_rate": 0.0004476779125260703,
      "loss": 2.2131,
      "step": 19150
    },
    {
      "epoch": 43.54545454545455,
      "grad_norm": 0.860846757888794,
      "learning_rate": 0.00044762271005878917,
      "loss": 2.2042,
      "step": 19160
    },
    {
      "epoch": 43.56818181818182,
      "grad_norm": 0.19042657315731049,
      "learning_rate": 0.00044756748189341743,
      "loss": 2.2023,
      "step": 19170
    },
    {
      "epoch": 43.59090909090909,
      "grad_norm": 0.17207099497318268,
      "learning_rate": 0.0004475122280371366,
      "loss": 2.2096,
      "step": 19180
    },
    {
      "epoch": 43.61363636363637,
      "grad_norm": 0.1746196448802948,
      "learning_rate": 0.0004474569484971317,
      "loss": 2.2019,
      "step": 19190
    },
    {
      "epoch": 43.63636363636363,
      "grad_norm": 0.1358814835548401,
      "learning_rate": 0.00044740164328059106,
      "loss": 2.2176,
      "step": 19200
    },
    {
      "epoch": 43.65909090909091,
      "grad_norm": 0.13346977531909943,
      "learning_rate": 0.00044734631239470645,
      "loss": 2.2152,
      "step": 19210
    },
    {
      "epoch": 43.68181818181818,
      "grad_norm": 0.2579842209815979,
      "learning_rate": 0.0004472909558466728,
      "loss": 2.2129,
      "step": 19220
    },
    {
      "epoch": 43.70454545454545,
      "grad_norm": 0.3695284128189087,
      "learning_rate": 0.00044723557364368836,
      "loss": 2.2004,
      "step": 19230
    },
    {
      "epoch": 43.72727272727273,
      "grad_norm": 0.29710495471954346,
      "learning_rate": 0.0004471801657929551,
      "loss": 2.2077,
      "step": 19240
    },
    {
      "epoch": 43.75,
      "grad_norm": 0.2404385507106781,
      "learning_rate": 0.0004471247323016777,
      "loss": 2.2114,
      "step": 19250
    },
    {
      "epoch": 43.77272727272727,
      "grad_norm": 0.23978617787361145,
      "learning_rate": 0.0004470692731770648,
      "loss": 2.2043,
      "step": 19260
    },
    {
      "epoch": 43.79545454545455,
      "grad_norm": 0.3081519305706024,
      "learning_rate": 0.00044701378842632787,
      "loss": 2.2057,
      "step": 19270
    },
    {
      "epoch": 43.81818181818182,
      "grad_norm": 0.5212291479110718,
      "learning_rate": 0.0004469582780566821,
      "loss": 2.2109,
      "step": 19280
    },
    {
      "epoch": 43.84090909090909,
      "grad_norm": 0.2115706205368042,
      "learning_rate": 0.0004469027420753458,
      "loss": 2.2009,
      "step": 19290
    },
    {
      "epoch": 43.86363636363637,
      "grad_norm": 0.6966915726661682,
      "learning_rate": 0.0004468471804895406,
      "loss": 2.2173,
      "step": 19300
    },
    {
      "epoch": 43.88636363636363,
      "grad_norm": 0.9267813563346863,
      "learning_rate": 0.00044679159330649154,
      "loss": 2.2284,
      "step": 19310
    },
    {
      "epoch": 43.90909090909091,
      "grad_norm": 0.213460311293602,
      "learning_rate": 0.000446735980533427,
      "loss": 2.2144,
      "step": 19320
    },
    {
      "epoch": 43.93181818181818,
      "grad_norm": 0.23054338991641998,
      "learning_rate": 0.00044668034217757856,
      "loss": 2.2078,
      "step": 19330
    },
    {
      "epoch": 43.95454545454545,
      "grad_norm": 1.1569064855575562,
      "learning_rate": 0.00044662467824618134,
      "loss": 2.2113,
      "step": 19340
    },
    {
      "epoch": 43.97727272727273,
      "grad_norm": 0.174917072057724,
      "learning_rate": 0.0004465689887464736,
      "loss": 2.2103,
      "step": 19350
    },
    {
      "epoch": 44.0,
      "grad_norm": 0.44703325629234314,
      "learning_rate": 0.0004465132736856969,
      "loss": 2.2113,
      "step": 19360
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.1013306379318237,
      "eval_runtime": 8.72,
      "eval_samples_per_second": 3489.811,
      "eval_steps_per_second": 13.647,
      "step": 19360
    },
    {
      "epoch": 44.02272727272727,
      "grad_norm": 0.1830533742904663,
      "learning_rate": 0.00044645753307109626,
      "loss": 2.2158,
      "step": 19370
    },
    {
      "epoch": 44.04545454545455,
      "grad_norm": 0.2794003188610077,
      "learning_rate": 0.00044640176690992014,
      "loss": 2.2156,
      "step": 19380
    },
    {
      "epoch": 44.06818181818182,
      "grad_norm": 0.3472379446029663,
      "learning_rate": 0.00044634597520942,
      "loss": 2.2061,
      "step": 19390
    },
    {
      "epoch": 44.09090909090909,
      "grad_norm": 0.29102039337158203,
      "learning_rate": 0.0004462901579768507,
      "loss": 2.2167,
      "step": 19400
    },
    {
      "epoch": 44.11363636363637,
      "grad_norm": 0.2792240381240845,
      "learning_rate": 0.00044623431521947064,
      "loss": 2.2059,
      "step": 19410
    },
    {
      "epoch": 44.13636363636363,
      "grad_norm": 0.25785157084465027,
      "learning_rate": 0.00044617844694454136,
      "loss": 2.2043,
      "step": 19420
    },
    {
      "epoch": 44.15909090909091,
      "grad_norm": 0.2717183232307434,
      "learning_rate": 0.0004461225531593278,
      "loss": 2.2043,
      "step": 19430
    },
    {
      "epoch": 44.18181818181818,
      "grad_norm": 0.42216888070106506,
      "learning_rate": 0.00044606663387109803,
      "loss": 2.206,
      "step": 19440
    },
    {
      "epoch": 44.20454545454545,
      "grad_norm": 0.17455610632896423,
      "learning_rate": 0.0004460106890871237,
      "loss": 2.2137,
      "step": 19450
    },
    {
      "epoch": 44.22727272727273,
      "grad_norm": 0.15332987904548645,
      "learning_rate": 0.0004459547188146795,
      "loss": 2.2154,
      "step": 19460
    },
    {
      "epoch": 44.25,
      "grad_norm": 0.366253137588501,
      "learning_rate": 0.00044589872306104384,
      "loss": 2.2019,
      "step": 19470
    },
    {
      "epoch": 44.27272727272727,
      "grad_norm": 0.26155656576156616,
      "learning_rate": 0.00044584270183349806,
      "loss": 2.2185,
      "step": 19480
    },
    {
      "epoch": 44.29545454545455,
      "grad_norm": 0.3958949148654938,
      "learning_rate": 0.00044578665513932684,
      "loss": 2.2141,
      "step": 19490
    },
    {
      "epoch": 44.31818181818182,
      "grad_norm": 0.26261472702026367,
      "learning_rate": 0.00044573058298581834,
      "loss": 2.1905,
      "step": 19500
    },
    {
      "epoch": 44.34090909090909,
      "grad_norm": 0.5553473234176636,
      "learning_rate": 0.0004456744853802641,
      "loss": 2.2235,
      "step": 19510
    },
    {
      "epoch": 44.36363636363637,
      "grad_norm": 0.4803883135318756,
      "learning_rate": 0.00044561836232995874,
      "loss": 2.2089,
      "step": 19520
    },
    {
      "epoch": 44.38636363636363,
      "grad_norm": 0.275643527507782,
      "learning_rate": 0.0004455622138422002,
      "loss": 2.2172,
      "step": 19530
    },
    {
      "epoch": 44.40909090909091,
      "grad_norm": 0.2168462872505188,
      "learning_rate": 0.00044550603992429,
      "loss": 2.2056,
      "step": 19540
    },
    {
      "epoch": 44.43181818181818,
      "grad_norm": 0.29000067710876465,
      "learning_rate": 0.0004454498405835325,
      "loss": 2.2028,
      "step": 19550
    },
    {
      "epoch": 44.45454545454545,
      "grad_norm": 0.2819092869758606,
      "learning_rate": 0.00044539361582723587,
      "loss": 2.219,
      "step": 19560
    },
    {
      "epoch": 44.47727272727273,
      "grad_norm": 0.43944159150123596,
      "learning_rate": 0.00044533736566271134,
      "loss": 2.2212,
      "step": 19570
    },
    {
      "epoch": 44.5,
      "grad_norm": 0.22299788892269135,
      "learning_rate": 0.00044528109009727335,
      "loss": 2.202,
      "step": 19580
    },
    {
      "epoch": 44.52272727272727,
      "grad_norm": 0.274513840675354,
      "learning_rate": 0.00044522478913823987,
      "loss": 2.2158,
      "step": 19590
    },
    {
      "epoch": 44.54545454545455,
      "grad_norm": 0.14708153903484344,
      "learning_rate": 0.000445168462792932,
      "loss": 2.2028,
      "step": 19600
    },
    {
      "epoch": 44.56818181818182,
      "grad_norm": 0.45731934905052185,
      "learning_rate": 0.0004451121110686742,
      "loss": 2.2116,
      "step": 19610
    },
    {
      "epoch": 44.59090909090909,
      "grad_norm": 0.3004244863986969,
      "learning_rate": 0.0004450557339727943,
      "loss": 2.2128,
      "step": 19620
    },
    {
      "epoch": 44.61363636363637,
      "grad_norm": 0.33345678448677063,
      "learning_rate": 0.0004449993315126233,
      "loss": 2.2022,
      "step": 19630
    },
    {
      "epoch": 44.63636363636363,
      "grad_norm": 0.3560604453086853,
      "learning_rate": 0.0004449429036954955,
      "loss": 2.2123,
      "step": 19640
    },
    {
      "epoch": 44.65909090909091,
      "grad_norm": 0.22414419054985046,
      "learning_rate": 0.00044488645052874866,
      "loss": 2.2049,
      "step": 19650
    },
    {
      "epoch": 44.68181818181818,
      "grad_norm": 0.27733638882637024,
      "learning_rate": 0.00044482997201972366,
      "loss": 2.2098,
      "step": 19660
    },
    {
      "epoch": 44.70454545454545,
      "grad_norm": 0.35509929060935974,
      "learning_rate": 0.0004447734681757648,
      "loss": 2.2135,
      "step": 19670
    },
    {
      "epoch": 44.72727272727273,
      "grad_norm": 0.20348532497882843,
      "learning_rate": 0.00044471693900421954,
      "loss": 2.2082,
      "step": 19680
    },
    {
      "epoch": 44.75,
      "grad_norm": 0.899233877658844,
      "learning_rate": 0.0004446603845124388,
      "loss": 2.1918,
      "step": 19690
    },
    {
      "epoch": 44.77272727272727,
      "grad_norm": 0.1581404209136963,
      "learning_rate": 0.00044460380470777666,
      "loss": 2.2209,
      "step": 19700
    },
    {
      "epoch": 44.79545454545455,
      "grad_norm": 0.1477651596069336,
      "learning_rate": 0.0004445471995975906,
      "loss": 2.2037,
      "step": 19710
    },
    {
      "epoch": 44.81818181818182,
      "grad_norm": 0.18556123971939087,
      "learning_rate": 0.0004444905691892412,
      "loss": 2.2068,
      "step": 19720
    },
    {
      "epoch": 44.84090909090909,
      "grad_norm": 0.2907581329345703,
      "learning_rate": 0.0004444339134900926,
      "loss": 2.2093,
      "step": 19730
    },
    {
      "epoch": 44.86363636363637,
      "grad_norm": 0.14625738561153412,
      "learning_rate": 0.000444377232507512,
      "loss": 2.2139,
      "step": 19740
    },
    {
      "epoch": 44.88636363636363,
      "grad_norm": 0.3266525864601135,
      "learning_rate": 0.00044432052624887,
      "loss": 2.209,
      "step": 19750
    },
    {
      "epoch": 44.90909090909091,
      "grad_norm": 0.2574315369129181,
      "learning_rate": 0.0004442637947215405,
      "loss": 2.1888,
      "step": 19760
    },
    {
      "epoch": 44.93181818181818,
      "grad_norm": 0.21881717443466187,
      "learning_rate": 0.0004442070379329006,
      "loss": 2.2103,
      "step": 19770
    },
    {
      "epoch": 44.95454545454545,
      "grad_norm": 0.28196975588798523,
      "learning_rate": 0.00044415025589033066,
      "loss": 2.2093,
      "step": 19780
    },
    {
      "epoch": 44.97727272727273,
      "grad_norm": 0.3570021092891693,
      "learning_rate": 0.0004440934486012146,
      "loss": 2.2043,
      "step": 19790
    },
    {
      "epoch": 45.0,
      "grad_norm": 0.3792143762111664,
      "learning_rate": 0.0004440366160729392,
      "loss": 2.217,
      "step": 19800
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.1016041040420532,
      "eval_runtime": 8.7126,
      "eval_samples_per_second": 3492.771,
      "eval_steps_per_second": 13.658,
      "step": 19800
    },
    {
      "epoch": 45.02272727272727,
      "grad_norm": 0.25218695402145386,
      "learning_rate": 0.00044397975831289495,
      "loss": 2.2033,
      "step": 19810
    },
    {
      "epoch": 45.04545454545455,
      "grad_norm": 0.18220603466033936,
      "learning_rate": 0.0004439228753284752,
      "loss": 2.1973,
      "step": 19820
    },
    {
      "epoch": 45.06818181818182,
      "grad_norm": 0.25489047169685364,
      "learning_rate": 0.00044386596712707693,
      "loss": 2.2143,
      "step": 19830
    },
    {
      "epoch": 45.09090909090909,
      "grad_norm": 0.22698792815208435,
      "learning_rate": 0.00044380903371610026,
      "loss": 2.2108,
      "step": 19840
    },
    {
      "epoch": 45.11363636363637,
      "grad_norm": 0.29606497287750244,
      "learning_rate": 0.0004437520751029485,
      "loss": 2.2103,
      "step": 19850
    },
    {
      "epoch": 45.13636363636363,
      "grad_norm": 0.2889847755432129,
      "learning_rate": 0.0004436950912950284,
      "loss": 2.2174,
      "step": 19860
    },
    {
      "epoch": 45.15909090909091,
      "grad_norm": 1.0646328926086426,
      "learning_rate": 0.00044363808229974997,
      "loss": 2.2118,
      "step": 19870
    },
    {
      "epoch": 45.18181818181818,
      "grad_norm": 0.2764391303062439,
      "learning_rate": 0.00044358104812452625,
      "loss": 2.2036,
      "step": 19880
    },
    {
      "epoch": 45.20454545454545,
      "grad_norm": 0.280060350894928,
      "learning_rate": 0.000443523988776774,
      "loss": 2.1992,
      "step": 19890
    },
    {
      "epoch": 45.22727272727273,
      "grad_norm": 0.19454485177993774,
      "learning_rate": 0.0004434669042639127,
      "loss": 2.204,
      "step": 19900
    },
    {
      "epoch": 45.25,
      "grad_norm": 0.5434748530387878,
      "learning_rate": 0.00044340979459336574,
      "loss": 2.2068,
      "step": 19910
    },
    {
      "epoch": 45.27272727272727,
      "grad_norm": 0.2500821650028229,
      "learning_rate": 0.0004433526597725591,
      "loss": 2.2056,
      "step": 19920
    },
    {
      "epoch": 45.29545454545455,
      "grad_norm": 0.18215537071228027,
      "learning_rate": 0.00044329549980892257,
      "loss": 2.2141,
      "step": 19930
    },
    {
      "epoch": 45.31818181818182,
      "grad_norm": 0.17405226826667786,
      "learning_rate": 0.00044323831470988894,
      "loss": 2.2118,
      "step": 19940
    },
    {
      "epoch": 45.34090909090909,
      "grad_norm": 0.19856393337249756,
      "learning_rate": 0.00044318110448289446,
      "loss": 2.2061,
      "step": 19950
    },
    {
      "epoch": 45.36363636363637,
      "grad_norm": 0.21082857251167297,
      "learning_rate": 0.0004431238691353784,
      "loss": 2.2093,
      "step": 19960
    },
    {
      "epoch": 45.38636363636363,
      "grad_norm": 0.22312994301319122,
      "learning_rate": 0.00044306660867478336,
      "loss": 2.1999,
      "step": 19970
    },
    {
      "epoch": 45.40909090909091,
      "grad_norm": 0.45381760597229004,
      "learning_rate": 0.00044300932310855544,
      "loss": 2.2183,
      "step": 19980
    },
    {
      "epoch": 45.43181818181818,
      "grad_norm": 0.2347128689289093,
      "learning_rate": 0.00044295201244414373,
      "loss": 2.2005,
      "step": 19990
    },
    {
      "epoch": 45.45454545454545,
      "grad_norm": 0.6380624771118164,
      "learning_rate": 0.0004428946766890007,
      "loss": 2.2178,
      "step": 20000
    },
    {
      "epoch": 45.47727272727273,
      "grad_norm": 0.27812033891677856,
      "learning_rate": 0.00044283731585058214,
      "loss": 2.2005,
      "step": 20010
    },
    {
      "epoch": 45.5,
      "grad_norm": 0.3363342583179474,
      "learning_rate": 0.0004427799299363469,
      "loss": 2.2148,
      "step": 20020
    },
    {
      "epoch": 45.52272727272727,
      "grad_norm": 0.715000331401825,
      "learning_rate": 0.00044272251895375735,
      "loss": 2.2098,
      "step": 20030
    },
    {
      "epoch": 45.54545454545455,
      "grad_norm": 0.526824951171875,
      "learning_rate": 0.00044266508291027887,
      "loss": 2.2137,
      "step": 20040
    },
    {
      "epoch": 45.56818181818182,
      "grad_norm": 0.16211441159248352,
      "learning_rate": 0.0004426076218133803,
      "loss": 2.2092,
      "step": 20050
    },
    {
      "epoch": 45.59090909090909,
      "grad_norm": 0.24814052879810333,
      "learning_rate": 0.0004425501356705337,
      "loss": 2.2215,
      "step": 20060
    },
    {
      "epoch": 45.61363636363637,
      "grad_norm": 0.21844568848609924,
      "learning_rate": 0.00044249262448921425,
      "loss": 2.2219,
      "step": 20070
    },
    {
      "epoch": 45.63636363636363,
      "grad_norm": 0.3458048105239868,
      "learning_rate": 0.00044243508827690047,
      "loss": 2.1982,
      "step": 20080
    },
    {
      "epoch": 45.65909090909091,
      "grad_norm": 0.19575050473213196,
      "learning_rate": 0.00044237752704107423,
      "loss": 2.2027,
      "step": 20090
    },
    {
      "epoch": 45.68181818181818,
      "grad_norm": 0.2095349133014679,
      "learning_rate": 0.0004423199407892206,
      "loss": 2.2111,
      "step": 20100
    },
    {
      "epoch": 45.70454545454545,
      "grad_norm": 0.2498229593038559,
      "learning_rate": 0.00044226232952882774,
      "loss": 2.2129,
      "step": 20110
    },
    {
      "epoch": 45.72727272727273,
      "grad_norm": 0.6936330199241638,
      "learning_rate": 0.00044220469326738726,
      "loss": 2.2096,
      "step": 20120
    },
    {
      "epoch": 45.75,
      "grad_norm": 0.22561007738113403,
      "learning_rate": 0.000442147032012394,
      "loss": 2.2027,
      "step": 20130
    },
    {
      "epoch": 45.77272727272727,
      "grad_norm": 0.5145153999328613,
      "learning_rate": 0.0004420893457713459,
      "loss": 2.2083,
      "step": 20140
    },
    {
      "epoch": 45.79545454545455,
      "grad_norm": 0.6778102517127991,
      "learning_rate": 0.0004420316345517444,
      "loss": 2.2067,
      "step": 20150
    },
    {
      "epoch": 45.81818181818182,
      "grad_norm": 0.26910102367401123,
      "learning_rate": 0.0004419738983610939,
      "loss": 2.2102,
      "step": 20160
    },
    {
      "epoch": 45.84090909090909,
      "grad_norm": 0.37477973103523254,
      "learning_rate": 0.0004419161372069023,
      "loss": 2.211,
      "step": 20170
    },
    {
      "epoch": 45.86363636363637,
      "grad_norm": 0.8538024425506592,
      "learning_rate": 0.0004418583510966805,
      "loss": 2.2166,
      "step": 20180
    },
    {
      "epoch": 45.88636363636363,
      "grad_norm": 0.2587391138076782,
      "learning_rate": 0.00044180054003794297,
      "loss": 2.2208,
      "step": 20190
    },
    {
      "epoch": 45.90909090909091,
      "grad_norm": 0.21136900782585144,
      "learning_rate": 0.00044174270403820703,
      "loss": 2.2016,
      "step": 20200
    },
    {
      "epoch": 45.93181818181818,
      "grad_norm": 0.3685615360736847,
      "learning_rate": 0.00044168484310499366,
      "loss": 2.2093,
      "step": 20210
    },
    {
      "epoch": 45.95454545454545,
      "grad_norm": 0.3557295501232147,
      "learning_rate": 0.0004416269572458267,
      "loss": 2.2044,
      "step": 20220
    },
    {
      "epoch": 45.97727272727273,
      "grad_norm": 0.16247648000717163,
      "learning_rate": 0.0004415690464682335,
      "loss": 2.2156,
      "step": 20230
    },
    {
      "epoch": 46.0,
      "grad_norm": 0.31126683950424194,
      "learning_rate": 0.0004415111107797445,
      "loss": 2.2076,
      "step": 20240
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.1011673212051392,
      "eval_runtime": 8.6687,
      "eval_samples_per_second": 3510.435,
      "eval_steps_per_second": 13.728,
      "step": 20240
    },
    {
      "epoch": 46.02272727272727,
      "grad_norm": 0.8882119655609131,
      "learning_rate": 0.0004414531501878935,
      "loss": 2.2105,
      "step": 20250
    },
    {
      "epoch": 46.04545454545455,
      "grad_norm": 18.80290985107422,
      "learning_rate": 0.00044139516470021736,
      "loss": 2.1998,
      "step": 20260
    },
    {
      "epoch": 46.06818181818182,
      "grad_norm": 0.22406208515167236,
      "learning_rate": 0.00044133715432425637,
      "loss": 2.2054,
      "step": 20270
    },
    {
      "epoch": 46.09090909090909,
      "grad_norm": 0.5553715229034424,
      "learning_rate": 0.00044127911906755404,
      "loss": 2.2073,
      "step": 20280
    },
    {
      "epoch": 46.11363636363637,
      "grad_norm": 0.29715776443481445,
      "learning_rate": 0.00044122105893765683,
      "loss": 2.2042,
      "step": 20290
    },
    {
      "epoch": 46.13636363636363,
      "grad_norm": 0.23703275620937347,
      "learning_rate": 0.00044116297394211493,
      "loss": 2.2068,
      "step": 20300
    },
    {
      "epoch": 46.15909090909091,
      "grad_norm": 0.4308179020881653,
      "learning_rate": 0.00044110486408848127,
      "loss": 2.2082,
      "step": 20310
    },
    {
      "epoch": 46.18181818181818,
      "grad_norm": 0.7627667784690857,
      "learning_rate": 0.0004410467293843123,
      "loss": 2.2029,
      "step": 20320
    },
    {
      "epoch": 46.20454545454545,
      "grad_norm": 0.20803995430469513,
      "learning_rate": 0.00044098856983716765,
      "loss": 2.2085,
      "step": 20330
    },
    {
      "epoch": 46.22727272727273,
      "grad_norm": 0.256933331489563,
      "learning_rate": 0.00044093038545461015,
      "loss": 2.2019,
      "step": 20340
    },
    {
      "epoch": 46.25,
      "grad_norm": 0.30619263648986816,
      "learning_rate": 0.0004408721762442059,
      "loss": 2.2051,
      "step": 20350
    },
    {
      "epoch": 46.27272727272727,
      "grad_norm": 0.928346574306488,
      "learning_rate": 0.00044081394221352413,
      "loss": 2.2176,
      "step": 20360
    },
    {
      "epoch": 46.29545454545455,
      "grad_norm": 0.33657264709472656,
      "learning_rate": 0.00044075568337013747,
      "loss": 2.2054,
      "step": 20370
    },
    {
      "epoch": 46.31818181818182,
      "grad_norm": 0.25772011280059814,
      "learning_rate": 0.0004406973997216216,
      "loss": 2.199,
      "step": 20380
    },
    {
      "epoch": 46.34090909090909,
      "grad_norm": 0.30911654233932495,
      "learning_rate": 0.00044063909127555547,
      "loss": 2.1943,
      "step": 20390
    },
    {
      "epoch": 46.36363636363637,
      "grad_norm": 0.3163098394870758,
      "learning_rate": 0.00044058075803952133,
      "loss": 2.2015,
      "step": 20400
    },
    {
      "epoch": 46.38636363636363,
      "grad_norm": 0.21406109631061554,
      "learning_rate": 0.00044052240002110467,
      "loss": 2.2079,
      "step": 20410
    },
    {
      "epoch": 46.40909090909091,
      "grad_norm": 0.27265408635139465,
      "learning_rate": 0.000440464017227894,
      "loss": 2.2158,
      "step": 20420
    },
    {
      "epoch": 46.43181818181818,
      "grad_norm": 0.3365088105201721,
      "learning_rate": 0.0004404056096674813,
      "loss": 2.2185,
      "step": 20430
    },
    {
      "epoch": 46.45454545454545,
      "grad_norm": 0.15374645590782166,
      "learning_rate": 0.00044034717734746164,
      "loss": 2.2073,
      "step": 20440
    },
    {
      "epoch": 46.47727272727273,
      "grad_norm": 1.1009081602096558,
      "learning_rate": 0.0004402887202754333,
      "loss": 2.2024,
      "step": 20450
    },
    {
      "epoch": 46.5,
      "grad_norm": 0.1928834468126297,
      "learning_rate": 0.0004402302384589979,
      "loss": 2.2072,
      "step": 20460
    },
    {
      "epoch": 46.52272727272727,
      "grad_norm": 0.2565022110939026,
      "learning_rate": 0.00044017173190576006,
      "loss": 2.2098,
      "step": 20470
    },
    {
      "epoch": 46.54545454545455,
      "grad_norm": 0.2931063175201416,
      "learning_rate": 0.0004401132006233278,
      "loss": 2.2079,
      "step": 20480
    },
    {
      "epoch": 46.56818181818182,
      "grad_norm": 0.3974829316139221,
      "learning_rate": 0.00044005464461931235,
      "loss": 2.1959,
      "step": 20490
    },
    {
      "epoch": 46.59090909090909,
      "grad_norm": 0.5619593262672424,
      "learning_rate": 0.00043999606390132803,
      "loss": 2.2115,
      "step": 20500
    },
    {
      "epoch": 46.61363636363637,
      "grad_norm": 0.2680591940879822,
      "learning_rate": 0.00043993745847699254,
      "loss": 2.2058,
      "step": 20510
    },
    {
      "epoch": 46.63636363636363,
      "grad_norm": 0.12207691371440887,
      "learning_rate": 0.00043987882835392657,
      "loss": 2.2051,
      "step": 20520
    },
    {
      "epoch": 46.65909090909091,
      "grad_norm": 0.1898522973060608,
      "learning_rate": 0.00043982017353975424,
      "loss": 2.2182,
      "step": 20530
    },
    {
      "epoch": 46.68181818181818,
      "grad_norm": 0.1766769289970398,
      "learning_rate": 0.00043976149404210285,
      "loss": 2.2052,
      "step": 20540
    },
    {
      "epoch": 46.70454545454545,
      "grad_norm": 0.16405782103538513,
      "learning_rate": 0.0004397027898686027,
      "loss": 2.2049,
      "step": 20550
    },
    {
      "epoch": 46.72727272727273,
      "grad_norm": 0.34908151626586914,
      "learning_rate": 0.0004396440610268876,
      "loss": 2.2161,
      "step": 20560
    },
    {
      "epoch": 46.75,
      "grad_norm": 0.2527220547199249,
      "learning_rate": 0.00043958530752459437,
      "loss": 2.2019,
      "step": 20570
    },
    {
      "epoch": 46.77272727272727,
      "grad_norm": 0.36631521582603455,
      "learning_rate": 0.00043952652936936305,
      "loss": 2.2039,
      "step": 20580
    },
    {
      "epoch": 46.79545454545455,
      "grad_norm": 0.206295907497406,
      "learning_rate": 0.00043946772656883697,
      "loss": 2.2098,
      "step": 20590
    },
    {
      "epoch": 46.81818181818182,
      "grad_norm": 0.19586947560310364,
      "learning_rate": 0.00043940889913066263,
      "loss": 2.2197,
      "step": 20600
    },
    {
      "epoch": 46.84090909090909,
      "grad_norm": 0.21818625926971436,
      "learning_rate": 0.0004393500470624896,
      "loss": 2.2001,
      "step": 20610
    },
    {
      "epoch": 46.86363636363637,
      "grad_norm": 0.2768985629081726,
      "learning_rate": 0.00043929117037197095,
      "loss": 2.2136,
      "step": 20620
    },
    {
      "epoch": 46.88636363636363,
      "grad_norm": 0.2279297560453415,
      "learning_rate": 0.0004392322690667627,
      "loss": 2.213,
      "step": 20630
    },
    {
      "epoch": 46.90909090909091,
      "grad_norm": 0.19360801577568054,
      "learning_rate": 0.00043917334315452406,
      "loss": 2.2057,
      "step": 20640
    },
    {
      "epoch": 46.93181818181818,
      "grad_norm": 0.254433810710907,
      "learning_rate": 0.0004391143926429177,
      "loss": 2.2087,
      "step": 20650
    },
    {
      "epoch": 46.95454545454545,
      "grad_norm": 0.27672234177589417,
      "learning_rate": 0.0004390554175396092,
      "loss": 2.216,
      "step": 20660
    },
    {
      "epoch": 46.97727272727273,
      "grad_norm": 0.20548096299171448,
      "learning_rate": 0.00043899641785226747,
      "loss": 2.2123,
      "step": 20670
    },
    {
      "epoch": 47.0,
      "grad_norm": 0.48624753952026367,
      "learning_rate": 0.0004389373935885646,
      "loss": 2.2214,
      "step": 20680
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.1011165380477905,
      "eval_runtime": 8.942,
      "eval_samples_per_second": 3403.159,
      "eval_steps_per_second": 13.308,
      "step": 20680
    },
    {
      "epoch": 47.02272727272727,
      "grad_norm": 0.19295737147331238,
      "learning_rate": 0.0004388783447561759,
      "loss": 2.2048,
      "step": 20690
    },
    {
      "epoch": 47.04545454545455,
      "grad_norm": 0.32851529121398926,
      "learning_rate": 0.00043881927136277975,
      "loss": 2.217,
      "step": 20700
    },
    {
      "epoch": 47.06818181818182,
      "grad_norm": 0.3019139766693115,
      "learning_rate": 0.000438760173416058,
      "loss": 2.2162,
      "step": 20710
    },
    {
      "epoch": 47.09090909090909,
      "grad_norm": 0.21834953129291534,
      "learning_rate": 0.0004387010509236955,
      "loss": 2.2034,
      "step": 20720
    },
    {
      "epoch": 47.11363636363637,
      "grad_norm": 0.28633442521095276,
      "learning_rate": 0.0004386419038933801,
      "loss": 2.2015,
      "step": 20730
    },
    {
      "epoch": 47.13636363636363,
      "grad_norm": 0.17766503989696503,
      "learning_rate": 0.00043858273233280326,
      "loss": 2.2084,
      "step": 20740
    },
    {
      "epoch": 47.15909090909091,
      "grad_norm": 0.31368714570999146,
      "learning_rate": 0.00043852353624965933,
      "loss": 2.1961,
      "step": 20750
    },
    {
      "epoch": 47.18181818181818,
      "grad_norm": 0.22229768335819244,
      "learning_rate": 0.00043846431565164594,
      "loss": 2.2195,
      "step": 20760
    },
    {
      "epoch": 47.20454545454545,
      "grad_norm": 0.5907983183860779,
      "learning_rate": 0.00043840507054646396,
      "loss": 2.2079,
      "step": 20770
    },
    {
      "epoch": 47.22727272727273,
      "grad_norm": 0.18523496389389038,
      "learning_rate": 0.00043834580094181735,
      "loss": 2.2012,
      "step": 20780
    },
    {
      "epoch": 47.25,
      "grad_norm": 0.21314363181591034,
      "learning_rate": 0.0004382865068454133,
      "loss": 2.2112,
      "step": 20790
    },
    {
      "epoch": 47.27272727272727,
      "grad_norm": 0.3676110506057739,
      "learning_rate": 0.00043822718826496223,
      "loss": 2.2119,
      "step": 20800
    },
    {
      "epoch": 47.29545454545455,
      "grad_norm": 0.9690036773681641,
      "learning_rate": 0.00043816784520817764,
      "loss": 2.1993,
      "step": 20810
    },
    {
      "epoch": 47.31818181818182,
      "grad_norm": 0.20641689002513885,
      "learning_rate": 0.0004381084776827763,
      "loss": 2.1959,
      "step": 20820
    },
    {
      "epoch": 47.34090909090909,
      "grad_norm": 0.20734164118766785,
      "learning_rate": 0.00043804908569647805,
      "loss": 2.2134,
      "step": 20830
    },
    {
      "epoch": 47.36363636363637,
      "grad_norm": 0.17016588151454926,
      "learning_rate": 0.0004379896692570061,
      "loss": 2.2048,
      "step": 20840
    },
    {
      "epoch": 47.38636363636363,
      "grad_norm": 0.2291494905948639,
      "learning_rate": 0.00043793022837208676,
      "loss": 2.2033,
      "step": 20850
    },
    {
      "epoch": 47.40909090909091,
      "grad_norm": 3.001603126525879,
      "learning_rate": 0.0004378707630494494,
      "loss": 2.211,
      "step": 20860
    },
    {
      "epoch": 47.43181818181818,
      "grad_norm": 0.24270065128803253,
      "learning_rate": 0.0004378112732968267,
      "loss": 2.2037,
      "step": 20870
    },
    {
      "epoch": 47.45454545454545,
      "grad_norm": 0.1921696662902832,
      "learning_rate": 0.00043775175912195443,
      "loss": 2.2045,
      "step": 20880
    },
    {
      "epoch": 47.47727272727273,
      "grad_norm": 0.5148451924324036,
      "learning_rate": 0.0004376922205325717,
      "loss": 2.2079,
      "step": 20890
    },
    {
      "epoch": 47.5,
      "grad_norm": 0.5301738381385803,
      "learning_rate": 0.0004376326575364206,
      "loss": 2.2016,
      "step": 20900
    },
    {
      "epoch": 47.52272727272727,
      "grad_norm": 0.3832264542579651,
      "learning_rate": 0.00043757307014124635,
      "loss": 2.2069,
      "step": 20910
    },
    {
      "epoch": 47.54545454545455,
      "grad_norm": 0.6011533737182617,
      "learning_rate": 0.0004375134583547977,
      "loss": 2.2037,
      "step": 20920
    },
    {
      "epoch": 47.56818181818182,
      "grad_norm": 0.19569526612758636,
      "learning_rate": 0.00043745382218482616,
      "loss": 2.2098,
      "step": 20930
    },
    {
      "epoch": 47.59090909090909,
      "grad_norm": 0.5403813719749451,
      "learning_rate": 0.00043739416163908663,
      "loss": 2.1995,
      "step": 20940
    },
    {
      "epoch": 47.61363636363637,
      "grad_norm": 0.272504597902298,
      "learning_rate": 0.0004373344767253372,
      "loss": 2.1999,
      "step": 20950
    },
    {
      "epoch": 47.63636363636363,
      "grad_norm": 0.28765150904655457,
      "learning_rate": 0.00043727476745133905,
      "loss": 2.205,
      "step": 20960
    },
    {
      "epoch": 47.65909090909091,
      "grad_norm": 0.8944193720817566,
      "learning_rate": 0.00043721503382485646,
      "loss": 2.2217,
      "step": 20970
    },
    {
      "epoch": 47.68181818181818,
      "grad_norm": 0.20784194767475128,
      "learning_rate": 0.00043715527585365697,
      "loss": 2.2102,
      "step": 20980
    },
    {
      "epoch": 47.70454545454545,
      "grad_norm": 0.19652943313121796,
      "learning_rate": 0.00043709549354551135,
      "loss": 2.2096,
      "step": 20990
    },
    {
      "epoch": 47.72727272727273,
      "grad_norm": 0.2584978938102722,
      "learning_rate": 0.00043703568690819347,
      "loss": 2.2108,
      "step": 21000
    },
    {
      "epoch": 47.75,
      "grad_norm": 0.4689248502254486,
      "learning_rate": 0.0004369758559494803,
      "loss": 2.206,
      "step": 21010
    },
    {
      "epoch": 47.77272727272727,
      "grad_norm": 0.2924670875072479,
      "learning_rate": 0.000436916000677152,
      "loss": 2.2137,
      "step": 21020
    },
    {
      "epoch": 47.79545454545455,
      "grad_norm": 0.2012554109096527,
      "learning_rate": 0.00043685612109899184,
      "loss": 2.2114,
      "step": 21030
    },
    {
      "epoch": 47.81818181818182,
      "grad_norm": 0.597834587097168,
      "learning_rate": 0.0004367962172227866,
      "loss": 2.1979,
      "step": 21040
    },
    {
      "epoch": 47.84090909090909,
      "grad_norm": 0.13406029343605042,
      "learning_rate": 0.0004367362890563257,
      "loss": 2.2031,
      "step": 21050
    },
    {
      "epoch": 47.86363636363637,
      "grad_norm": 0.8393791317939758,
      "learning_rate": 0.00043667633660740215,
      "loss": 2.209,
      "step": 21060
    },
    {
      "epoch": 47.88636363636363,
      "grad_norm": 0.29257023334503174,
      "learning_rate": 0.00043661635988381174,
      "loss": 2.2275,
      "step": 21070
    },
    {
      "epoch": 47.90909090909091,
      "grad_norm": 0.1731904298067093,
      "learning_rate": 0.0004365563588933538,
      "loss": 2.2141,
      "step": 21080
    },
    {
      "epoch": 47.93181818181818,
      "grad_norm": 0.1950686275959015,
      "learning_rate": 0.0004364963336438304,
      "loss": 2.202,
      "step": 21090
    },
    {
      "epoch": 47.95454545454545,
      "grad_norm": 0.20525985956192017,
      "learning_rate": 0.00043643628414304715,
      "loss": 2.2111,
      "step": 21100
    },
    {
      "epoch": 47.97727272727273,
      "grad_norm": 0.5004721283912659,
      "learning_rate": 0.0004363762103988127,
      "loss": 2.2098,
      "step": 21110
    },
    {
      "epoch": 48.0,
      "grad_norm": 0.25562572479248047,
      "learning_rate": 0.0004363161124189387,
      "loss": 2.2189,
      "step": 21120
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.100982666015625,
      "eval_runtime": 8.7219,
      "eval_samples_per_second": 3489.041,
      "eval_steps_per_second": 13.644,
      "step": 21120
    },
    {
      "epoch": 48.02272727272727,
      "grad_norm": 0.2100779116153717,
      "learning_rate": 0.0004362559902112401,
      "loss": 2.2049,
      "step": 21130
    },
    {
      "epoch": 48.04545454545455,
      "grad_norm": 0.1780281960964203,
      "learning_rate": 0.00043619584378353497,
      "loss": 2.2138,
      "step": 21140
    },
    {
      "epoch": 48.06818181818182,
      "grad_norm": 0.260951966047287,
      "learning_rate": 0.00043613567314364443,
      "loss": 2.2002,
      "step": 21150
    },
    {
      "epoch": 48.09090909090909,
      "grad_norm": 0.4486209750175476,
      "learning_rate": 0.00043607547829939295,
      "loss": 2.2133,
      "step": 21160
    },
    {
      "epoch": 48.11363636363637,
      "grad_norm": 0.40573975443840027,
      "learning_rate": 0.00043601525925860796,
      "loss": 2.2132,
      "step": 21170
    },
    {
      "epoch": 48.13636363636363,
      "grad_norm": 0.29803550243377686,
      "learning_rate": 0.0004359550160291201,
      "loss": 2.1991,
      "step": 21180
    },
    {
      "epoch": 48.15909090909091,
      "grad_norm": 0.512021005153656,
      "learning_rate": 0.0004358947486187633,
      "loss": 2.202,
      "step": 21190
    },
    {
      "epoch": 48.18181818181818,
      "grad_norm": 0.7758261561393738,
      "learning_rate": 0.0004358344570353744,
      "loss": 2.2056,
      "step": 21200
    },
    {
      "epoch": 48.20454545454545,
      "grad_norm": 1.6263375282287598,
      "learning_rate": 0.0004357741412867935,
      "loss": 2.2182,
      "step": 21210
    },
    {
      "epoch": 48.22727272727273,
      "grad_norm": 0.29638683795928955,
      "learning_rate": 0.0004357138013808637,
      "loss": 2.2041,
      "step": 21220
    },
    {
      "epoch": 48.25,
      "grad_norm": 0.23365040123462677,
      "learning_rate": 0.0004356534373254316,
      "loss": 2.2053,
      "step": 21230
    },
    {
      "epoch": 48.27272727272727,
      "grad_norm": 0.18303215503692627,
      "learning_rate": 0.00043559304912834653,
      "loss": 2.2157,
      "step": 21240
    },
    {
      "epoch": 48.29545454545455,
      "grad_norm": 0.24315311014652252,
      "learning_rate": 0.00043553263679746126,
      "loss": 2.2099,
      "step": 21250
    },
    {
      "epoch": 48.31818181818182,
      "grad_norm": 0.2424187809228897,
      "learning_rate": 0.0004354722003406314,
      "loss": 2.2009,
      "step": 21260
    },
    {
      "epoch": 48.34090909090909,
      "grad_norm": 0.40206947922706604,
      "learning_rate": 0.00043541173976571616,
      "loss": 2.213,
      "step": 21270
    },
    {
      "epoch": 48.36363636363637,
      "grad_norm": 0.24793894588947296,
      "learning_rate": 0.00043535125508057727,
      "loss": 2.2111,
      "step": 21280
    },
    {
      "epoch": 48.38636363636363,
      "grad_norm": 0.868593156337738,
      "learning_rate": 0.0004352907462930801,
      "loss": 2.2156,
      "step": 21290
    },
    {
      "epoch": 48.40909090909091,
      "grad_norm": 0.17828874289989471,
      "learning_rate": 0.00043523021341109304,
      "loss": 2.2013,
      "step": 21300
    },
    {
      "epoch": 48.43181818181818,
      "grad_norm": 0.16964353621006012,
      "learning_rate": 0.00043516965644248734,
      "loss": 2.2113,
      "step": 21310
    },
    {
      "epoch": 48.45454545454545,
      "grad_norm": 0.12633436918258667,
      "learning_rate": 0.00043510907539513776,
      "loss": 2.2026,
      "step": 21320
    },
    {
      "epoch": 48.47727272727273,
      "grad_norm": 0.19661584496498108,
      "learning_rate": 0.00043504847027692206,
      "loss": 2.2095,
      "step": 21330
    },
    {
      "epoch": 48.5,
      "grad_norm": 0.36609673500061035,
      "learning_rate": 0.00043498784109572097,
      "loss": 2.2097,
      "step": 21340
    },
    {
      "epoch": 48.52272727272727,
      "grad_norm": 0.1810886263847351,
      "learning_rate": 0.00043492718785941855,
      "loss": 2.2231,
      "step": 21350
    },
    {
      "epoch": 48.54545454545455,
      "grad_norm": 0.20788845419883728,
      "learning_rate": 0.0004348665105759019,
      "loss": 2.2126,
      "step": 21360
    },
    {
      "epoch": 48.56818181818182,
      "grad_norm": 0.2389509379863739,
      "learning_rate": 0.0004348058092530611,
      "loss": 2.2015,
      "step": 21370
    },
    {
      "epoch": 48.59090909090909,
      "grad_norm": 0.11427388340234756,
      "learning_rate": 0.00043474508389878977,
      "loss": 2.1984,
      "step": 21380
    },
    {
      "epoch": 48.61363636363637,
      "grad_norm": 0.31596243381500244,
      "learning_rate": 0.00043468433452098433,
      "loss": 2.2067,
      "step": 21390
    },
    {
      "epoch": 48.63636363636363,
      "grad_norm": 0.40450790524482727,
      "learning_rate": 0.0004346235611275443,
      "loss": 2.2042,
      "step": 21400
    },
    {
      "epoch": 48.65909090909091,
      "grad_norm": 0.22910188138484955,
      "learning_rate": 0.00043456276372637244,
      "loss": 2.1945,
      "step": 21410
    },
    {
      "epoch": 48.68181818181818,
      "grad_norm": 0.19721633195877075,
      "learning_rate": 0.00043450194232537464,
      "loss": 2.2084,
      "step": 21420
    },
    {
      "epoch": 48.70454545454545,
      "grad_norm": 0.6898384690284729,
      "learning_rate": 0.0004344410969324599,
      "loss": 2.2018,
      "step": 21430
    },
    {
      "epoch": 48.72727272727273,
      "grad_norm": 1.541551947593689,
      "learning_rate": 0.0004343802275555403,
      "loss": 2.1998,
      "step": 21440
    },
    {
      "epoch": 48.75,
      "grad_norm": 0.15476185083389282,
      "learning_rate": 0.000434319334202531,
      "loss": 2.2015,
      "step": 21450
    },
    {
      "epoch": 48.77272727272727,
      "grad_norm": 0.5384659767150879,
      "learning_rate": 0.0004342584168813505,
      "loss": 2.2012,
      "step": 21460
    },
    {
      "epoch": 48.79545454545455,
      "grad_norm": 0.1974141150712967,
      "learning_rate": 0.00043419747559991995,
      "loss": 2.2147,
      "step": 21470
    },
    {
      "epoch": 48.81818181818182,
      "grad_norm": 0.16074331104755402,
      "learning_rate": 0.0004341365103661643,
      "loss": 2.2034,
      "step": 21480
    },
    {
      "epoch": 48.84090909090909,
      "grad_norm": 0.24313709139823914,
      "learning_rate": 0.00043407552118801096,
      "loss": 2.2052,
      "step": 21490
    },
    {
      "epoch": 48.86363636363637,
      "grad_norm": 0.1579907387495041,
      "learning_rate": 0.00043401450807339074,
      "loss": 2.2046,
      "step": 21500
    },
    {
      "epoch": 48.88636363636363,
      "grad_norm": 0.14385952055454254,
      "learning_rate": 0.00043395347103023776,
      "loss": 2.2088,
      "step": 21510
    },
    {
      "epoch": 48.90909090909091,
      "grad_norm": 0.373294860124588,
      "learning_rate": 0.0004338924100664887,
      "loss": 2.2106,
      "step": 21520
    },
    {
      "epoch": 48.93181818181818,
      "grad_norm": 0.22196565568447113,
      "learning_rate": 0.00043383132519008404,
      "loss": 2.2158,
      "step": 21530
    },
    {
      "epoch": 48.95454545454545,
      "grad_norm": 1.2504020929336548,
      "learning_rate": 0.00043377021640896685,
      "loss": 2.2108,
      "step": 21540
    },
    {
      "epoch": 48.97727272727273,
      "grad_norm": 0.17453113198280334,
      "learning_rate": 0.00043370908373108343,
      "loss": 2.2206,
      "step": 21550
    },
    {
      "epoch": 49.0,
      "grad_norm": 0.6700819134712219,
      "learning_rate": 0.0004336479271643833,
      "loss": 2.1961,
      "step": 21560
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.1008498668670654,
      "eval_runtime": 8.9448,
      "eval_samples_per_second": 3402.089,
      "eval_steps_per_second": 13.304,
      "step": 21560
    },
    {
      "epoch": 49.02272727272727,
      "grad_norm": 0.1971859484910965,
      "learning_rate": 0.0004335867467168191,
      "loss": 2.2097,
      "step": 21570
    },
    {
      "epoch": 49.04545454545455,
      "grad_norm": 0.160035640001297,
      "learning_rate": 0.00043352554239634635,
      "loss": 2.2078,
      "step": 21580
    },
    {
      "epoch": 49.06818181818182,
      "grad_norm": 0.16681812703609467,
      "learning_rate": 0.00043346431421092393,
      "loss": 2.1873,
      "step": 21590
    },
    {
      "epoch": 49.09090909090909,
      "grad_norm": 0.19916217029094696,
      "learning_rate": 0.0004334030621685137,
      "loss": 2.181,
      "step": 21600
    },
    {
      "epoch": 49.11363636363637,
      "grad_norm": 0.39476388692855835,
      "learning_rate": 0.00043334178627708056,
      "loss": 2.2024,
      "step": 21610
    },
    {
      "epoch": 49.13636363636363,
      "grad_norm": 0.2582623064517975,
      "learning_rate": 0.0004332804865445926,
      "loss": 2.2093,
      "step": 21620
    },
    {
      "epoch": 49.15909090909091,
      "grad_norm": 0.13935904204845428,
      "learning_rate": 0.0004332191629790212,
      "loss": 2.2169,
      "step": 21630
    },
    {
      "epoch": 49.18181818181818,
      "grad_norm": 0.245404914021492,
      "learning_rate": 0.00043315781558834045,
      "loss": 2.2032,
      "step": 21640
    },
    {
      "epoch": 49.20454545454545,
      "grad_norm": 0.12201764434576035,
      "learning_rate": 0.00043309644438052766,
      "loss": 2.216,
      "step": 21650
    },
    {
      "epoch": 49.22727272727273,
      "grad_norm": 0.15679922699928284,
      "learning_rate": 0.0004330350493635635,
      "loss": 2.2023,
      "step": 21660
    },
    {
      "epoch": 49.25,
      "grad_norm": 0.4112890362739563,
      "learning_rate": 0.00043297363054543137,
      "loss": 2.2026,
      "step": 21670
    },
    {
      "epoch": 49.27272727272727,
      "grad_norm": 0.3279380798339844,
      "learning_rate": 0.000432912187934118,
      "loss": 2.211,
      "step": 21680
    },
    {
      "epoch": 49.29545454545455,
      "grad_norm": 0.22140267491340637,
      "learning_rate": 0.00043285072153761327,
      "loss": 2.2092,
      "step": 21690
    },
    {
      "epoch": 49.31818181818182,
      "grad_norm": 0.34095510840415955,
      "learning_rate": 0.00043278923136390985,
      "loss": 2.2144,
      "step": 21700
    },
    {
      "epoch": 49.34090909090909,
      "grad_norm": 0.17014940083026886,
      "learning_rate": 0.00043272771742100373,
      "loss": 2.2057,
      "step": 21710
    },
    {
      "epoch": 49.36363636363637,
      "grad_norm": 0.14056925475597382,
      "learning_rate": 0.00043266617971689404,
      "loss": 2.2162,
      "step": 21720
    },
    {
      "epoch": 49.38636363636363,
      "grad_norm": 0.23876596987247467,
      "learning_rate": 0.00043260461825958275,
      "loss": 2.2053,
      "step": 21730
    },
    {
      "epoch": 49.40909090909091,
      "grad_norm": 0.43850529193878174,
      "learning_rate": 0.0004325430330570752,
      "loss": 2.2024,
      "step": 21740
    },
    {
      "epoch": 49.43181818181818,
      "grad_norm": 0.25948235392570496,
      "learning_rate": 0.0004324814241173797,
      "loss": 2.188,
      "step": 21750
    },
    {
      "epoch": 49.45454545454545,
      "grad_norm": 0.1589275598526001,
      "learning_rate": 0.0004324197914485075,
      "loss": 2.2198,
      "step": 21760
    },
    {
      "epoch": 49.47727272727273,
      "grad_norm": 0.19472511112689972,
      "learning_rate": 0.00043235813505847323,
      "loss": 2.2142,
      "step": 21770
    },
    {
      "epoch": 49.5,
      "grad_norm": 0.18500514328479767,
      "learning_rate": 0.0004322964549552943,
      "loss": 2.206,
      "step": 21780
    },
    {
      "epoch": 49.52272727272727,
      "grad_norm": 0.1568218320608139,
      "learning_rate": 0.0004322347511469915,
      "loss": 2.2038,
      "step": 21790
    },
    {
      "epoch": 49.54545454545455,
      "grad_norm": 0.2934912443161011,
      "learning_rate": 0.0004321730236415884,
      "loss": 2.2034,
      "step": 21800
    },
    {
      "epoch": 49.56818181818182,
      "grad_norm": 0.1233089417219162,
      "learning_rate": 0.0004321112724471119,
      "loss": 2.2097,
      "step": 21810
    },
    {
      "epoch": 49.59090909090909,
      "grad_norm": 0.26283374428749084,
      "learning_rate": 0.00043204949757159194,
      "loss": 2.2158,
      "step": 21820
    },
    {
      "epoch": 49.61363636363637,
      "grad_norm": 0.6656042337417603,
      "learning_rate": 0.0004319876990230614,
      "loss": 2.2125,
      "step": 21830
    },
    {
      "epoch": 49.63636363636363,
      "grad_norm": 0.3204549252986908,
      "learning_rate": 0.0004319258768095563,
      "loss": 2.207,
      "step": 21840
    },
    {
      "epoch": 49.65909090909091,
      "grad_norm": 0.17130933701992035,
      "learning_rate": 0.00043186403093911586,
      "loss": 2.2143,
      "step": 21850
    },
    {
      "epoch": 49.68181818181818,
      "grad_norm": 0.12887775897979736,
      "learning_rate": 0.0004318021614197822,
      "loss": 2.2116,
      "step": 21860
    },
    {
      "epoch": 49.70454545454545,
      "grad_norm": 0.16790498793125153,
      "learning_rate": 0.0004317402682596006,
      "loss": 2.2241,
      "step": 21870
    },
    {
      "epoch": 49.72727272727273,
      "grad_norm": 0.16777952015399933,
      "learning_rate": 0.0004316783514666194,
      "loss": 2.1969,
      "step": 21880
    },
    {
      "epoch": 49.75,
      "grad_norm": 0.18580931425094604,
      "learning_rate": 0.00043161641104889003,
      "loss": 2.2096,
      "step": 21890
    },
    {
      "epoch": 49.77272727272727,
      "grad_norm": 0.19282326102256775,
      "learning_rate": 0.000431554447014467,
      "loss": 2.1932,
      "step": 21900
    },
    {
      "epoch": 49.79545454545455,
      "grad_norm": 0.1970929205417633,
      "learning_rate": 0.0004314924593714079,
      "loss": 2.2046,
      "step": 21910
    },
    {
      "epoch": 49.81818181818182,
      "grad_norm": 0.21407678723335266,
      "learning_rate": 0.00043143044812777334,
      "loss": 2.2023,
      "step": 21920
    },
    {
      "epoch": 49.84090909090909,
      "grad_norm": 0.23990494012832642,
      "learning_rate": 0.0004313684132916269,
      "loss": 2.2033,
      "step": 21930
    },
    {
      "epoch": 49.86363636363637,
      "grad_norm": 0.20584094524383545,
      "learning_rate": 0.00043130635487103556,
      "loss": 2.2109,
      "step": 21940
    },
    {
      "epoch": 49.88636363636363,
      "grad_norm": 0.19200032949447632,
      "learning_rate": 0.000431244272874069,
      "loss": 2.2059,
      "step": 21950
    },
    {
      "epoch": 49.90909090909091,
      "grad_norm": 0.1738175004720688,
      "learning_rate": 0.0004311821673088002,
      "loss": 2.2027,
      "step": 21960
    },
    {
      "epoch": 49.93181818181818,
      "grad_norm": 0.4225209653377533,
      "learning_rate": 0.00043112003818330505,
      "loss": 2.2124,
      "step": 21970
    },
    {
      "epoch": 49.95454545454545,
      "grad_norm": 0.1667013168334961,
      "learning_rate": 0.0004310578855056627,
      "loss": 2.1988,
      "step": 21980
    },
    {
      "epoch": 49.97727272727273,
      "grad_norm": 0.1431966871023178,
      "learning_rate": 0.0004309957092839551,
      "loss": 2.2064,
      "step": 21990
    },
    {
      "epoch": 50.0,
      "grad_norm": 0.8217813372612,
      "learning_rate": 0.0004309335095262675,
      "loss": 2.2079,
      "step": 22000
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.1005910634994507,
      "eval_runtime": 8.6871,
      "eval_samples_per_second": 3503.02,
      "eval_steps_per_second": 13.699,
      "step": 22000
    },
    {
      "epoch": 50.02272727272727,
      "grad_norm": 0.14874395728111267,
      "learning_rate": 0.00043087128624068815,
      "loss": 2.2029,
      "step": 22010
    },
    {
      "epoch": 50.04545454545455,
      "grad_norm": 0.14593316614627838,
      "learning_rate": 0.00043080903943530825,
      "loss": 2.2076,
      "step": 22020
    },
    {
      "epoch": 50.06818181818182,
      "grad_norm": 0.18253736197948456,
      "learning_rate": 0.00043074676911822215,
      "loss": 2.221,
      "step": 22030
    },
    {
      "epoch": 50.09090909090909,
      "grad_norm": 0.3928646445274353,
      "learning_rate": 0.0004306844752975272,
      "loss": 2.2077,
      "step": 22040
    },
    {
      "epoch": 50.11363636363637,
      "grad_norm": 0.2483067363500595,
      "learning_rate": 0.00043062215798132396,
      "loss": 2.2037,
      "step": 22050
    },
    {
      "epoch": 50.13636363636363,
      "grad_norm": 0.745998740196228,
      "learning_rate": 0.00043055981717771586,
      "loss": 2.1999,
      "step": 22060
    },
    {
      "epoch": 50.15909090909091,
      "grad_norm": 0.5978641510009766,
      "learning_rate": 0.0004304974528948094,
      "loss": 2.2014,
      "step": 22070
    },
    {
      "epoch": 50.18181818181818,
      "grad_norm": 0.2974945604801178,
      "learning_rate": 0.0004304350651407143,
      "loss": 2.2157,
      "step": 22080
    },
    {
      "epoch": 50.20454545454545,
      "grad_norm": 0.1668812334537506,
      "learning_rate": 0.0004303726539235431,
      "loss": 2.2045,
      "step": 22090
    },
    {
      "epoch": 50.22727272727273,
      "grad_norm": 0.20660603046417236,
      "learning_rate": 0.0004303102192514116,
      "loss": 2.1971,
      "step": 22100
    },
    {
      "epoch": 50.25,
      "grad_norm": 0.2356123924255371,
      "learning_rate": 0.00043024776113243857,
      "loss": 2.2031,
      "step": 22110
    },
    {
      "epoch": 50.27272727272727,
      "grad_norm": 0.33286118507385254,
      "learning_rate": 0.0004301852795747458,
      "loss": 2.1978,
      "step": 22120
    },
    {
      "epoch": 50.29545454545455,
      "grad_norm": 0.21492595970630646,
      "learning_rate": 0.00043012277458645814,
      "loss": 2.2185,
      "step": 22130
    },
    {
      "epoch": 50.31818181818182,
      "grad_norm": 0.3073095381259918,
      "learning_rate": 0.0004300602461757035,
      "loss": 2.2064,
      "step": 22140
    },
    {
      "epoch": 50.34090909090909,
      "grad_norm": 0.3321683406829834,
      "learning_rate": 0.00042999769435061286,
      "loss": 2.2041,
      "step": 22150
    },
    {
      "epoch": 50.36363636363637,
      "grad_norm": 0.30170220136642456,
      "learning_rate": 0.00042993511911932014,
      "loss": 2.2151,
      "step": 22160
    },
    {
      "epoch": 50.38636363636363,
      "grad_norm": 0.22032347321510315,
      "learning_rate": 0.00042987252048996247,
      "loss": 2.2078,
      "step": 22170
    },
    {
      "epoch": 50.40909090909091,
      "grad_norm": 0.266599178314209,
      "learning_rate": 0.00042980989847067987,
      "loss": 2.2055,
      "step": 22180
    },
    {
      "epoch": 50.43181818181818,
      "grad_norm": 0.269631564617157,
      "learning_rate": 0.00042974725306961547,
      "loss": 2.2051,
      "step": 22190
    },
    {
      "epoch": 50.45454545454545,
      "grad_norm": 0.4080266058444977,
      "learning_rate": 0.0004296845842949155,
      "loss": 2.2106,
      "step": 22200
    },
    {
      "epoch": 50.47727272727273,
      "grad_norm": 0.38617414236068726,
      "learning_rate": 0.00042962189215472915,
      "loss": 2.2089,
      "step": 22210
    },
    {
      "epoch": 50.5,
      "grad_norm": 0.18412883579730988,
      "learning_rate": 0.00042955917665720854,
      "loss": 2.1977,
      "step": 22220
    },
    {
      "epoch": 50.52272727272727,
      "grad_norm": 0.17689716815948486,
      "learning_rate": 0.00042949643781050906,
      "loss": 2.2065,
      "step": 22230
    },
    {
      "epoch": 50.54545454545455,
      "grad_norm": 0.22680073976516724,
      "learning_rate": 0.000429433675622789,
      "loss": 2.2057,
      "step": 22240
    },
    {
      "epoch": 50.56818181818182,
      "grad_norm": 0.15095344185829163,
      "learning_rate": 0.00042937089010220976,
      "loss": 2.1996,
      "step": 22250
    },
    {
      "epoch": 50.59090909090909,
      "grad_norm": 0.3947502076625824,
      "learning_rate": 0.00042930808125693563,
      "loss": 2.2029,
      "step": 22260
    },
    {
      "epoch": 50.61363636363637,
      "grad_norm": 2.891984701156616,
      "learning_rate": 0.0004292452490951342,
      "loss": 2.2262,
      "step": 22270
    },
    {
      "epoch": 50.63636363636363,
      "grad_norm": 0.2504919171333313,
      "learning_rate": 0.00042918239362497567,
      "loss": 2.2045,
      "step": 22280
    },
    {
      "epoch": 50.65909090909091,
      "grad_norm": 0.15615122020244598,
      "learning_rate": 0.00042911951485463375,
      "loss": 2.2138,
      "step": 22290
    },
    {
      "epoch": 50.68181818181818,
      "grad_norm": 0.18318817019462585,
      "learning_rate": 0.00042905661279228483,
      "loss": 2.2033,
      "step": 22300
    },
    {
      "epoch": 50.70454545454545,
      "grad_norm": 0.2906145453453064,
      "learning_rate": 0.00042899368744610847,
      "loss": 2.2065,
      "step": 22310
    },
    {
      "epoch": 50.72727272727273,
      "grad_norm": 0.17954526841640472,
      "learning_rate": 0.0004289307388242872,
      "loss": 2.196,
      "step": 22320
    },
    {
      "epoch": 50.75,
      "grad_norm": 0.27287158370018005,
      "learning_rate": 0.00042886776693500664,
      "loss": 2.2002,
      "step": 22330
    },
    {
      "epoch": 50.77272727272727,
      "grad_norm": 0.30887508392333984,
      "learning_rate": 0.0004288047717864555,
      "loss": 2.2148,
      "step": 22340
    },
    {
      "epoch": 50.79545454545455,
      "grad_norm": 0.15315915644168854,
      "learning_rate": 0.00042874175338682537,
      "loss": 2.2104,
      "step": 22350
    },
    {
      "epoch": 50.81818181818182,
      "grad_norm": 0.18974663317203522,
      "learning_rate": 0.00042867871174431083,
      "loss": 2.2133,
      "step": 22360
    },
    {
      "epoch": 50.84090909090909,
      "grad_norm": 0.6306270956993103,
      "learning_rate": 0.0004286156468671097,
      "loss": 2.2097,
      "step": 22370
    },
    {
      "epoch": 50.86363636363637,
      "grad_norm": 0.1294528841972351,
      "learning_rate": 0.0004285525587634226,
      "loss": 2.1966,
      "step": 22380
    },
    {
      "epoch": 50.88636363636363,
      "grad_norm": 0.11983069777488708,
      "learning_rate": 0.0004284894474414533,
      "loss": 2.2112,
      "step": 22390
    },
    {
      "epoch": 50.90909090909091,
      "grad_norm": 0.15244916081428528,
      "learning_rate": 0.0004284263129094086,
      "loss": 2.1955,
      "step": 22400
    },
    {
      "epoch": 50.93181818181818,
      "grad_norm": 0.46897363662719727,
      "learning_rate": 0.0004283631551754982,
      "loss": 2.2117,
      "step": 22410
    },
    {
      "epoch": 50.95454545454545,
      "grad_norm": 0.13895171880722046,
      "learning_rate": 0.0004282999742479348,
      "loss": 2.197,
      "step": 22420
    },
    {
      "epoch": 50.97727272727273,
      "grad_norm": 0.22664161026477814,
      "learning_rate": 0.0004282367701349345,
      "loss": 2.2095,
      "step": 22430
    },
    {
      "epoch": 51.0,
      "grad_norm": 1.555501937866211,
      "learning_rate": 0.00042817354284471575,
      "loss": 2.2069,
      "step": 22440
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.1007341146469116,
      "eval_runtime": 8.9144,
      "eval_samples_per_second": 3413.697,
      "eval_steps_per_second": 13.349,
      "step": 22440
    },
    {
      "epoch": 51.02272727272727,
      "grad_norm": 0.12626180052757263,
      "learning_rate": 0.00042811029238550057,
      "loss": 2.2026,
      "step": 22450
    },
    {
      "epoch": 51.04545454545455,
      "grad_norm": 0.1760711818933487,
      "learning_rate": 0.00042804701876551386,
      "loss": 2.2096,
      "step": 22460
    },
    {
      "epoch": 51.06818181818182,
      "grad_norm": 0.29980403184890747,
      "learning_rate": 0.0004279837219929834,
      "loss": 2.1837,
      "step": 22470
    },
    {
      "epoch": 51.09090909090909,
      "grad_norm": 0.16471463441848755,
      "learning_rate": 0.00042792040207614005,
      "loss": 2.2163,
      "step": 22480
    },
    {
      "epoch": 51.11363636363637,
      "grad_norm": 0.7984183430671692,
      "learning_rate": 0.0004278570590232177,
      "loss": 2.21,
      "step": 22490
    },
    {
      "epoch": 51.13636363636363,
      "grad_norm": 0.2132900357246399,
      "learning_rate": 0.00042779369284245327,
      "loss": 2.197,
      "step": 22500
    },
    {
      "epoch": 51.15909090909091,
      "grad_norm": 0.46687671542167664,
      "learning_rate": 0.00042773030354208657,
      "loss": 2.2042,
      "step": 22510
    },
    {
      "epoch": 51.18181818181818,
      "grad_norm": 0.2111956626176834,
      "learning_rate": 0.0004276668911303606,
      "loss": 2.2016,
      "step": 22520
    },
    {
      "epoch": 51.20454545454545,
      "grad_norm": 0.18380919098854065,
      "learning_rate": 0.00042760345561552115,
      "loss": 2.2033,
      "step": 22530
    },
    {
      "epoch": 51.22727272727273,
      "grad_norm": 0.49327418208122253,
      "learning_rate": 0.00042753999700581726,
      "loss": 2.2046,
      "step": 22540
    },
    {
      "epoch": 51.25,
      "grad_norm": 0.265631765127182,
      "learning_rate": 0.00042747651530950073,
      "loss": 2.2085,
      "step": 22550
    },
    {
      "epoch": 51.27272727272727,
      "grad_norm": 0.23580457270145416,
      "learning_rate": 0.0004274130105348265,
      "loss": 2.2065,
      "step": 22560
    },
    {
      "epoch": 51.29545454545455,
      "grad_norm": 0.23400650918483734,
      "learning_rate": 0.00042734948269005247,
      "loss": 2.1952,
      "step": 22570
    },
    {
      "epoch": 51.31818181818182,
      "grad_norm": 0.1836869716644287,
      "learning_rate": 0.0004272859317834396,
      "loss": 2.2129,
      "step": 22580
    },
    {
      "epoch": 51.34090909090909,
      "grad_norm": 0.3559061288833618,
      "learning_rate": 0.0004272223578232519,
      "loss": 2.2129,
      "step": 22590
    },
    {
      "epoch": 51.36363636363637,
      "grad_norm": 0.13756956160068512,
      "learning_rate": 0.0004271587608177561,
      "loss": 2.2169,
      "step": 22600
    },
    {
      "epoch": 51.38636363636363,
      "grad_norm": 0.38863128423690796,
      "learning_rate": 0.00042709514077522205,
      "loss": 2.2063,
      "step": 22610
    },
    {
      "epoch": 51.40909090909091,
      "grad_norm": 0.22950291633605957,
      "learning_rate": 0.0004270314977039229,
      "loss": 2.2031,
      "step": 22620
    },
    {
      "epoch": 51.43181818181818,
      "grad_norm": 0.19533386826515198,
      "learning_rate": 0.00042696783161213435,
      "loss": 2.2086,
      "step": 22630
    },
    {
      "epoch": 51.45454545454545,
      "grad_norm": 0.21911458671092987,
      "learning_rate": 0.0004269041425081355,
      "loss": 2.2129,
      "step": 22640
    },
    {
      "epoch": 51.47727272727273,
      "grad_norm": 0.3185841143131256,
      "learning_rate": 0.00042684043040020804,
      "loss": 2.2089,
      "step": 22650
    },
    {
      "epoch": 51.5,
      "grad_norm": 0.37764623761177063,
      "learning_rate": 0.00042677669529663686,
      "loss": 2.2062,
      "step": 22660
    },
    {
      "epoch": 51.52272727272727,
      "grad_norm": 0.17975744605064392,
      "learning_rate": 0.00042671293720570995,
      "loss": 2.2061,
      "step": 22670
    },
    {
      "epoch": 51.54545454545455,
      "grad_norm": 0.43980786204338074,
      "learning_rate": 0.0004266491561357181,
      "loss": 2.2056,
      "step": 22680
    },
    {
      "epoch": 51.56818181818182,
      "grad_norm": 0.22822342813014984,
      "learning_rate": 0.0004265853520949551,
      "loss": 2.2014,
      "step": 22690
    },
    {
      "epoch": 51.59090909090909,
      "grad_norm": 0.289471834897995,
      "learning_rate": 0.0004265215250917178,
      "loss": 2.2229,
      "step": 22700
    },
    {
      "epoch": 51.61363636363637,
      "grad_norm": 0.2247069627046585,
      "learning_rate": 0.000426457675134306,
      "loss": 2.2217,
      "step": 22710
    },
    {
      "epoch": 51.63636363636363,
      "grad_norm": 0.22915077209472656,
      "learning_rate": 0.00042639380223102267,
      "loss": 2.2055,
      "step": 22720
    },
    {
      "epoch": 51.65909090909091,
      "grad_norm": 0.2488602250814438,
      "learning_rate": 0.00042632990639017334,
      "loss": 2.1827,
      "step": 22730
    },
    {
      "epoch": 51.68181818181818,
      "grad_norm": 0.2637566924095154,
      "learning_rate": 0.00042626598762006694,
      "loss": 2.2046,
      "step": 22740
    },
    {
      "epoch": 51.70454545454545,
      "grad_norm": 0.31147709488868713,
      "learning_rate": 0.0004262020459290152,
      "loss": 2.2119,
      "step": 22750
    },
    {
      "epoch": 51.72727272727273,
      "grad_norm": 0.2699662446975708,
      "learning_rate": 0.0004261380813253328,
      "loss": 2.2081,
      "step": 22760
    },
    {
      "epoch": 51.75,
      "grad_norm": 0.17898884415626526,
      "learning_rate": 0.00042607409381733755,
      "loss": 2.2044,
      "step": 22770
    },
    {
      "epoch": 51.77272727272727,
      "grad_norm": 0.16679620742797852,
      "learning_rate": 0.00042601008341335,
      "loss": 2.2041,
      "step": 22780
    },
    {
      "epoch": 51.79545454545455,
      "grad_norm": 0.28270286321640015,
      "learning_rate": 0.00042594605012169387,
      "loss": 2.2017,
      "step": 22790
    },
    {
      "epoch": 51.81818181818182,
      "grad_norm": 0.19247746467590332,
      "learning_rate": 0.0004258819939506958,
      "loss": 2.1973,
      "step": 22800
    },
    {
      "epoch": 51.84090909090909,
      "grad_norm": 0.4046059250831604,
      "learning_rate": 0.0004258179149086855,
      "loss": 2.2038,
      "step": 22810
    },
    {
      "epoch": 51.86363636363637,
      "grad_norm": 0.2820051908493042,
      "learning_rate": 0.0004257538130039954,
      "loss": 2.2019,
      "step": 22820
    },
    {
      "epoch": 51.88636363636363,
      "grad_norm": 0.35441386699676514,
      "learning_rate": 0.0004256896882449612,
      "loss": 2.2051,
      "step": 22830
    },
    {
      "epoch": 51.90909090909091,
      "grad_norm": 0.2274377942085266,
      "learning_rate": 0.0004256255406399213,
      "loss": 2.2157,
      "step": 22840
    },
    {
      "epoch": 51.93181818181818,
      "grad_norm": 0.24650496244430542,
      "learning_rate": 0.00042556137019721736,
      "loss": 2.2093,
      "step": 22850
    },
    {
      "epoch": 51.95454545454545,
      "grad_norm": 0.16534391045570374,
      "learning_rate": 0.00042549717692519373,
      "loss": 2.2104,
      "step": 22860
    },
    {
      "epoch": 51.97727272727273,
      "grad_norm": 0.33078011870384216,
      "learning_rate": 0.00042543296083219793,
      "loss": 2.2028,
      "step": 22870
    },
    {
      "epoch": 52.0,
      "grad_norm": 0.7082218527793884,
      "learning_rate": 0.00042536872192658034,
      "loss": 2.2004,
      "step": 22880
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.1005730628967285,
      "eval_runtime": 8.7254,
      "eval_samples_per_second": 3487.62,
      "eval_steps_per_second": 13.638,
      "step": 22880
    },
    {
      "epoch": 52.02272727272727,
      "grad_norm": 0.19865639507770538,
      "learning_rate": 0.00042530446021669434,
      "loss": 2.2155,
      "step": 22890
    },
    {
      "epoch": 52.04545454545455,
      "grad_norm": 0.2428155243396759,
      "learning_rate": 0.00042524017571089626,
      "loss": 2.1981,
      "step": 22900
    },
    {
      "epoch": 52.06818181818182,
      "grad_norm": 0.16227221488952637,
      "learning_rate": 0.0004251758684175454,
      "loss": 2.1902,
      "step": 22910
    },
    {
      "epoch": 52.09090909090909,
      "grad_norm": 0.2525358498096466,
      "learning_rate": 0.00042511153834500404,
      "loss": 2.2065,
      "step": 22920
    },
    {
      "epoch": 52.11363636363637,
      "grad_norm": 0.4793722629547119,
      "learning_rate": 0.00042504718550163745,
      "loss": 2.2053,
      "step": 22930
    },
    {
      "epoch": 52.13636363636363,
      "grad_norm": 0.1771026849746704,
      "learning_rate": 0.00042498280989581384,
      "loss": 2.2122,
      "step": 22940
    },
    {
      "epoch": 52.15909090909091,
      "grad_norm": 0.5887184143066406,
      "learning_rate": 0.0004249184115359043,
      "loss": 2.2085,
      "step": 22950
    },
    {
      "epoch": 52.18181818181818,
      "grad_norm": 0.2709944248199463,
      "learning_rate": 0.0004248539904302829,
      "loss": 2.2018,
      "step": 22960
    },
    {
      "epoch": 52.20454545454545,
      "grad_norm": 0.6360200643539429,
      "learning_rate": 0.00042478954658732696,
      "loss": 2.1941,
      "step": 22970
    },
    {
      "epoch": 52.22727272727273,
      "grad_norm": 0.2665135860443115,
      "learning_rate": 0.00042472508001541615,
      "loss": 2.2018,
      "step": 22980
    },
    {
      "epoch": 52.25,
      "grad_norm": 0.19766631722450256,
      "learning_rate": 0.00042466059072293367,
      "loss": 2.2016,
      "step": 22990
    },
    {
      "epoch": 52.27272727272727,
      "grad_norm": 0.2294672727584839,
      "learning_rate": 0.0004245960787182654,
      "loss": 2.1994,
      "step": 23000
    },
    {
      "epoch": 52.29545454545455,
      "grad_norm": 0.40612080693244934,
      "learning_rate": 0.00042453154400980033,
      "loss": 2.2005,
      "step": 23010
    },
    {
      "epoch": 52.31818181818182,
      "grad_norm": 0.23653945326805115,
      "learning_rate": 0.00042446698660593017,
      "loss": 2.2113,
      "step": 23020
    },
    {
      "epoch": 52.34090909090909,
      "grad_norm": 0.28004205226898193,
      "learning_rate": 0.0004244024065150497,
      "loss": 2.2061,
      "step": 23030
    },
    {
      "epoch": 52.36363636363637,
      "grad_norm": 0.1735585778951645,
      "learning_rate": 0.0004243378037455568,
      "loss": 2.2142,
      "step": 23040
    },
    {
      "epoch": 52.38636363636363,
      "grad_norm": 0.1684737652540207,
      "learning_rate": 0.0004242731783058521,
      "loss": 2.1995,
      "step": 23050
    },
    {
      "epoch": 52.40909090909091,
      "grad_norm": 0.36056143045425415,
      "learning_rate": 0.0004242085302043392,
      "loss": 2.2107,
      "step": 23060
    },
    {
      "epoch": 52.43181818181818,
      "grad_norm": 0.24836291372776031,
      "learning_rate": 0.00042414385944942475,
      "loss": 2.1985,
      "step": 23070
    },
    {
      "epoch": 52.45454545454545,
      "grad_norm": 0.21441982686519623,
      "learning_rate": 0.0004240791660495182,
      "loss": 2.2069,
      "step": 23080
    },
    {
      "epoch": 52.47727272727273,
      "grad_norm": 0.16000394523143768,
      "learning_rate": 0.00042401445001303216,
      "loss": 2.2069,
      "step": 23090
    },
    {
      "epoch": 52.5,
      "grad_norm": 0.2590012848377228,
      "learning_rate": 0.0004239497113483819,
      "loss": 2.2044,
      "step": 23100
    },
    {
      "epoch": 52.52272727272727,
      "grad_norm": 0.17742887139320374,
      "learning_rate": 0.0004238849500639859,
      "loss": 2.2051,
      "step": 23110
    },
    {
      "epoch": 52.54545454545455,
      "grad_norm": 0.3620412349700928,
      "learning_rate": 0.00042382016616826544,
      "loss": 2.212,
      "step": 23120
    },
    {
      "epoch": 52.56818181818182,
      "grad_norm": 0.1834278106689453,
      "learning_rate": 0.00042375535966964476,
      "loss": 2.2008,
      "step": 23130
    },
    {
      "epoch": 52.59090909090909,
      "grad_norm": 0.20931348204612732,
      "learning_rate": 0.00042369053057655107,
      "loss": 2.1981,
      "step": 23140
    },
    {
      "epoch": 52.61363636363637,
      "grad_norm": 0.1742783933877945,
      "learning_rate": 0.0004236256788974144,
      "loss": 2.2097,
      "step": 23150
    },
    {
      "epoch": 52.63636363636363,
      "grad_norm": 0.24760717153549194,
      "learning_rate": 0.00042356080464066786,
      "loss": 2.2075,
      "step": 23160
    },
    {
      "epoch": 52.65909090909091,
      "grad_norm": 0.4328440725803375,
      "learning_rate": 0.0004234959078147475,
      "loss": 2.2061,
      "step": 23170
    },
    {
      "epoch": 52.68181818181818,
      "grad_norm": 0.18290975689888,
      "learning_rate": 0.0004234309884280922,
      "loss": 2.2003,
      "step": 23180
    },
    {
      "epoch": 52.70454545454545,
      "grad_norm": 0.2893403172492981,
      "learning_rate": 0.0004233660464891439,
      "loss": 2.1995,
      "step": 23190
    },
    {
      "epoch": 52.72727272727273,
      "grad_norm": 0.16164052486419678,
      "learning_rate": 0.00042330108200634725,
      "loss": 2.1929,
      "step": 23200
    },
    {
      "epoch": 52.75,
      "grad_norm": 0.2671036422252655,
      "learning_rate": 0.00042323609498815006,
      "loss": 2.211,
      "step": 23210
    },
    {
      "epoch": 52.77272727272727,
      "grad_norm": 0.2384018748998642,
      "learning_rate": 0.0004231710854430031,
      "loss": 2.2083,
      "step": 23220
    },
    {
      "epoch": 52.79545454545455,
      "grad_norm": 0.18639343976974487,
      "learning_rate": 0.0004231060533793598,
      "loss": 2.2163,
      "step": 23230
    },
    {
      "epoch": 52.81818181818182,
      "grad_norm": 0.19278451800346375,
      "learning_rate": 0.0004230409988056767,
      "loss": 2.2156,
      "step": 23240
    },
    {
      "epoch": 52.84090909090909,
      "grad_norm": 0.23683218657970428,
      "learning_rate": 0.0004229759217304133,
      "loss": 2.1981,
      "step": 23250
    },
    {
      "epoch": 52.86363636363637,
      "grad_norm": 0.364671915769577,
      "learning_rate": 0.00042291082216203195,
      "loss": 2.2095,
      "step": 23260
    },
    {
      "epoch": 52.88636363636363,
      "grad_norm": 0.3485511243343353,
      "learning_rate": 0.000422845700108998,
      "loss": 2.203,
      "step": 23270
    },
    {
      "epoch": 52.90909090909091,
      "grad_norm": 0.8049511313438416,
      "learning_rate": 0.0004227805555797795,
      "loss": 2.2014,
      "step": 23280
    },
    {
      "epoch": 52.93181818181818,
      "grad_norm": 0.2577398717403412,
      "learning_rate": 0.00042271538858284775,
      "loss": 2.2144,
      "step": 23290
    },
    {
      "epoch": 52.95454545454545,
      "grad_norm": 0.8688614368438721,
      "learning_rate": 0.0004226501991266768,
      "loss": 2.2109,
      "step": 23300
    },
    {
      "epoch": 52.97727272727273,
      "grad_norm": 0.15360510349273682,
      "learning_rate": 0.0004225849872197436,
      "loss": 2.2075,
      "step": 23310
    },
    {
      "epoch": 53.0,
      "grad_norm": 0.7002435326576233,
      "learning_rate": 0.000422519752870528,
      "loss": 2.2152,
      "step": 23320
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.1006343364715576,
      "eval_runtime": 8.8757,
      "eval_samples_per_second": 3428.581,
      "eval_steps_per_second": 13.407,
      "step": 23320
    },
    {
      "epoch": 53.02272727272727,
      "grad_norm": 0.298808217048645,
      "learning_rate": 0.0004224544960875129,
      "loss": 2.2076,
      "step": 23330
    },
    {
      "epoch": 53.04545454545455,
      "grad_norm": 0.21131929755210876,
      "learning_rate": 0.000422389216879184,
      "loss": 2.2039,
      "step": 23340
    },
    {
      "epoch": 53.06818181818182,
      "grad_norm": 0.22939664125442505,
      "learning_rate": 0.00042232391525403,
      "loss": 2.205,
      "step": 23350
    },
    {
      "epoch": 53.09090909090909,
      "grad_norm": 0.40119990706443787,
      "learning_rate": 0.0004222585912205424,
      "loss": 2.2042,
      "step": 23360
    },
    {
      "epoch": 53.11363636363637,
      "grad_norm": 0.2892421782016754,
      "learning_rate": 0.00042219324478721585,
      "loss": 2.2004,
      "step": 23370
    },
    {
      "epoch": 53.13636363636363,
      "grad_norm": 0.21657338738441467,
      "learning_rate": 0.0004221278759625475,
      "loss": 2.2111,
      "step": 23380
    },
    {
      "epoch": 53.15909090909091,
      "grad_norm": 0.19083015620708466,
      "learning_rate": 0.0004220624847550378,
      "loss": 2.194,
      "step": 23390
    },
    {
      "epoch": 53.18181818181818,
      "grad_norm": 0.19001838564872742,
      "learning_rate": 0.00042199707117319,
      "loss": 2.195,
      "step": 23400
    },
    {
      "epoch": 53.20454545454545,
      "grad_norm": 0.3608604669570923,
      "learning_rate": 0.0004219316352255101,
      "loss": 2.2013,
      "step": 23410
    },
    {
      "epoch": 53.22727272727273,
      "grad_norm": 0.20815032720565796,
      "learning_rate": 0.00042186617692050723,
      "loss": 2.2099,
      "step": 23420
    },
    {
      "epoch": 53.25,
      "grad_norm": 0.3082945942878723,
      "learning_rate": 0.0004218006962666934,
      "loss": 2.2143,
      "step": 23430
    },
    {
      "epoch": 53.27272727272727,
      "grad_norm": 0.23167820274829865,
      "learning_rate": 0.0004217351932725832,
      "loss": 2.2057,
      "step": 23440
    },
    {
      "epoch": 53.29545454545455,
      "grad_norm": 0.22443929314613342,
      "learning_rate": 0.00042166966794669476,
      "loss": 2.2124,
      "step": 23450
    },
    {
      "epoch": 53.31818181818182,
      "grad_norm": 0.274768590927124,
      "learning_rate": 0.00042160412029754846,
      "loss": 2.218,
      "step": 23460
    },
    {
      "epoch": 53.34090909090909,
      "grad_norm": 0.19068486988544464,
      "learning_rate": 0.00042153855033366796,
      "loss": 2.2002,
      "step": 23470
    },
    {
      "epoch": 53.36363636363637,
      "grad_norm": 0.21092449128627777,
      "learning_rate": 0.00042147295806357963,
      "loss": 2.2002,
      "step": 23480
    },
    {
      "epoch": 53.38636363636363,
      "grad_norm": 0.3102244734764099,
      "learning_rate": 0.000421407343495813,
      "loss": 2.2123,
      "step": 23490
    },
    {
      "epoch": 53.40909090909091,
      "grad_norm": 0.15808109939098358,
      "learning_rate": 0.00042134170663890027,
      "loss": 2.2062,
      "step": 23500
    },
    {
      "epoch": 53.43181818181818,
      "grad_norm": 0.6356277465820312,
      "learning_rate": 0.00042127604750137647,
      "loss": 2.208,
      "step": 23510
    },
    {
      "epoch": 53.45454545454545,
      "grad_norm": 0.2544889450073242,
      "learning_rate": 0.00042121036609177987,
      "loss": 2.2128,
      "step": 23520
    },
    {
      "epoch": 53.47727272727273,
      "grad_norm": 0.37085413932800293,
      "learning_rate": 0.0004211446624186512,
      "loss": 2.2074,
      "step": 23530
    },
    {
      "epoch": 53.5,
      "grad_norm": 0.835058331489563,
      "learning_rate": 0.00042107893649053456,
      "loss": 2.195,
      "step": 23540
    },
    {
      "epoch": 53.52272727272727,
      "grad_norm": 0.16815951466560364,
      "learning_rate": 0.00042101318831597656,
      "loss": 2.2007,
      "step": 23550
    },
    {
      "epoch": 53.54545454545455,
      "grad_norm": 0.42449453473091125,
      "learning_rate": 0.00042094741790352675,
      "loss": 2.1971,
      "step": 23560
    },
    {
      "epoch": 53.56818181818182,
      "grad_norm": 0.27281197905540466,
      "learning_rate": 0.0004208816252617379,
      "loss": 2.1997,
      "step": 23570
    },
    {
      "epoch": 53.59090909090909,
      "grad_norm": 0.39141836762428284,
      "learning_rate": 0.0004208158103991652,
      "loss": 2.2016,
      "step": 23580
    },
    {
      "epoch": 53.61363636363637,
      "grad_norm": 0.17346780002117157,
      "learning_rate": 0.00042074997332436706,
      "loss": 2.2049,
      "step": 23590
    },
    {
      "epoch": 53.63636363636363,
      "grad_norm": 0.25313785672187805,
      "learning_rate": 0.00042068411404590466,
      "loss": 2.2115,
      "step": 23600
    },
    {
      "epoch": 53.65909090909091,
      "grad_norm": 0.16074030101299286,
      "learning_rate": 0.0004206182325723421,
      "loss": 2.2109,
      "step": 23610
    },
    {
      "epoch": 53.68181818181818,
      "grad_norm": 0.23646171391010284,
      "learning_rate": 0.0004205523289122463,
      "loss": 2.2006,
      "step": 23620
    },
    {
      "epoch": 53.70454545454545,
      "grad_norm": 0.23790974915027618,
      "learning_rate": 0.00042048640307418717,
      "loss": 2.2081,
      "step": 23630
    },
    {
      "epoch": 53.72727272727273,
      "grad_norm": 0.34076976776123047,
      "learning_rate": 0.0004204204550667375,
      "loss": 2.203,
      "step": 23640
    },
    {
      "epoch": 53.75,
      "grad_norm": 0.18136250972747803,
      "learning_rate": 0.00042035448489847284,
      "loss": 2.2099,
      "step": 23650
    },
    {
      "epoch": 53.77272727272727,
      "grad_norm": 0.21808044612407684,
      "learning_rate": 0.00042028849257797163,
      "loss": 2.2005,
      "step": 23660
    },
    {
      "epoch": 53.79545454545455,
      "grad_norm": 0.29139244556427,
      "learning_rate": 0.00042022247811381535,
      "loss": 2.2143,
      "step": 23670
    },
    {
      "epoch": 53.81818181818182,
      "grad_norm": 0.3139859139919281,
      "learning_rate": 0.0004201564415145883,
      "loss": 2.1999,
      "step": 23680
    },
    {
      "epoch": 53.84090909090909,
      "grad_norm": 0.18503400683403015,
      "learning_rate": 0.0004200903827888775,
      "loss": 2.1937,
      "step": 23690
    },
    {
      "epoch": 53.86363636363637,
      "grad_norm": 0.19659700989723206,
      "learning_rate": 0.00042002430194527305,
      "loss": 2.1999,
      "step": 23700
    },
    {
      "epoch": 53.88636363636363,
      "grad_norm": 0.37519514560699463,
      "learning_rate": 0.0004199581989923679,
      "loss": 2.2153,
      "step": 23710
    },
    {
      "epoch": 53.90909090909091,
      "grad_norm": 0.3234323561191559,
      "learning_rate": 0.00041989207393875773,
      "loss": 2.2047,
      "step": 23720
    },
    {
      "epoch": 53.93181818181818,
      "grad_norm": 0.17906853556632996,
      "learning_rate": 0.0004198259267930412,
      "loss": 2.2199,
      "step": 23730
    },
    {
      "epoch": 53.95454545454545,
      "grad_norm": 0.17418475449085236,
      "learning_rate": 0.0004197597575638198,
      "loss": 2.193,
      "step": 23740
    },
    {
      "epoch": 53.97727272727273,
      "grad_norm": 0.4914945662021637,
      "learning_rate": 0.000419693566259698,
      "loss": 2.1986,
      "step": 23750
    },
    {
      "epoch": 54.0,
      "grad_norm": 0.4029172360897064,
      "learning_rate": 0.00041962735288928306,
      "loss": 2.2004,
      "step": 23760
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.100650429725647,
      "eval_runtime": 8.748,
      "eval_samples_per_second": 3478.618,
      "eval_steps_per_second": 13.603,
      "step": 23760
    },
    {
      "epoch": 54.02272727272727,
      "grad_norm": 0.35524284839630127,
      "learning_rate": 0.00041956111746118503,
      "loss": 2.1858,
      "step": 23770
    },
    {
      "epoch": 54.04545454545455,
      "grad_norm": 0.16515548527240753,
      "learning_rate": 0.0004194948599840169,
      "loss": 2.1971,
      "step": 23780
    },
    {
      "epoch": 54.06818181818182,
      "grad_norm": 0.1579931676387787,
      "learning_rate": 0.0004194285804663947,
      "loss": 2.1989,
      "step": 23790
    },
    {
      "epoch": 54.09090909090909,
      "grad_norm": 0.16917730867862701,
      "learning_rate": 0.000419362278916937,
      "loss": 2.2035,
      "step": 23800
    },
    {
      "epoch": 54.11363636363637,
      "grad_norm": 0.2858141362667084,
      "learning_rate": 0.0004192959553442654,
      "loss": 2.1961,
      "step": 23810
    },
    {
      "epoch": 54.13636363636363,
      "grad_norm": 0.1886780560016632,
      "learning_rate": 0.0004192296097570044,
      "loss": 2.1951,
      "step": 23820
    },
    {
      "epoch": 54.15909090909091,
      "grad_norm": 0.33232828974723816,
      "learning_rate": 0.00041916324216378144,
      "loss": 2.2084,
      "step": 23830
    },
    {
      "epoch": 54.18181818181818,
      "grad_norm": 0.1835661083459854,
      "learning_rate": 0.0004190968525732264,
      "loss": 2.2066,
      "step": 23840
    },
    {
      "epoch": 54.20454545454545,
      "grad_norm": 0.18283887207508087,
      "learning_rate": 0.0004190304409939727,
      "loss": 2.1943,
      "step": 23850
    },
    {
      "epoch": 54.22727272727273,
      "grad_norm": 0.34283870458602905,
      "learning_rate": 0.000418964007434656,
      "loss": 2.1979,
      "step": 23860
    },
    {
      "epoch": 54.25,
      "grad_norm": 0.506650447845459,
      "learning_rate": 0.0004188975519039151,
      "loss": 2.2002,
      "step": 23870
    },
    {
      "epoch": 54.27272727272727,
      "grad_norm": 0.42922189831733704,
      "learning_rate": 0.00041883107441039163,
      "loss": 2.2003,
      "step": 23880
    },
    {
      "epoch": 54.29545454545455,
      "grad_norm": 0.2311536967754364,
      "learning_rate": 0.00041876457496273006,
      "loss": 2.1902,
      "step": 23890
    },
    {
      "epoch": 54.31818181818182,
      "grad_norm": 0.24256384372711182,
      "learning_rate": 0.0004186980535695778,
      "loss": 2.2026,
      "step": 23900
    },
    {
      "epoch": 54.34090909090909,
      "grad_norm": 0.15153850615024567,
      "learning_rate": 0.0004186315102395849,
      "loss": 2.2193,
      "step": 23910
    },
    {
      "epoch": 54.36363636363637,
      "grad_norm": 0.8132234811782837,
      "learning_rate": 0.0004185649449814045,
      "loss": 2.2087,
      "step": 23920
    },
    {
      "epoch": 54.38636363636363,
      "grad_norm": 0.20562678575515747,
      "learning_rate": 0.0004184983578036925,
      "loss": 2.2137,
      "step": 23930
    },
    {
      "epoch": 54.40909090909091,
      "grad_norm": 0.4984847903251648,
      "learning_rate": 0.0004184317487151075,
      "loss": 2.2099,
      "step": 23940
    },
    {
      "epoch": 54.43181818181818,
      "grad_norm": 0.2929602563381195,
      "learning_rate": 0.0004183651177243113,
      "loss": 2.2069,
      "step": 23950
    },
    {
      "epoch": 54.45454545454545,
      "grad_norm": 0.19514523446559906,
      "learning_rate": 0.00041829846483996817,
      "loss": 2.2099,
      "step": 23960
    },
    {
      "epoch": 54.47727272727273,
      "grad_norm": 0.19514381885528564,
      "learning_rate": 0.00041823179007074535,
      "loss": 2.2098,
      "step": 23970
    },
    {
      "epoch": 54.5,
      "grad_norm": 0.20448391139507294,
      "learning_rate": 0.0004181650934253132,
      "loss": 2.208,
      "step": 23980
    },
    {
      "epoch": 54.52272727272727,
      "grad_norm": 0.17206431925296783,
      "learning_rate": 0.0004180983749123444,
      "loss": 2.2062,
      "step": 23990
    },
    {
      "epoch": 54.54545454545455,
      "grad_norm": 0.2022242248058319,
      "learning_rate": 0.000418031634540515,
      "loss": 2.2041,
      "step": 24000
    },
    {
      "epoch": 54.56818181818182,
      "grad_norm": 0.19706737995147705,
      "learning_rate": 0.0004179648723185036,
      "loss": 2.2099,
      "step": 24010
    },
    {
      "epoch": 54.59090909090909,
      "grad_norm": 0.2235192209482193,
      "learning_rate": 0.00041789808825499163,
      "loss": 2.1984,
      "step": 24020
    },
    {
      "epoch": 54.61363636363637,
      "grad_norm": 0.23478102684020996,
      "learning_rate": 0.00041783128235866354,
      "loss": 2.1975,
      "step": 24030
    },
    {
      "epoch": 54.63636363636363,
      "grad_norm": 0.1879957914352417,
      "learning_rate": 0.00041776445463820636,
      "loss": 2.2193,
      "step": 24040
    },
    {
      "epoch": 54.65909090909091,
      "grad_norm": 0.2472354769706726,
      "learning_rate": 0.00041769760510231024,
      "loss": 2.2093,
      "step": 24050
    },
    {
      "epoch": 54.68181818181818,
      "grad_norm": 0.9615475535392761,
      "learning_rate": 0.000417630733759668,
      "loss": 2.1992,
      "step": 24060
    },
    {
      "epoch": 54.70454545454545,
      "grad_norm": 0.3453364968299866,
      "learning_rate": 0.0004175638406189752,
      "loss": 2.2132,
      "step": 24070
    },
    {
      "epoch": 54.72727272727273,
      "grad_norm": 0.18354879319667816,
      "learning_rate": 0.0004174969256889307,
      "loss": 2.198,
      "step": 24080
    },
    {
      "epoch": 54.75,
      "grad_norm": 0.31741467118263245,
      "learning_rate": 0.0004174299889782355,
      "loss": 2.202,
      "step": 24090
    },
    {
      "epoch": 54.77272727272727,
      "grad_norm": 0.1863020807504654,
      "learning_rate": 0.0004173630304955939,
      "loss": 2.2007,
      "step": 24100
    },
    {
      "epoch": 54.79545454545455,
      "grad_norm": 0.30812346935272217,
      "learning_rate": 0.0004172960502497131,
      "loss": 2.2175,
      "step": 24110
    },
    {
      "epoch": 54.81818181818182,
      "grad_norm": 0.8616418838500977,
      "learning_rate": 0.00041722904824930264,
      "loss": 2.2127,
      "step": 24120
    },
    {
      "epoch": 54.84090909090909,
      "grad_norm": 0.20852364599704742,
      "learning_rate": 0.0004171620245030755,
      "loss": 2.2082,
      "step": 24130
    },
    {
      "epoch": 54.86363636363637,
      "grad_norm": 0.24787946045398712,
      "learning_rate": 0.0004170949790197469,
      "loss": 2.2032,
      "step": 24140
    },
    {
      "epoch": 54.88636363636363,
      "grad_norm": 0.20050455629825592,
      "learning_rate": 0.0004170279118080355,
      "loss": 2.2092,
      "step": 24150
    },
    {
      "epoch": 54.90909090909091,
      "grad_norm": 0.2049674093723297,
      "learning_rate": 0.00041696082287666223,
      "loss": 2.2011,
      "step": 24160
    },
    {
      "epoch": 54.93181818181818,
      "grad_norm": 0.15725524723529816,
      "learning_rate": 0.000416893712234351,
      "loss": 2.2124,
      "step": 24170
    },
    {
      "epoch": 54.95454545454545,
      "grad_norm": 0.2990231215953827,
      "learning_rate": 0.00041682657988982894,
      "loss": 2.1976,
      "step": 24180
    },
    {
      "epoch": 54.97727272727273,
      "grad_norm": 0.16692955791950226,
      "learning_rate": 0.0004167594258518254,
      "loss": 2.2054,
      "step": 24190
    },
    {
      "epoch": 55.0,
      "grad_norm": 0.3068157732486725,
      "learning_rate": 0.0004166922501290729,
      "loss": 2.205,
      "step": 24200
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.1002265214920044,
      "eval_runtime": 8.7049,
      "eval_samples_per_second": 3495.865,
      "eval_steps_per_second": 13.671,
      "step": 24200
    },
    {
      "epoch": 55.02272727272727,
      "grad_norm": 0.13950026035308838,
      "learning_rate": 0.00041662505273030683,
      "loss": 2.1971,
      "step": 24210
    },
    {
      "epoch": 55.04545454545455,
      "grad_norm": 0.5139100551605225,
      "learning_rate": 0.000416557833664265,
      "loss": 2.2115,
      "step": 24220
    },
    {
      "epoch": 55.06818181818182,
      "grad_norm": 0.1950657218694687,
      "learning_rate": 0.00041649059293968866,
      "loss": 2.1923,
      "step": 24230
    },
    {
      "epoch": 55.09090909090909,
      "grad_norm": 0.1873108446598053,
      "learning_rate": 0.00041642333056532134,
      "loss": 2.2039,
      "step": 24240
    },
    {
      "epoch": 55.11363636363637,
      "grad_norm": 0.2351974993944168,
      "learning_rate": 0.00041635604654990954,
      "loss": 2.196,
      "step": 24250
    },
    {
      "epoch": 55.13636363636363,
      "grad_norm": 0.3876747488975525,
      "learning_rate": 0.0004162887409022027,
      "loss": 2.2107,
      "step": 24260
    },
    {
      "epoch": 55.15909090909091,
      "grad_norm": 0.2081231027841568,
      "learning_rate": 0.000416221413630953,
      "loss": 2.2054,
      "step": 24270
    },
    {
      "epoch": 55.18181818181818,
      "grad_norm": 0.23495271801948547,
      "learning_rate": 0.00041615406474491533,
      "loss": 2.2022,
      "step": 24280
    },
    {
      "epoch": 55.20454545454545,
      "grad_norm": 0.30614444613456726,
      "learning_rate": 0.0004160866942528476,
      "loss": 2.2021,
      "step": 24290
    },
    {
      "epoch": 55.22727272727273,
      "grad_norm": 0.33557409048080444,
      "learning_rate": 0.0004160193021635103,
      "loss": 2.197,
      "step": 24300
    },
    {
      "epoch": 55.25,
      "grad_norm": 0.20642565190792084,
      "learning_rate": 0.0004159518884856669,
      "loss": 2.2029,
      "step": 24310
    },
    {
      "epoch": 55.27272727272727,
      "grad_norm": 0.2289198487997055,
      "learning_rate": 0.0004158844532280835,
      "loss": 2.2011,
      "step": 24320
    },
    {
      "epoch": 55.29545454545455,
      "grad_norm": 0.1614823043346405,
      "learning_rate": 0.0004158169963995293,
      "loss": 2.1929,
      "step": 24330
    },
    {
      "epoch": 55.31818181818182,
      "grad_norm": 0.19572463631629944,
      "learning_rate": 0.00041574951800877604,
      "loss": 2.2055,
      "step": 24340
    },
    {
      "epoch": 55.34090909090909,
      "grad_norm": 0.17722375690937042,
      "learning_rate": 0.0004156820180645983,
      "loss": 2.2059,
      "step": 24350
    },
    {
      "epoch": 55.36363636363637,
      "grad_norm": 0.20545914769172668,
      "learning_rate": 0.0004156144965757735,
      "loss": 2.2072,
      "step": 24360
    },
    {
      "epoch": 55.38636363636363,
      "grad_norm": 0.2812444865703583,
      "learning_rate": 0.000415546953551082,
      "loss": 2.2038,
      "step": 24370
    },
    {
      "epoch": 55.40909090909091,
      "grad_norm": 0.20178456604480743,
      "learning_rate": 0.0004154793889993067,
      "loss": 2.2032,
      "step": 24380
    },
    {
      "epoch": 55.43181818181818,
      "grad_norm": 0.2127700299024582,
      "learning_rate": 0.00041541180292923354,
      "loss": 2.1973,
      "step": 24390
    },
    {
      "epoch": 55.45454545454545,
      "grad_norm": 0.1930904984474182,
      "learning_rate": 0.00041534419534965106,
      "loss": 2.2011,
      "step": 24400
    },
    {
      "epoch": 55.47727272727273,
      "grad_norm": 0.22566533088684082,
      "learning_rate": 0.0004152765662693507,
      "loss": 2.2028,
      "step": 24410
    },
    {
      "epoch": 55.5,
      "grad_norm": 0.2180597484111786,
      "learning_rate": 0.00041520891569712677,
      "loss": 2.2099,
      "step": 24420
    },
    {
      "epoch": 55.52272727272727,
      "grad_norm": 0.1482100635766983,
      "learning_rate": 0.0004151412436417762,
      "loss": 2.2072,
      "step": 24430
    },
    {
      "epoch": 55.54545454545455,
      "grad_norm": 0.20237258076667786,
      "learning_rate": 0.00041507355011209885,
      "loss": 2.2021,
      "step": 24440
    },
    {
      "epoch": 55.56818181818182,
      "grad_norm": 0.184446781873703,
      "learning_rate": 0.00041500583511689727,
      "loss": 2.1959,
      "step": 24450
    },
    {
      "epoch": 55.59090909090909,
      "grad_norm": 0.2319900393486023,
      "learning_rate": 0.0004149380986649769,
      "loss": 2.2144,
      "step": 24460
    },
    {
      "epoch": 55.61363636363637,
      "grad_norm": 0.1647927165031433,
      "learning_rate": 0.00041487034076514583,
      "loss": 2.2043,
      "step": 24470
    },
    {
      "epoch": 55.63636363636363,
      "grad_norm": 0.38119176030158997,
      "learning_rate": 0.00041480256142621523,
      "loss": 2.2052,
      "step": 24480
    },
    {
      "epoch": 55.65909090909091,
      "grad_norm": 0.2119702249765396,
      "learning_rate": 0.0004147347606569988,
      "loss": 2.2158,
      "step": 24490
    },
    {
      "epoch": 55.68181818181818,
      "grad_norm": 0.20132316648960114,
      "learning_rate": 0.00041466693846631286,
      "loss": 2.1995,
      "step": 24500
    },
    {
      "epoch": 55.70454545454545,
      "grad_norm": 0.19745782017707825,
      "learning_rate": 0.0004145990948629771,
      "loss": 2.213,
      "step": 24510
    },
    {
      "epoch": 55.72727272727273,
      "grad_norm": 0.24800217151641846,
      "learning_rate": 0.0004145312298558133,
      "loss": 2.2088,
      "step": 24520
    },
    {
      "epoch": 55.75,
      "grad_norm": 0.18076787889003754,
      "learning_rate": 0.00041446334345364666,
      "loss": 2.2065,
      "step": 24530
    },
    {
      "epoch": 55.77272727272727,
      "grad_norm": 0.38991543650627136,
      "learning_rate": 0.0004143954356653047,
      "loss": 2.2004,
      "step": 24540
    },
    {
      "epoch": 55.79545454545455,
      "grad_norm": 0.16943341493606567,
      "learning_rate": 0.00041432750649961783,
      "loss": 2.2054,
      "step": 24550
    },
    {
      "epoch": 55.81818181818182,
      "grad_norm": 0.14841784536838531,
      "learning_rate": 0.0004142595559654194,
      "loss": 2.1974,
      "step": 24560
    },
    {
      "epoch": 55.84090909090909,
      "grad_norm": 0.27246832847595215,
      "learning_rate": 0.0004141915840715454,
      "loss": 2.2078,
      "step": 24570
    },
    {
      "epoch": 55.86363636363637,
      "grad_norm": 0.2251972109079361,
      "learning_rate": 0.0004141235908268348,
      "loss": 2.2096,
      "step": 24580
    },
    {
      "epoch": 55.88636363636363,
      "grad_norm": 0.40986359119415283,
      "learning_rate": 0.0004140555762401289,
      "loss": 2.2052,
      "step": 24590
    },
    {
      "epoch": 55.90909090909091,
      "grad_norm": 0.5614176392555237,
      "learning_rate": 0.0004139875403202722,
      "loss": 2.2212,
      "step": 24600
    },
    {
      "epoch": 55.93181818181818,
      "grad_norm": 0.28325748443603516,
      "learning_rate": 0.00041391948307611183,
      "loss": 2.2039,
      "step": 24610
    },
    {
      "epoch": 55.95454545454545,
      "grad_norm": 0.17892670631408691,
      "learning_rate": 0.00041385140451649763,
      "loss": 2.2008,
      "step": 24620
    },
    {
      "epoch": 55.97727272727273,
      "grad_norm": 0.24809014797210693,
      "learning_rate": 0.00041378330465028225,
      "loss": 2.2092,
      "step": 24630
    },
    {
      "epoch": 56.0,
      "grad_norm": 0.3034047782421112,
      "learning_rate": 0.0004137151834863213,
      "loss": 2.1947,
      "step": 24640
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.1002123355865479,
      "eval_runtime": 8.7643,
      "eval_samples_per_second": 3472.159,
      "eval_steps_per_second": 13.578,
      "step": 24640
    },
    {
      "epoch": 56.02272727272727,
      "grad_norm": 0.3431071937084198,
      "learning_rate": 0.00041364704103347285,
      "loss": 2.2031,
      "step": 24650
    },
    {
      "epoch": 56.04545454545455,
      "grad_norm": 0.32061195373535156,
      "learning_rate": 0.00041357887730059784,
      "loss": 2.1937,
      "step": 24660
    },
    {
      "epoch": 56.06818181818182,
      "grad_norm": 0.5100781321525574,
      "learning_rate": 0.00041351069229656016,
      "loss": 2.1986,
      "step": 24670
    },
    {
      "epoch": 56.09090909090909,
      "grad_norm": 0.2202606201171875,
      "learning_rate": 0.0004134424860302262,
      "loss": 2.208,
      "step": 24680
    },
    {
      "epoch": 56.11363636363637,
      "grad_norm": 0.1828395128250122,
      "learning_rate": 0.0004133742585104653,
      "loss": 2.2027,
      "step": 24690
    },
    {
      "epoch": 56.13636363636363,
      "grad_norm": 0.15813520550727844,
      "learning_rate": 0.00041330600974614953,
      "loss": 2.1999,
      "step": 24700
    },
    {
      "epoch": 56.15909090909091,
      "grad_norm": 0.27129843831062317,
      "learning_rate": 0.00041323773974615366,
      "loss": 2.1873,
      "step": 24710
    },
    {
      "epoch": 56.18181818181818,
      "grad_norm": 0.26730453968048096,
      "learning_rate": 0.00041316944851935513,
      "loss": 2.2032,
      "step": 24720
    },
    {
      "epoch": 56.20454545454545,
      "grad_norm": 0.21750107407569885,
      "learning_rate": 0.0004131011360746345,
      "loss": 2.2108,
      "step": 24730
    },
    {
      "epoch": 56.22727272727273,
      "grad_norm": 0.28581392765045166,
      "learning_rate": 0.0004130328024208747,
      "loss": 2.2018,
      "step": 24740
    },
    {
      "epoch": 56.25,
      "grad_norm": 0.2655509114265442,
      "learning_rate": 0.0004129644475669616,
      "loss": 2.2023,
      "step": 24750
    },
    {
      "epoch": 56.27272727272727,
      "grad_norm": 0.18839119374752045,
      "learning_rate": 0.0004128960715217839,
      "loss": 2.2051,
      "step": 24760
    },
    {
      "epoch": 56.29545454545455,
      "grad_norm": 0.203691303730011,
      "learning_rate": 0.00041282767429423286,
      "loss": 2.2119,
      "step": 24770
    },
    {
      "epoch": 56.31818181818182,
      "grad_norm": 0.2518711984157562,
      "learning_rate": 0.0004127592558932025,
      "loss": 2.2137,
      "step": 24780
    },
    {
      "epoch": 56.34090909090909,
      "grad_norm": 0.22042758762836456,
      "learning_rate": 0.00041269081632758984,
      "loss": 2.1923,
      "step": 24790
    },
    {
      "epoch": 56.36363636363637,
      "grad_norm": 0.2849924564361572,
      "learning_rate": 0.0004126223556062945,
      "loss": 2.216,
      "step": 24800
    },
    {
      "epoch": 56.38636363636363,
      "grad_norm": 0.18908414244651794,
      "learning_rate": 0.00041255387373821873,
      "loss": 2.2041,
      "step": 24810
    },
    {
      "epoch": 56.40909090909091,
      "grad_norm": 0.2266548126935959,
      "learning_rate": 0.0004124853707322678,
      "loss": 2.2067,
      "step": 24820
    },
    {
      "epoch": 56.43181818181818,
      "grad_norm": 0.14914588630199432,
      "learning_rate": 0.00041241684659734933,
      "loss": 2.2007,
      "step": 24830
    },
    {
      "epoch": 56.45454545454545,
      "grad_norm": 0.19525021314620972,
      "learning_rate": 0.00041234830134237417,
      "loss": 2.1949,
      "step": 24840
    },
    {
      "epoch": 56.47727272727273,
      "grad_norm": 0.26994654536247253,
      "learning_rate": 0.0004122797349762555,
      "loss": 2.2069,
      "step": 24850
    },
    {
      "epoch": 56.5,
      "grad_norm": 0.5666472911834717,
      "learning_rate": 0.00041221114750790964,
      "loss": 2.2031,
      "step": 24860
    },
    {
      "epoch": 56.52272727272727,
      "grad_norm": 0.23705041408538818,
      "learning_rate": 0.0004121425389462553,
      "loss": 2.1976,
      "step": 24870
    },
    {
      "epoch": 56.54545454545455,
      "grad_norm": 0.3143240511417389,
      "learning_rate": 0.000412073909300214,
      "loss": 2.2061,
      "step": 24880
    },
    {
      "epoch": 56.56818181818182,
      "grad_norm": 0.7182442545890808,
      "learning_rate": 0.0004120052585787102,
      "loss": 2.2143,
      "step": 24890
    },
    {
      "epoch": 56.59090909090909,
      "grad_norm": 0.2662580907344818,
      "learning_rate": 0.0004119365867906709,
      "loss": 2.2092,
      "step": 24900
    },
    {
      "epoch": 56.61363636363637,
      "grad_norm": 0.14202190935611725,
      "learning_rate": 0.00041186789394502603,
      "loss": 2.2122,
      "step": 24910
    },
    {
      "epoch": 56.63636363636363,
      "grad_norm": 0.4061161279678345,
      "learning_rate": 0.00041179918005070803,
      "loss": 2.2157,
      "step": 24920
    },
    {
      "epoch": 56.65909090909091,
      "grad_norm": 0.3427516222000122,
      "learning_rate": 0.0004117304451166522,
      "loss": 2.2032,
      "step": 24930
    },
    {
      "epoch": 56.68181818181818,
      "grad_norm": 0.24620339274406433,
      "learning_rate": 0.00041166168915179657,
      "loss": 2.2071,
      "step": 24940
    },
    {
      "epoch": 56.70454545454545,
      "grad_norm": 0.31991899013519287,
      "learning_rate": 0.00041159291216508187,
      "loss": 2.197,
      "step": 24950
    },
    {
      "epoch": 56.72727272727273,
      "grad_norm": 0.3518638610839844,
      "learning_rate": 0.0004115241141654517,
      "loss": 2.1918,
      "step": 24960
    },
    {
      "epoch": 56.75,
      "grad_norm": 0.25723743438720703,
      "learning_rate": 0.00041145529516185223,
      "loss": 2.2055,
      "step": 24970
    },
    {
      "epoch": 56.77272727272727,
      "grad_norm": 0.19559752941131592,
      "learning_rate": 0.0004113864551632323,
      "loss": 2.2128,
      "step": 24980
    },
    {
      "epoch": 56.79545454545455,
      "grad_norm": 0.29056864976882935,
      "learning_rate": 0.0004113175941785438,
      "loss": 2.1978,
      "step": 24990
    },
    {
      "epoch": 56.81818181818182,
      "grad_norm": 0.24836912751197815,
      "learning_rate": 0.000411248712216741,
      "loss": 2.2063,
      "step": 25000
    },
    {
      "epoch": 56.84090909090909,
      "grad_norm": 0.241599440574646,
      "learning_rate": 0.00041117980928678105,
      "loss": 2.2014,
      "step": 25010
    },
    {
      "epoch": 56.86363636363637,
      "grad_norm": 0.44879645109176636,
      "learning_rate": 0.000411110885397624,
      "loss": 2.2037,
      "step": 25020
    },
    {
      "epoch": 56.88636363636363,
      "grad_norm": 0.2091628462076187,
      "learning_rate": 0.00041104194055823206,
      "loss": 2.2052,
      "step": 25030
    },
    {
      "epoch": 56.90909090909091,
      "grad_norm": 0.2809937000274658,
      "learning_rate": 0.000410972974777571,
      "loss": 2.2054,
      "step": 25040
    },
    {
      "epoch": 56.93181818181818,
      "grad_norm": 0.3843725323677063,
      "learning_rate": 0.00041090398806460854,
      "loss": 2.1918,
      "step": 25050
    },
    {
      "epoch": 56.95454545454545,
      "grad_norm": 0.2865647077560425,
      "learning_rate": 0.00041083498042831563,
      "loss": 2.1999,
      "step": 25060
    },
    {
      "epoch": 56.97727272727273,
      "grad_norm": 0.40058571100234985,
      "learning_rate": 0.0004107659518776656,
      "loss": 2.2069,
      "step": 25070
    },
    {
      "epoch": 57.0,
      "grad_norm": 0.7985745668411255,
      "learning_rate": 0.0004106969024216348,
      "loss": 2.1911,
      "step": 25080
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.10054349899292,
      "eval_runtime": 8.7297,
      "eval_samples_per_second": 3485.908,
      "eval_steps_per_second": 13.632,
      "step": 25080
    },
    {
      "epoch": 57.02272727272727,
      "grad_norm": 0.2789314389228821,
      "learning_rate": 0.00041062783206920216,
      "loss": 2.204,
      "step": 25090
    },
    {
      "epoch": 57.04545454545455,
      "grad_norm": 0.2080635130405426,
      "learning_rate": 0.0004105587408293492,
      "loss": 2.2068,
      "step": 25100
    },
    {
      "epoch": 57.06818181818182,
      "grad_norm": 0.18636539578437805,
      "learning_rate": 0.00041048962871106026,
      "loss": 2.2031,
      "step": 25110
    },
    {
      "epoch": 57.09090909090909,
      "grad_norm": 0.3094862699508667,
      "learning_rate": 0.0004104204957233225,
      "loss": 2.2034,
      "step": 25120
    },
    {
      "epoch": 57.11363636363637,
      "grad_norm": 0.35933035612106323,
      "learning_rate": 0.00041035134187512574,
      "loss": 2.2041,
      "step": 25130
    },
    {
      "epoch": 57.13636363636363,
      "grad_norm": 0.233138769865036,
      "learning_rate": 0.0004102821671754624,
      "loss": 2.195,
      "step": 25140
    },
    {
      "epoch": 57.15909090909091,
      "grad_norm": 0.18079225718975067,
      "learning_rate": 0.0004102129716333277,
      "loss": 2.2066,
      "step": 25150
    },
    {
      "epoch": 57.18181818181818,
      "grad_norm": 0.2546811103820801,
      "learning_rate": 0.0004101437552577196,
      "loss": 2.1961,
      "step": 25160
    },
    {
      "epoch": 57.20454545454545,
      "grad_norm": 0.663280189037323,
      "learning_rate": 0.00041007451805763874,
      "loss": 2.2054,
      "step": 25170
    },
    {
      "epoch": 57.22727272727273,
      "grad_norm": 0.45947858691215515,
      "learning_rate": 0.00041000526004208836,
      "loss": 2.2005,
      "step": 25180
    },
    {
      "epoch": 57.25,
      "grad_norm": 0.3117471933364868,
      "learning_rate": 0.0004099359812200746,
      "loss": 2.2008,
      "step": 25190
    },
    {
      "epoch": 57.27272727272727,
      "grad_norm": 0.21861302852630615,
      "learning_rate": 0.00040986668160060614,
      "loss": 2.2046,
      "step": 25200
    },
    {
      "epoch": 57.29545454545455,
      "grad_norm": 0.38941165804862976,
      "learning_rate": 0.0004097973611926945,
      "loss": 2.1975,
      "step": 25210
    },
    {
      "epoch": 57.31818181818182,
      "grad_norm": 0.2060479074716568,
      "learning_rate": 0.0004097280200053538,
      "loss": 2.2045,
      "step": 25220
    },
    {
      "epoch": 57.34090909090909,
      "grad_norm": 0.4635714888572693,
      "learning_rate": 0.0004096586580476008,
      "loss": 2.1933,
      "step": 25230
    },
    {
      "epoch": 57.36363636363637,
      "grad_norm": 0.7857822179794312,
      "learning_rate": 0.00040958927532845537,
      "loss": 2.2094,
      "step": 25240
    },
    {
      "epoch": 57.38636363636363,
      "grad_norm": 0.27017828822135925,
      "learning_rate": 0.0004095198718569394,
      "loss": 2.2124,
      "step": 25250
    },
    {
      "epoch": 57.40909090909091,
      "grad_norm": 0.3700818121433258,
      "learning_rate": 0.0004094504476420781,
      "loss": 2.2153,
      "step": 25260
    },
    {
      "epoch": 57.43181818181818,
      "grad_norm": 0.16612890362739563,
      "learning_rate": 0.000409381002692899,
      "loss": 2.2035,
      "step": 25270
    },
    {
      "epoch": 57.45454545454545,
      "grad_norm": 0.2033572494983673,
      "learning_rate": 0.0004093115370184324,
      "loss": 2.2051,
      "step": 25280
    },
    {
      "epoch": 57.47727272727273,
      "grad_norm": 0.2756322920322418,
      "learning_rate": 0.0004092420506277116,
      "loss": 2.1991,
      "step": 25290
    },
    {
      "epoch": 57.5,
      "grad_norm": 0.24469389021396637,
      "learning_rate": 0.00040917254352977204,
      "loss": 2.217,
      "step": 25300
    },
    {
      "epoch": 57.52272727272727,
      "grad_norm": 0.20496319234371185,
      "learning_rate": 0.0004091030157336524,
      "loss": 2.2013,
      "step": 25310
    },
    {
      "epoch": 57.54545454545455,
      "grad_norm": 0.2354210466146469,
      "learning_rate": 0.0004090334672483936,
      "loss": 2.1998,
      "step": 25320
    },
    {
      "epoch": 57.56818181818182,
      "grad_norm": 0.29859980940818787,
      "learning_rate": 0.0004089638980830396,
      "loss": 2.2071,
      "step": 25330
    },
    {
      "epoch": 57.59090909090909,
      "grad_norm": 0.3554973602294922,
      "learning_rate": 0.00040889430824663685,
      "loss": 2.2009,
      "step": 25340
    },
    {
      "epoch": 57.61363636363637,
      "grad_norm": 0.3036842346191406,
      "learning_rate": 0.0004088246977482346,
      "loss": 2.212,
      "step": 25350
    },
    {
      "epoch": 57.63636363636363,
      "grad_norm": 0.29369550943374634,
      "learning_rate": 0.0004087550665968846,
      "loss": 2.2026,
      "step": 25360
    },
    {
      "epoch": 57.65909090909091,
      "grad_norm": 0.49135664105415344,
      "learning_rate": 0.00040868541480164154,
      "loss": 2.1918,
      "step": 25370
    },
    {
      "epoch": 57.68181818181818,
      "grad_norm": 0.2906557321548462,
      "learning_rate": 0.0004086157423715626,
      "loss": 2.2043,
      "step": 25380
    },
    {
      "epoch": 57.70454545454545,
      "grad_norm": 1.0487418174743652,
      "learning_rate": 0.0004085460493157077,
      "loss": 2.1977,
      "step": 25390
    },
    {
      "epoch": 57.72727272727273,
      "grad_norm": 0.17451122403144836,
      "learning_rate": 0.0004084763356431396,
      "loss": 2.2028,
      "step": 25400
    },
    {
      "epoch": 57.75,
      "grad_norm": 0.168075829744339,
      "learning_rate": 0.0004084066013629233,
      "loss": 2.2004,
      "step": 25410
    },
    {
      "epoch": 57.77272727272727,
      "grad_norm": 0.35567334294319153,
      "learning_rate": 0.0004083368464841271,
      "loss": 2.2009,
      "step": 25420
    },
    {
      "epoch": 57.79545454545455,
      "grad_norm": 0.3616563081741333,
      "learning_rate": 0.00040826707101582145,
      "loss": 2.2123,
      "step": 25430
    },
    {
      "epoch": 57.81818181818182,
      "grad_norm": 0.24417202174663544,
      "learning_rate": 0.00040819727496707977,
      "loss": 2.2044,
      "step": 25440
    },
    {
      "epoch": 57.84090909090909,
      "grad_norm": 0.1772553026676178,
      "learning_rate": 0.00040812745834697804,
      "loss": 2.2109,
      "step": 25450
    },
    {
      "epoch": 57.86363636363637,
      "grad_norm": 0.19543151557445526,
      "learning_rate": 0.00040805762116459494,
      "loss": 2.1931,
      "step": 25460
    },
    {
      "epoch": 57.88636363636363,
      "grad_norm": 0.2417723536491394,
      "learning_rate": 0.00040798776342901186,
      "loss": 2.2085,
      "step": 25470
    },
    {
      "epoch": 57.90909090909091,
      "grad_norm": 0.2117428034543991,
      "learning_rate": 0.0004079178851493127,
      "loss": 2.1955,
      "step": 25480
    },
    {
      "epoch": 57.93181818181818,
      "grad_norm": 0.17831410467624664,
      "learning_rate": 0.00040784798633458435,
      "loss": 2.1972,
      "step": 25490
    },
    {
      "epoch": 57.95454545454545,
      "grad_norm": 0.16859814524650574,
      "learning_rate": 0.00040777806699391614,
      "loss": 2.1946,
      "step": 25500
    },
    {
      "epoch": 57.97727272727273,
      "grad_norm": 0.17225217819213867,
      "learning_rate": 0.00040770812713639994,
      "loss": 2.2074,
      "step": 25510
    },
    {
      "epoch": 58.0,
      "grad_norm": 0.3397919833660126,
      "learning_rate": 0.00040763816677113064,
      "loss": 2.2004,
      "step": 25520
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.099855899810791,
      "eval_runtime": 8.7126,
      "eval_samples_per_second": 3492.752,
      "eval_steps_per_second": 13.658,
      "step": 25520
    },
    {
      "epoch": 58.02272727272727,
      "grad_norm": 0.16887396574020386,
      "learning_rate": 0.0004075681859072055,
      "loss": 2.1991,
      "step": 25530
    },
    {
      "epoch": 58.04545454545455,
      "grad_norm": 0.19292686879634857,
      "learning_rate": 0.0004074981845537247,
      "loss": 2.1963,
      "step": 25540
    },
    {
      "epoch": 58.06818181818182,
      "grad_norm": 0.26211899518966675,
      "learning_rate": 0.0004074281627197908,
      "loss": 2.2077,
      "step": 25550
    },
    {
      "epoch": 58.09090909090909,
      "grad_norm": 0.15487338602542877,
      "learning_rate": 0.0004073581204145093,
      "loss": 2.1999,
      "step": 25560
    },
    {
      "epoch": 58.11363636363637,
      "grad_norm": 0.22486458718776703,
      "learning_rate": 0.00040728805764698805,
      "loss": 2.2084,
      "step": 25570
    },
    {
      "epoch": 58.13636363636363,
      "grad_norm": 0.22572775185108185,
      "learning_rate": 0.0004072179744263378,
      "loss": 2.2017,
      "step": 25580
    },
    {
      "epoch": 58.15909090909091,
      "grad_norm": 0.31931695342063904,
      "learning_rate": 0.0004071478707616721,
      "loss": 2.1939,
      "step": 25590
    },
    {
      "epoch": 58.18181818181818,
      "grad_norm": 0.19835928082466125,
      "learning_rate": 0.00040707774666210676,
      "loss": 2.2074,
      "step": 25600
    },
    {
      "epoch": 58.20454545454545,
      "grad_norm": 0.1666366308927536,
      "learning_rate": 0.00040700760213676046,
      "loss": 2.1973,
      "step": 25610
    },
    {
      "epoch": 58.22727272727273,
      "grad_norm": 0.35817384719848633,
      "learning_rate": 0.00040693743719475453,
      "loss": 2.2165,
      "step": 25620
    },
    {
      "epoch": 58.25,
      "grad_norm": 0.7723163962364197,
      "learning_rate": 0.000406867251845213,
      "loss": 2.2047,
      "step": 25630
    },
    {
      "epoch": 58.27272727272727,
      "grad_norm": 0.268681138753891,
      "learning_rate": 0.0004067970460972625,
      "loss": 2.2037,
      "step": 25640
    },
    {
      "epoch": 58.29545454545455,
      "grad_norm": 0.6327763795852661,
      "learning_rate": 0.00040672681996003227,
      "loss": 2.2078,
      "step": 25650
    },
    {
      "epoch": 58.31818181818182,
      "grad_norm": 0.2637805938720703,
      "learning_rate": 0.0004066565734426542,
      "loss": 2.2174,
      "step": 25660
    },
    {
      "epoch": 58.34090909090909,
      "grad_norm": 0.318905234336853,
      "learning_rate": 0.000406586306554263,
      "loss": 2.1968,
      "step": 25670
    },
    {
      "epoch": 58.36363636363637,
      "grad_norm": 0.24074119329452515,
      "learning_rate": 0.00040651601930399573,
      "loss": 2.2104,
      "step": 25680
    },
    {
      "epoch": 58.38636363636363,
      "grad_norm": 0.23040121793746948,
      "learning_rate": 0.0004064457117009925,
      "loss": 2.19,
      "step": 25690
    },
    {
      "epoch": 58.40909090909091,
      "grad_norm": 0.21373112499713898,
      "learning_rate": 0.00040637538375439566,
      "loss": 2.2028,
      "step": 25700
    },
    {
      "epoch": 58.43181818181818,
      "grad_norm": 0.21720346808433533,
      "learning_rate": 0.0004063050354733504,
      "loss": 2.2004,
      "step": 25710
    },
    {
      "epoch": 58.45454545454545,
      "grad_norm": 0.15009944140911102,
      "learning_rate": 0.0004062346668670046,
      "loss": 2.2089,
      "step": 25720
    },
    {
      "epoch": 58.47727272727273,
      "grad_norm": 0.16098777949810028,
      "learning_rate": 0.0004061642779445087,
      "loss": 2.1966,
      "step": 25730
    },
    {
      "epoch": 58.5,
      "grad_norm": 0.18364140391349792,
      "learning_rate": 0.00040609386871501583,
      "loss": 2.1884,
      "step": 25740
    },
    {
      "epoch": 58.52272727272727,
      "grad_norm": 0.12145426124334335,
      "learning_rate": 0.0004060234391876817,
      "loss": 2.2103,
      "step": 25750
    },
    {
      "epoch": 58.54545454545455,
      "grad_norm": 0.13971512019634247,
      "learning_rate": 0.0004059529893716647,
      "loss": 2.1955,
      "step": 25760
    },
    {
      "epoch": 58.56818181818182,
      "grad_norm": 0.18361739814281464,
      "learning_rate": 0.00040588251927612583,
      "loss": 2.1922,
      "step": 25770
    },
    {
      "epoch": 58.59090909090909,
      "grad_norm": 0.17335578799247742,
      "learning_rate": 0.0004058120289102288,
      "loss": 2.2042,
      "step": 25780
    },
    {
      "epoch": 58.61363636363637,
      "grad_norm": 0.2746987044811249,
      "learning_rate": 0.00040574151828313976,
      "loss": 2.1962,
      "step": 25790
    },
    {
      "epoch": 58.63636363636363,
      "grad_norm": 0.19031313061714172,
      "learning_rate": 0.00040567098740402786,
      "loss": 2.2103,
      "step": 25800
    },
    {
      "epoch": 58.65909090909091,
      "grad_norm": 0.16691267490386963,
      "learning_rate": 0.00040560043628206445,
      "loss": 2.2149,
      "step": 25810
    },
    {
      "epoch": 58.68181818181818,
      "grad_norm": 0.8006591200828552,
      "learning_rate": 0.0004055298649264239,
      "loss": 2.2,
      "step": 25820
    },
    {
      "epoch": 58.70454545454545,
      "grad_norm": 0.3064268231391907,
      "learning_rate": 0.00040545927334628294,
      "loss": 2.2154,
      "step": 25830
    },
    {
      "epoch": 58.72727272727273,
      "grad_norm": 1.0327779054641724,
      "learning_rate": 0.000405388661550821,
      "loss": 2.217,
      "step": 25840
    },
    {
      "epoch": 58.75,
      "grad_norm": 0.5495724678039551,
      "learning_rate": 0.0004053180295492203,
      "loss": 2.206,
      "step": 25850
    },
    {
      "epoch": 58.77272727272727,
      "grad_norm": 0.39322397112846375,
      "learning_rate": 0.00040524737735066533,
      "loss": 2.2014,
      "step": 25860
    },
    {
      "epoch": 58.79545454545455,
      "grad_norm": 0.22498176991939545,
      "learning_rate": 0.00040517670496434367,
      "loss": 2.2007,
      "step": 25870
    },
    {
      "epoch": 58.81818181818182,
      "grad_norm": 0.4175259470939636,
      "learning_rate": 0.0004051060123994451,
      "loss": 2.1945,
      "step": 25880
    },
    {
      "epoch": 58.84090909090909,
      "grad_norm": 0.250521719455719,
      "learning_rate": 0.0004050352996651623,
      "loss": 2.1991,
      "step": 25890
    },
    {
      "epoch": 58.86363636363637,
      "grad_norm": 0.4910549819469452,
      "learning_rate": 0.00040496456677069045,
      "loss": 2.2109,
      "step": 25900
    },
    {
      "epoch": 58.88636363636363,
      "grad_norm": 1.423685073852539,
      "learning_rate": 0.00040489381372522736,
      "loss": 2.211,
      "step": 25910
    },
    {
      "epoch": 58.90909090909091,
      "grad_norm": 0.48726439476013184,
      "learning_rate": 0.0004048230405379736,
      "loss": 2.193,
      "step": 25920
    },
    {
      "epoch": 58.93181818181818,
      "grad_norm": 0.3274953067302704,
      "learning_rate": 0.00040475224721813206,
      "loss": 2.1983,
      "step": 25930
    },
    {
      "epoch": 58.95454545454545,
      "grad_norm": 0.5644007325172424,
      "learning_rate": 0.0004046814337749085,
      "loss": 2.2016,
      "step": 25940
    },
    {
      "epoch": 58.97727272727273,
      "grad_norm": 2.4023633003234863,
      "learning_rate": 0.0004046106002175113,
      "loss": 2.2009,
      "step": 25950
    },
    {
      "epoch": 59.0,
      "grad_norm": 0.82928466796875,
      "learning_rate": 0.0004045397465551513,
      "loss": 2.2017,
      "step": 25960
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.1003183126449585,
      "eval_runtime": 8.7327,
      "eval_samples_per_second": 3484.726,
      "eval_steps_per_second": 13.627,
      "step": 25960
    },
    {
      "epoch": 59.02272727272727,
      "grad_norm": 0.42422208189964294,
      "learning_rate": 0.0004044688727970421,
      "loss": 2.199,
      "step": 25970
    },
    {
      "epoch": 59.04545454545455,
      "grad_norm": 0.29016104340553284,
      "learning_rate": 0.0004043979789523998,
      "loss": 2.1892,
      "step": 25980
    },
    {
      "epoch": 59.06818181818182,
      "grad_norm": 0.21091383695602417,
      "learning_rate": 0.00040432706503044317,
      "loss": 2.1983,
      "step": 25990
    },
    {
      "epoch": 59.09090909090909,
      "grad_norm": 0.2503257989883423,
      "learning_rate": 0.0004042561310403935,
      "loss": 2.1956,
      "step": 26000
    },
    {
      "epoch": 59.11363636363637,
      "grad_norm": 0.7710888981819153,
      "learning_rate": 0.0004041851769914749,
      "loss": 2.208,
      "step": 26010
    },
    {
      "epoch": 59.13636363636363,
      "grad_norm": 0.9671084880828857,
      "learning_rate": 0.0004041142028929139,
      "loss": 2.2112,
      "step": 26020
    },
    {
      "epoch": 59.15909090909091,
      "grad_norm": 0.5226677060127258,
      "learning_rate": 0.00040404320875393973,
      "loss": 2.1886,
      "step": 26030
    },
    {
      "epoch": 59.18181818181818,
      "grad_norm": 0.38711482286453247,
      "learning_rate": 0.0004039721945837842,
      "loss": 2.2181,
      "step": 26040
    },
    {
      "epoch": 59.20454545454545,
      "grad_norm": 0.15670539438724518,
      "learning_rate": 0.0004039011603916817,
      "loss": 2.2131,
      "step": 26050
    },
    {
      "epoch": 59.22727272727273,
      "grad_norm": 0.3102886974811554,
      "learning_rate": 0.0004038301061868691,
      "loss": 2.2048,
      "step": 26060
    },
    {
      "epoch": 59.25,
      "grad_norm": 0.1963948756456375,
      "learning_rate": 0.0004037590319785863,
      "loss": 2.2003,
      "step": 26070
    },
    {
      "epoch": 59.27272727272727,
      "grad_norm": 0.6798303127288818,
      "learning_rate": 0.0004036879377760753,
      "loss": 2.2131,
      "step": 26080
    },
    {
      "epoch": 59.29545454545455,
      "grad_norm": 0.44037532806396484,
      "learning_rate": 0.0004036168235885809,
      "loss": 2.1948,
      "step": 26090
    },
    {
      "epoch": 59.31818181818182,
      "grad_norm": 0.22067198157310486,
      "learning_rate": 0.00040354568942535063,
      "loss": 2.1928,
      "step": 26100
    },
    {
      "epoch": 59.34090909090909,
      "grad_norm": 0.2931511104106903,
      "learning_rate": 0.0004034745352956344,
      "loss": 2.1998,
      "step": 26110
    },
    {
      "epoch": 59.36363636363637,
      "grad_norm": 0.3896417021751404,
      "learning_rate": 0.00040340336120868496,
      "loss": 2.2087,
      "step": 26120
    },
    {
      "epoch": 59.38636363636363,
      "grad_norm": 0.16131266951560974,
      "learning_rate": 0.00040333216717375743,
      "loss": 2.2083,
      "step": 26130
    },
    {
      "epoch": 59.40909090909091,
      "grad_norm": 0.15568824112415314,
      "learning_rate": 0.00040326095320010946,
      "loss": 2.2128,
      "step": 26140
    },
    {
      "epoch": 59.43181818181818,
      "grad_norm": 0.22547616064548492,
      "learning_rate": 0.00040318971929700177,
      "loss": 2.1997,
      "step": 26150
    },
    {
      "epoch": 59.45454545454545,
      "grad_norm": 0.41130372881889343,
      "learning_rate": 0.000403118465473697,
      "loss": 2.2118,
      "step": 26160
    },
    {
      "epoch": 59.47727272727273,
      "grad_norm": 0.2499101459980011,
      "learning_rate": 0.0004030471917394609,
      "loss": 2.2054,
      "step": 26170
    },
    {
      "epoch": 59.5,
      "grad_norm": 0.31387031078338623,
      "learning_rate": 0.00040297589810356165,
      "loss": 2.2111,
      "step": 26180
    },
    {
      "epoch": 59.52272727272727,
      "grad_norm": 0.21869856119155884,
      "learning_rate": 0.0004029045845752699,
      "loss": 2.1998,
      "step": 26190
    },
    {
      "epoch": 59.54545454545455,
      "grad_norm": 0.43332281708717346,
      "learning_rate": 0.00040283325116385904,
      "loss": 2.2091,
      "step": 26200
    },
    {
      "epoch": 59.56818181818182,
      "grad_norm": 0.4522305130958557,
      "learning_rate": 0.00040276189787860503,
      "loss": 2.1991,
      "step": 26210
    },
    {
      "epoch": 59.59090909090909,
      "grad_norm": 0.220464289188385,
      "learning_rate": 0.00040269052472878634,
      "loss": 2.1924,
      "step": 26220
    },
    {
      "epoch": 59.61363636363637,
      "grad_norm": 0.24447967112064362,
      "learning_rate": 0.00040261913172368394,
      "loss": 2.1924,
      "step": 26230
    },
    {
      "epoch": 59.63636363636363,
      "grad_norm": 0.2999964654445648,
      "learning_rate": 0.0004025477188725817,
      "loss": 2.1911,
      "step": 26240
    },
    {
      "epoch": 59.65909090909091,
      "grad_norm": 0.26954543590545654,
      "learning_rate": 0.0004024762861847657,
      "loss": 2.2097,
      "step": 26250
    },
    {
      "epoch": 59.68181818181818,
      "grad_norm": 0.2362111508846283,
      "learning_rate": 0.00040240483366952483,
      "loss": 2.1991,
      "step": 26260
    },
    {
      "epoch": 59.70454545454545,
      "grad_norm": 0.16481080651283264,
      "learning_rate": 0.00040233336133615056,
      "loss": 2.2002,
      "step": 26270
    },
    {
      "epoch": 59.72727272727273,
      "grad_norm": 0.41898488998413086,
      "learning_rate": 0.0004022618691939368,
      "loss": 2.2089,
      "step": 26280
    },
    {
      "epoch": 59.75,
      "grad_norm": 0.27712762355804443,
      "learning_rate": 0.0004021903572521802,
      "loss": 2.195,
      "step": 26290
    },
    {
      "epoch": 59.77272727272727,
      "grad_norm": 0.2284635305404663,
      "learning_rate": 0.00040211882552017975,
      "loss": 2.2158,
      "step": 26300
    },
    {
      "epoch": 59.79545454545455,
      "grad_norm": 0.22522124648094177,
      "learning_rate": 0.0004020472740072373,
      "loss": 2.2074,
      "step": 26310
    },
    {
      "epoch": 59.81818181818182,
      "grad_norm": 0.13106796145439148,
      "learning_rate": 0.00040197570272265703,
      "loss": 2.1934,
      "step": 26320
    },
    {
      "epoch": 59.84090909090909,
      "grad_norm": 0.2891511023044586,
      "learning_rate": 0.0004019041116757459,
      "loss": 2.2163,
      "step": 26330
    },
    {
      "epoch": 59.86363636363637,
      "grad_norm": 0.20171129703521729,
      "learning_rate": 0.0004018325008758132,
      "loss": 2.2054,
      "step": 26340
    },
    {
      "epoch": 59.88636363636363,
      "grad_norm": 0.20859991014003754,
      "learning_rate": 0.000401760870332171,
      "loss": 2.2052,
      "step": 26350
    },
    {
      "epoch": 59.90909090909091,
      "grad_norm": 0.46704286336898804,
      "learning_rate": 0.0004016892200541339,
      "loss": 2.2004,
      "step": 26360
    },
    {
      "epoch": 59.93181818181818,
      "grad_norm": 1.0901594161987305,
      "learning_rate": 0.00040161755005101884,
      "loss": 2.1951,
      "step": 26370
    },
    {
      "epoch": 59.95454545454545,
      "grad_norm": 0.9756749272346497,
      "learning_rate": 0.0004015458603321458,
      "loss": 2.2169,
      "step": 26380
    },
    {
      "epoch": 59.97727272727273,
      "grad_norm": 0.23692049086093903,
      "learning_rate": 0.0004014741509068367,
      "loss": 2.2057,
      "step": 26390
    },
    {
      "epoch": 60.0,
      "grad_norm": 0.3607257604598999,
      "learning_rate": 0.00040140242178441667,
      "loss": 2.2007,
      "step": 26400
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.1004397869110107,
      "eval_runtime": 8.7199,
      "eval_samples_per_second": 3489.84,
      "eval_steps_per_second": 13.647,
      "step": 26400
    },
    {
      "epoch": 60.02272727272727,
      "grad_norm": 0.20963233709335327,
      "learning_rate": 0.00040133067297421296,
      "loss": 2.2051,
      "step": 26410
    },
    {
      "epoch": 60.04545454545455,
      "grad_norm": 0.4667654037475586,
      "learning_rate": 0.00040125890448555534,
      "loss": 2.2018,
      "step": 26420
    },
    {
      "epoch": 60.06818181818182,
      "grad_norm": 0.14791037142276764,
      "learning_rate": 0.00040118711632777663,
      "loss": 2.2064,
      "step": 26430
    },
    {
      "epoch": 60.09090909090909,
      "grad_norm": 0.1436278223991394,
      "learning_rate": 0.0004011153085102116,
      "loss": 2.1994,
      "step": 26440
    },
    {
      "epoch": 60.11363636363637,
      "grad_norm": 0.17097222805023193,
      "learning_rate": 0.000401043481042198,
      "loss": 2.2077,
      "step": 26450
    },
    {
      "epoch": 60.13636363636363,
      "grad_norm": 0.2735685706138611,
      "learning_rate": 0.000400971633933076,
      "loss": 2.1998,
      "step": 26460
    },
    {
      "epoch": 60.15909090909091,
      "grad_norm": 0.2563171684741974,
      "learning_rate": 0.0004008997671921882,
      "loss": 2.2046,
      "step": 26470
    },
    {
      "epoch": 60.18181818181818,
      "grad_norm": 0.16161932051181793,
      "learning_rate": 0.00040082788082888,
      "loss": 2.198,
      "step": 26480
    },
    {
      "epoch": 60.20454545454545,
      "grad_norm": 0.14071065187454224,
      "learning_rate": 0.0004007559748524992,
      "loss": 2.198,
      "step": 26490
    },
    {
      "epoch": 60.22727272727273,
      "grad_norm": 0.25662538409233093,
      "learning_rate": 0.00040068404927239617,
      "loss": 2.1972,
      "step": 26500
    },
    {
      "epoch": 60.25,
      "grad_norm": 0.26061776280403137,
      "learning_rate": 0.0004006121040979238,
      "loss": 2.202,
      "step": 26510
    },
    {
      "epoch": 60.27272727272727,
      "grad_norm": 0.43839195370674133,
      "learning_rate": 0.00040054013933843755,
      "loss": 2.2127,
      "step": 26520
    },
    {
      "epoch": 60.29545454545455,
      "grad_norm": 0.14686180651187897,
      "learning_rate": 0.00040046815500329553,
      "loss": 2.2038,
      "step": 26530
    },
    {
      "epoch": 60.31818181818182,
      "grad_norm": 0.4110797047615051,
      "learning_rate": 0.00040039615110185815,
      "loss": 2.2001,
      "step": 26540
    },
    {
      "epoch": 60.34090909090909,
      "grad_norm": 0.23194004595279694,
      "learning_rate": 0.0004003241276434887,
      "loss": 2.1957,
      "step": 26550
    },
    {
      "epoch": 60.36363636363637,
      "grad_norm": 0.15893495082855225,
      "learning_rate": 0.00040025208463755275,
      "loss": 2.2008,
      "step": 26560
    },
    {
      "epoch": 60.38636363636363,
      "grad_norm": 0.16589833796024323,
      "learning_rate": 0.00040018002209341844,
      "loss": 2.1966,
      "step": 26570
    },
    {
      "epoch": 60.40909090909091,
      "grad_norm": 0.15297654271125793,
      "learning_rate": 0.00040010794002045657,
      "loss": 2.2008,
      "step": 26580
    },
    {
      "epoch": 60.43181818181818,
      "grad_norm": 0.21948662400245667,
      "learning_rate": 0.00040003583842804035,
      "loss": 2.2056,
      "step": 26590
    },
    {
      "epoch": 60.45454545454545,
      "grad_norm": 0.22771412134170532,
      "learning_rate": 0.00039996371732554573,
      "loss": 2.2034,
      "step": 26600
    },
    {
      "epoch": 60.47727272727273,
      "grad_norm": 0.19804677367210388,
      "learning_rate": 0.0003998915767223509,
      "loss": 2.2078,
      "step": 26610
    },
    {
      "epoch": 60.5,
      "grad_norm": 0.19517746567726135,
      "learning_rate": 0.0003998194166278367,
      "loss": 2.2114,
      "step": 26620
    },
    {
      "epoch": 60.52272727272727,
      "grad_norm": 0.226649209856987,
      "learning_rate": 0.0003997472370513868,
      "loss": 2.1906,
      "step": 26630
    },
    {
      "epoch": 60.54545454545455,
      "grad_norm": 0.11538825184106827,
      "learning_rate": 0.000399675038002387,
      "loss": 2.1971,
      "step": 26640
    },
    {
      "epoch": 60.56818181818182,
      "grad_norm": 0.1406150758266449,
      "learning_rate": 0.0003996028194902257,
      "loss": 2.1961,
      "step": 26650
    },
    {
      "epoch": 60.59090909090909,
      "grad_norm": 0.3766431212425232,
      "learning_rate": 0.0003995305815242941,
      "loss": 2.2086,
      "step": 26660
    },
    {
      "epoch": 60.61363636363637,
      "grad_norm": 0.4573870897293091,
      "learning_rate": 0.0003994583241139856,
      "loss": 2.2062,
      "step": 26670
    },
    {
      "epoch": 60.63636363636363,
      "grad_norm": 0.4288577735424042,
      "learning_rate": 0.0003993860472686964,
      "loss": 2.1997,
      "step": 26680
    },
    {
      "epoch": 60.65909090909091,
      "grad_norm": 0.3725046217441559,
      "learning_rate": 0.00039931375099782495,
      "loss": 2.2054,
      "step": 26690
    },
    {
      "epoch": 60.68181818181818,
      "grad_norm": 0.28647974133491516,
      "learning_rate": 0.0003992414353107724,
      "loss": 2.2025,
      "step": 26700
    },
    {
      "epoch": 60.70454545454545,
      "grad_norm": 0.20334772765636444,
      "learning_rate": 0.0003991691002169426,
      "loss": 2.1958,
      "step": 26710
    },
    {
      "epoch": 60.72727272727273,
      "grad_norm": 0.363778293132782,
      "learning_rate": 0.00039909674572574145,
      "loss": 2.2071,
      "step": 26720
    },
    {
      "epoch": 60.75,
      "grad_norm": 0.28572767972946167,
      "learning_rate": 0.0003990243718465779,
      "loss": 2.2024,
      "step": 26730
    },
    {
      "epoch": 60.77272727272727,
      "grad_norm": 0.18927660584449768,
      "learning_rate": 0.0003989519785888629,
      "loss": 2.2027,
      "step": 26740
    },
    {
      "epoch": 60.79545454545455,
      "grad_norm": 0.2399037480354309,
      "learning_rate": 0.0003988795659620105,
      "loss": 2.2144,
      "step": 26750
    },
    {
      "epoch": 60.81818181818182,
      "grad_norm": 0.2362537682056427,
      "learning_rate": 0.00039880713397543664,
      "loss": 2.206,
      "step": 26760
    },
    {
      "epoch": 60.84090909090909,
      "grad_norm": 0.12503798305988312,
      "learning_rate": 0.0003987346826385603,
      "loss": 2.1993,
      "step": 26770
    },
    {
      "epoch": 60.86363636363637,
      "grad_norm": 0.1327834278345108,
      "learning_rate": 0.00039866221196080274,
      "loss": 2.2033,
      "step": 26780
    },
    {
      "epoch": 60.88636363636363,
      "grad_norm": 0.2954195439815521,
      "learning_rate": 0.00039858972195158773,
      "loss": 2.2066,
      "step": 26790
    },
    {
      "epoch": 60.90909090909091,
      "grad_norm": 0.4069357216358185,
      "learning_rate": 0.00039851721262034156,
      "loss": 2.2109,
      "step": 26800
    },
    {
      "epoch": 60.93181818181818,
      "grad_norm": 0.1823408454656601,
      "learning_rate": 0.00039844468397649314,
      "loss": 2.2109,
      "step": 26810
    },
    {
      "epoch": 60.95454545454545,
      "grad_norm": 0.1482345163822174,
      "learning_rate": 0.00039837213602947374,
      "loss": 2.191,
      "step": 26820
    },
    {
      "epoch": 60.97727272727273,
      "grad_norm": 0.13330024480819702,
      "learning_rate": 0.0003982995687887173,
      "loss": 2.2035,
      "step": 26830
    },
    {
      "epoch": 61.0,
      "grad_norm": 0.2522723078727722,
      "learning_rate": 0.00039822698226366017,
      "loss": 2.2074,
      "step": 26840
    },
    {
      "epoch": 61.0,
      "eval_loss": 1.0999057292938232,
      "eval_runtime": 8.7247,
      "eval_samples_per_second": 3487.915,
      "eval_steps_per_second": 13.639,
      "step": 26840
    },
    {
      "epoch": 61.02272727272727,
      "grad_norm": 0.19282777607440948,
      "learning_rate": 0.0003981543764637411,
      "loss": 2.2057,
      "step": 26850
    },
    {
      "epoch": 61.04545454545455,
      "grad_norm": 0.9537818431854248,
      "learning_rate": 0.00039808175139840156,
      "loss": 2.1901,
      "step": 26860
    },
    {
      "epoch": 61.06818181818182,
      "grad_norm": 0.1876264214515686,
      "learning_rate": 0.00039800910707708543,
      "loss": 2.1964,
      "step": 26870
    },
    {
      "epoch": 61.09090909090909,
      "grad_norm": 0.16152538359165192,
      "learning_rate": 0.00039793644350923915,
      "loss": 2.202,
      "step": 26880
    },
    {
      "epoch": 61.11363636363637,
      "grad_norm": 0.12395540624856949,
      "learning_rate": 0.00039786376070431154,
      "loss": 2.2073,
      "step": 26890
    },
    {
      "epoch": 61.13636363636363,
      "grad_norm": 0.19789668917655945,
      "learning_rate": 0.0003977910586717539,
      "loss": 2.1949,
      "step": 26900
    },
    {
      "epoch": 61.15909090909091,
      "grad_norm": 0.1804300844669342,
      "learning_rate": 0.00039771833742102025,
      "loss": 2.1906,
      "step": 26910
    },
    {
      "epoch": 61.18181818181818,
      "grad_norm": 0.3101966679096222,
      "learning_rate": 0.000397645596961567,
      "loss": 2.1976,
      "step": 26920
    },
    {
      "epoch": 61.20454545454545,
      "grad_norm": 0.1923058182001114,
      "learning_rate": 0.00039757283730285293,
      "loss": 2.204,
      "step": 26930
    },
    {
      "epoch": 61.22727272727273,
      "grad_norm": 0.18815185129642487,
      "learning_rate": 0.0003975000584543395,
      "loss": 2.2045,
      "step": 26940
    },
    {
      "epoch": 61.25,
      "grad_norm": 0.1638789176940918,
      "learning_rate": 0.00039742726042549053,
      "loss": 2.1956,
      "step": 26950
    },
    {
      "epoch": 61.27272727272727,
      "grad_norm": 0.1684754192829132,
      "learning_rate": 0.00039735444322577254,
      "loss": 2.2026,
      "step": 26960
    },
    {
      "epoch": 61.29545454545455,
      "grad_norm": 0.22884945571422577,
      "learning_rate": 0.0003972816068646541,
      "loss": 2.207,
      "step": 26970
    },
    {
      "epoch": 61.31818181818182,
      "grad_norm": 0.21861357986927032,
      "learning_rate": 0.0003972087513516069,
      "loss": 2.2064,
      "step": 26980
    },
    {
      "epoch": 61.34090909090909,
      "grad_norm": 0.23188893496990204,
      "learning_rate": 0.0003971358766961046,
      "loss": 2.2048,
      "step": 26990
    },
    {
      "epoch": 61.36363636363637,
      "grad_norm": 0.23517996072769165,
      "learning_rate": 0.00039706298290762357,
      "loss": 2.2056,
      "step": 27000
    },
    {
      "epoch": 61.38636363636363,
      "grad_norm": 0.3569212853908539,
      "learning_rate": 0.0003969900699956427,
      "loss": 2.2028,
      "step": 27010
    },
    {
      "epoch": 61.40909090909091,
      "grad_norm": 0.22843576967716217,
      "learning_rate": 0.00039691713796964314,
      "loss": 2.2109,
      "step": 27020
    },
    {
      "epoch": 61.43181818181818,
      "grad_norm": 0.15646810829639435,
      "learning_rate": 0.0003968441868391089,
      "loss": 2.1989,
      "step": 27030
    },
    {
      "epoch": 61.45454545454545,
      "grad_norm": 0.23021405935287476,
      "learning_rate": 0.00039677121661352613,
      "loss": 2.2087,
      "step": 27040
    },
    {
      "epoch": 61.47727272727273,
      "grad_norm": 0.26776009798049927,
      "learning_rate": 0.0003966982273023836,
      "loss": 2.1951,
      "step": 27050
    },
    {
      "epoch": 61.5,
      "grad_norm": 0.5463770031929016,
      "learning_rate": 0.00039662521891517256,
      "loss": 2.1953,
      "step": 27060
    },
    {
      "epoch": 61.52272727272727,
      "grad_norm": 0.3953760862350464,
      "learning_rate": 0.0003965521914613868,
      "loss": 2.2035,
      "step": 27070
    },
    {
      "epoch": 61.54545454545455,
      "grad_norm": 0.22870005667209625,
      "learning_rate": 0.0003964791449505225,
      "loss": 2.201,
      "step": 27080
    },
    {
      "epoch": 61.56818181818182,
      "grad_norm": 0.20092229545116425,
      "learning_rate": 0.0003964060793920783,
      "loss": 2.1992,
      "step": 27090
    },
    {
      "epoch": 61.59090909090909,
      "grad_norm": 0.15847647190093994,
      "learning_rate": 0.00039633299479555534,
      "loss": 2.1999,
      "step": 27100
    },
    {
      "epoch": 61.61363636363637,
      "grad_norm": 0.2867043912410736,
      "learning_rate": 0.00039625989117045735,
      "loss": 2.2014,
      "step": 27110
    },
    {
      "epoch": 61.63636363636363,
      "grad_norm": 0.2150101214647293,
      "learning_rate": 0.00039618676852629044,
      "loss": 2.2078,
      "step": 27120
    },
    {
      "epoch": 61.65909090909091,
      "grad_norm": 0.12649832665920258,
      "learning_rate": 0.0003961136268725631,
      "loss": 2.1941,
      "step": 27130
    },
    {
      "epoch": 61.68181818181818,
      "grad_norm": 0.18457376956939697,
      "learning_rate": 0.0003960404662187864,
      "loss": 2.2053,
      "step": 27140
    },
    {
      "epoch": 61.70454545454545,
      "grad_norm": 0.13901950418949127,
      "learning_rate": 0.000395967286574474,
      "loss": 2.2071,
      "step": 27150
    },
    {
      "epoch": 61.72727272727273,
      "grad_norm": 0.2995755076408386,
      "learning_rate": 0.0003958940879491418,
      "loss": 2.2108,
      "step": 27160
    },
    {
      "epoch": 61.75,
      "grad_norm": 0.23415492475032806,
      "learning_rate": 0.0003958208703523083,
      "loss": 2.2,
      "step": 27170
    },
    {
      "epoch": 61.77272727272727,
      "grad_norm": 0.2026137411594391,
      "learning_rate": 0.0003957476337934943,
      "loss": 2.2122,
      "step": 27180
    },
    {
      "epoch": 61.79545454545455,
      "grad_norm": 0.18478918075561523,
      "learning_rate": 0.00039567437828222336,
      "loss": 2.2044,
      "step": 27190
    },
    {
      "epoch": 61.81818181818182,
      "grad_norm": 0.3567301630973816,
      "learning_rate": 0.0003956011038280213,
      "loss": 2.2015,
      "step": 27200
    },
    {
      "epoch": 61.84090909090909,
      "grad_norm": 0.18210583925247192,
      "learning_rate": 0.0003955278104404164,
      "loss": 2.2108,
      "step": 27210
    },
    {
      "epoch": 61.86363636363637,
      "grad_norm": 0.21601824462413788,
      "learning_rate": 0.0003954544981289395,
      "loss": 2.2026,
      "step": 27220
    },
    {
      "epoch": 61.88636363636363,
      "grad_norm": 0.24198664724826813,
      "learning_rate": 0.00039538116690312374,
      "loss": 2.2011,
      "step": 27230
    },
    {
      "epoch": 61.90909090909091,
      "grad_norm": 0.2002963423728943,
      "learning_rate": 0.00039530781677250506,
      "loss": 2.1991,
      "step": 27240
    },
    {
      "epoch": 61.93181818181818,
      "grad_norm": 0.35113292932510376,
      "learning_rate": 0.00039523444774662136,
      "loss": 2.207,
      "step": 27250
    },
    {
      "epoch": 61.95454545454545,
      "grad_norm": 0.16959793865680695,
      "learning_rate": 0.0003951610598350135,
      "loss": 2.1968,
      "step": 27260
    },
    {
      "epoch": 61.97727272727273,
      "grad_norm": 0.3219803273677826,
      "learning_rate": 0.0003950876530472244,
      "loss": 2.2039,
      "step": 27270
    },
    {
      "epoch": 62.0,
      "grad_norm": 0.26996371150016785,
      "learning_rate": 0.0003950142273927996,
      "loss": 2.1898,
      "step": 27280
    },
    {
      "epoch": 62.0,
      "eval_loss": 1.0999417304992676,
      "eval_runtime": 8.7132,
      "eval_samples_per_second": 3492.523,
      "eval_steps_per_second": 13.657,
      "step": 27280
    },
    {
      "epoch": 62.02272727272727,
      "grad_norm": 0.2635307013988495,
      "learning_rate": 0.00039494078288128707,
      "loss": 2.1969,
      "step": 27290
    },
    {
      "epoch": 62.04545454545455,
      "grad_norm": 0.3995952904224396,
      "learning_rate": 0.0003948673195222374,
      "loss": 2.2035,
      "step": 27300
    },
    {
      "epoch": 62.06818181818182,
      "grad_norm": 0.3031617999076843,
      "learning_rate": 0.00039479383732520337,
      "loss": 2.2024,
      "step": 27310
    },
    {
      "epoch": 62.09090909090909,
      "grad_norm": 0.36788028478622437,
      "learning_rate": 0.0003947203362997404,
      "loss": 2.2006,
      "step": 27320
    },
    {
      "epoch": 62.11363636363637,
      "grad_norm": 0.3737037777900696,
      "learning_rate": 0.0003946468164554061,
      "loss": 2.1972,
      "step": 27330
    },
    {
      "epoch": 62.13636363636363,
      "grad_norm": 0.4597119688987732,
      "learning_rate": 0.0003945732778017609,
      "loss": 2.1913,
      "step": 27340
    },
    {
      "epoch": 62.15909090909091,
      "grad_norm": 0.32673510909080505,
      "learning_rate": 0.00039449972034836736,
      "loss": 2.1861,
      "step": 27350
    },
    {
      "epoch": 62.18181818181818,
      "grad_norm": 0.19063438475131989,
      "learning_rate": 0.0003944261441047906,
      "loss": 2.1987,
      "step": 27360
    },
    {
      "epoch": 62.20454545454545,
      "grad_norm": 0.21529610455036163,
      "learning_rate": 0.0003943525490805983,
      "loss": 2.2062,
      "step": 27370
    },
    {
      "epoch": 62.22727272727273,
      "grad_norm": 0.21071425080299377,
      "learning_rate": 0.0003942789352853604,
      "loss": 2.1993,
      "step": 27380
    },
    {
      "epoch": 62.25,
      "grad_norm": 0.2230391949415207,
      "learning_rate": 0.00039420530272864934,
      "loss": 2.2084,
      "step": 27390
    },
    {
      "epoch": 62.27272727272727,
      "grad_norm": 0.3289017081260681,
      "learning_rate": 0.00039413165142004,
      "loss": 2.1997,
      "step": 27400
    },
    {
      "epoch": 62.29545454545455,
      "grad_norm": 0.21395596861839294,
      "learning_rate": 0.0003940579813691098,
      "loss": 2.2014,
      "step": 27410
    },
    {
      "epoch": 62.31818181818182,
      "grad_norm": 0.16809451580047607,
      "learning_rate": 0.00039398429258543846,
      "loss": 2.1997,
      "step": 27420
    },
    {
      "epoch": 62.34090909090909,
      "grad_norm": 0.282687783241272,
      "learning_rate": 0.0003939105850786081,
      "loss": 2.1961,
      "step": 27430
    },
    {
      "epoch": 62.36363636363637,
      "grad_norm": 0.18449844419956207,
      "learning_rate": 0.0003938368588582035,
      "loss": 2.2017,
      "step": 27440
    },
    {
      "epoch": 62.38636363636363,
      "grad_norm": 0.6211410760879517,
      "learning_rate": 0.00039376311393381165,
      "loss": 2.2168,
      "step": 27450
    },
    {
      "epoch": 62.40909090909091,
      "grad_norm": 0.34304600954055786,
      "learning_rate": 0.00039368935031502207,
      "loss": 2.2091,
      "step": 27460
    },
    {
      "epoch": 62.43181818181818,
      "grad_norm": 0.1736133098602295,
      "learning_rate": 0.00039361556801142673,
      "loss": 2.1935,
      "step": 27470
    },
    {
      "epoch": 62.45454545454545,
      "grad_norm": 0.6376873254776001,
      "learning_rate": 0.0003935417670326199,
      "loss": 2.206,
      "step": 27480
    },
    {
      "epoch": 62.47727272727273,
      "grad_norm": 0.4142962396144867,
      "learning_rate": 0.0003934679473881985,
      "loss": 2.1988,
      "step": 27490
    },
    {
      "epoch": 62.5,
      "grad_norm": 0.22621259093284607,
      "learning_rate": 0.00039339410908776154,
      "loss": 2.2022,
      "step": 27500
    },
    {
      "epoch": 62.52272727272727,
      "grad_norm": 0.6956714391708374,
      "learning_rate": 0.00039332025214091086,
      "loss": 2.2007,
      "step": 27510
    },
    {
      "epoch": 62.54545454545455,
      "grad_norm": 0.1616658717393875,
      "learning_rate": 0.00039324637655725053,
      "loss": 2.1958,
      "step": 27520
    },
    {
      "epoch": 62.56818181818182,
      "grad_norm": 0.162434384226799,
      "learning_rate": 0.0003931724823463869,
      "loss": 2.2,
      "step": 27530
    },
    {
      "epoch": 62.59090909090909,
      "grad_norm": 0.146663099527359,
      "learning_rate": 0.0003930985695179291,
      "loss": 2.2025,
      "step": 27540
    },
    {
      "epoch": 62.61363636363637,
      "grad_norm": 0.22538547217845917,
      "learning_rate": 0.00039302463808148826,
      "loss": 2.202,
      "step": 27550
    },
    {
      "epoch": 62.63636363636363,
      "grad_norm": 0.19715562462806702,
      "learning_rate": 0.00039295068804667826,
      "loss": 2.2022,
      "step": 27560
    },
    {
      "epoch": 62.65909090909091,
      "grad_norm": 0.16408613324165344,
      "learning_rate": 0.0003928767194231153,
      "loss": 2.2095,
      "step": 27570
    },
    {
      "epoch": 62.68181818181818,
      "grad_norm": 0.2856980860233307,
      "learning_rate": 0.0003928027322204178,
      "loss": 2.1975,
      "step": 27580
    },
    {
      "epoch": 62.70454545454545,
      "grad_norm": 0.24768505990505219,
      "learning_rate": 0.0003927287264482069,
      "loss": 2.2026,
      "step": 27590
    },
    {
      "epoch": 62.72727272727273,
      "grad_norm": 0.4037618637084961,
      "learning_rate": 0.00039265470211610607,
      "loss": 2.1906,
      "step": 27600
    },
    {
      "epoch": 62.75,
      "grad_norm": 0.2083139568567276,
      "learning_rate": 0.000392580659233741,
      "loss": 2.1966,
      "step": 27610
    },
    {
      "epoch": 62.77272727272727,
      "grad_norm": 0.16381698846817017,
      "learning_rate": 0.00039250659781074013,
      "loss": 2.2057,
      "step": 27620
    },
    {
      "epoch": 62.79545454545455,
      "grad_norm": 0.19615735113620758,
      "learning_rate": 0.0003924325178567339,
      "loss": 2.2015,
      "step": 27630
    },
    {
      "epoch": 62.81818181818182,
      "grad_norm": 0.5171034932136536,
      "learning_rate": 0.00039235841938135553,
      "loss": 2.1997,
      "step": 27640
    },
    {
      "epoch": 62.84090909090909,
      "grad_norm": 0.1792650818824768,
      "learning_rate": 0.0003922843023942405,
      "loss": 2.1981,
      "step": 27650
    },
    {
      "epoch": 62.86363636363637,
      "grad_norm": 0.330652117729187,
      "learning_rate": 0.00039221016690502657,
      "loss": 2.2208,
      "step": 27660
    },
    {
      "epoch": 62.88636363636363,
      "grad_norm": 0.19406823813915253,
      "learning_rate": 0.0003921360129233542,
      "loss": 2.1955,
      "step": 27670
    },
    {
      "epoch": 62.90909090909091,
      "grad_norm": 0.3037331998348236,
      "learning_rate": 0.0003920618404588659,
      "loss": 2.2119,
      "step": 27680
    },
    {
      "epoch": 62.93181818181818,
      "grad_norm": 0.3195663392543793,
      "learning_rate": 0.0003919876495212069,
      "loss": 2.2073,
      "step": 27690
    },
    {
      "epoch": 62.95454545454545,
      "grad_norm": 0.24182522296905518,
      "learning_rate": 0.00039191344012002476,
      "loss": 2.1996,
      "step": 27700
    },
    {
      "epoch": 62.97727272727273,
      "grad_norm": 0.3337334990501404,
      "learning_rate": 0.0003918392122649692,
      "loss": 2.2031,
      "step": 27710
    },
    {
      "epoch": 63.0,
      "grad_norm": 0.21115034818649292,
      "learning_rate": 0.00039176496596569265,
      "loss": 2.2284,
      "step": 27720
    },
    {
      "epoch": 63.0,
      "eval_loss": 1.100253939628601,
      "eval_runtime": 8.7323,
      "eval_samples_per_second": 3484.86,
      "eval_steps_per_second": 13.627,
      "step": 27720
    },
    {
      "epoch": 63.02272727272727,
      "grad_norm": 0.4005582928657532,
      "learning_rate": 0.0003916907012318498,
      "loss": 2.1983,
      "step": 27730
    },
    {
      "epoch": 63.04545454545455,
      "grad_norm": 0.274345338344574,
      "learning_rate": 0.0003916164180730978,
      "loss": 2.1964,
      "step": 27740
    },
    {
      "epoch": 63.06818181818182,
      "grad_norm": 0.20680153369903564,
      "learning_rate": 0.000391542116499096,
      "loss": 2.1971,
      "step": 27750
    },
    {
      "epoch": 63.09090909090909,
      "grad_norm": 0.20366960763931274,
      "learning_rate": 0.0003914677965195063,
      "loss": 2.1932,
      "step": 27760
    },
    {
      "epoch": 63.11363636363637,
      "grad_norm": 0.23952749371528625,
      "learning_rate": 0.0003913934581439931,
      "loss": 2.1945,
      "step": 27770
    },
    {
      "epoch": 63.13636363636363,
      "grad_norm": 0.2601188123226166,
      "learning_rate": 0.000391319101382223,
      "loss": 2.199,
      "step": 27780
    },
    {
      "epoch": 63.15909090909091,
      "grad_norm": 0.28662917017936707,
      "learning_rate": 0.0003912447262438651,
      "loss": 2.1989,
      "step": 27790
    },
    {
      "epoch": 63.18181818181818,
      "grad_norm": 0.22525744140148163,
      "learning_rate": 0.00039117033273859084,
      "loss": 2.212,
      "step": 27800
    },
    {
      "epoch": 63.20454545454545,
      "grad_norm": 0.30363598465919495,
      "learning_rate": 0.0003910959208760739,
      "loss": 2.2001,
      "step": 27810
    },
    {
      "epoch": 63.22727272727273,
      "grad_norm": 0.894230306148529,
      "learning_rate": 0.00039102149066599093,
      "loss": 2.1912,
      "step": 27820
    },
    {
      "epoch": 63.25,
      "grad_norm": 0.17034222185611725,
      "learning_rate": 0.0003909470421180201,
      "loss": 2.2113,
      "step": 27830
    },
    {
      "epoch": 63.27272727272727,
      "grad_norm": 0.3045692443847656,
      "learning_rate": 0.0003908725752418426,
      "loss": 2.1978,
      "step": 27840
    },
    {
      "epoch": 63.29545454545455,
      "grad_norm": 0.8136091828346252,
      "learning_rate": 0.000390798090047142,
      "loss": 2.186,
      "step": 27850
    },
    {
      "epoch": 63.31818181818182,
      "grad_norm": 0.5087809562683105,
      "learning_rate": 0.0003907235865436036,
      "loss": 2.2035,
      "step": 27860
    },
    {
      "epoch": 63.34090909090909,
      "grad_norm": 0.20025528967380524,
      "learning_rate": 0.000390649064740916,
      "loss": 2.1855,
      "step": 27870
    },
    {
      "epoch": 63.36363636363637,
      "grad_norm": 0.17832337319850922,
      "learning_rate": 0.00039057452464876945,
      "loss": 2.202,
      "step": 27880
    },
    {
      "epoch": 63.38636363636363,
      "grad_norm": 0.23783043026924133,
      "learning_rate": 0.000390499966276857,
      "loss": 2.1914,
      "step": 27890
    },
    {
      "epoch": 63.40909090909091,
      "grad_norm": 0.1764523684978485,
      "learning_rate": 0.00039042538963487397,
      "loss": 2.2047,
      "step": 27900
    },
    {
      "epoch": 63.43181818181818,
      "grad_norm": 0.21496926248073578,
      "learning_rate": 0.0003903507947325178,
      "loss": 2.2053,
      "step": 27910
    },
    {
      "epoch": 63.45454545454545,
      "grad_norm": 0.17147694528102875,
      "learning_rate": 0.0003902761815794887,
      "loss": 2.2102,
      "step": 27920
    },
    {
      "epoch": 63.47727272727273,
      "grad_norm": 0.5807805061340332,
      "learning_rate": 0.000390201550185489,
      "loss": 2.1945,
      "step": 27930
    },
    {
      "epoch": 63.5,
      "grad_norm": 0.1273214966058731,
      "learning_rate": 0.0003901269005602235,
      "loss": 2.2057,
      "step": 27940
    },
    {
      "epoch": 63.52272727272727,
      "grad_norm": 0.16585813462734222,
      "learning_rate": 0.0003900522327133994,
      "loss": 2.1967,
      "step": 27950
    },
    {
      "epoch": 63.54545454545455,
      "grad_norm": 0.6270330548286438,
      "learning_rate": 0.0003899775466547261,
      "loss": 2.2067,
      "step": 27960
    },
    {
      "epoch": 63.56818181818182,
      "grad_norm": 0.30065351724624634,
      "learning_rate": 0.0003899028423939156,
      "loss": 2.202,
      "step": 27970
    },
    {
      "epoch": 63.59090909090909,
      "grad_norm": 0.16419954597949982,
      "learning_rate": 0.0003898281199406821,
      "loss": 2.2112,
      "step": 27980
    },
    {
      "epoch": 63.61363636363637,
      "grad_norm": 0.3180878460407257,
      "learning_rate": 0.0003897533793047422,
      "loss": 2.1998,
      "step": 27990
    },
    {
      "epoch": 63.63636363636363,
      "grad_norm": 0.1807989627122879,
      "learning_rate": 0.00038967862049581496,
      "loss": 2.2024,
      "step": 28000
    },
    {
      "epoch": 63.65909090909091,
      "grad_norm": 0.6537839770317078,
      "learning_rate": 0.00038960384352362164,
      "loss": 2.2111,
      "step": 28010
    },
    {
      "epoch": 63.68181818181818,
      "grad_norm": 0.35967817902565,
      "learning_rate": 0.000389529048397886,
      "loss": 2.2084,
      "step": 28020
    },
    {
      "epoch": 63.70454545454545,
      "grad_norm": 0.21076038479804993,
      "learning_rate": 0.0003894542351283341,
      "loss": 2.2068,
      "step": 28030
    },
    {
      "epoch": 63.72727272727273,
      "grad_norm": 0.5007050037384033,
      "learning_rate": 0.0003893794037246943,
      "loss": 2.21,
      "step": 28040
    },
    {
      "epoch": 63.75,
      "grad_norm": 0.23373328149318695,
      "learning_rate": 0.00038930455419669747,
      "loss": 2.2045,
      "step": 28050
    },
    {
      "epoch": 63.77272727272727,
      "grad_norm": 0.3647395074367523,
      "learning_rate": 0.0003892296865540767,
      "loss": 2.2003,
      "step": 28060
    },
    {
      "epoch": 63.79545454545455,
      "grad_norm": 0.2713280916213989,
      "learning_rate": 0.0003891548008065675,
      "loss": 2.2065,
      "step": 28070
    },
    {
      "epoch": 63.81818181818182,
      "grad_norm": 0.13887357711791992,
      "learning_rate": 0.0003890798969639078,
      "loss": 2.2023,
      "step": 28080
    },
    {
      "epoch": 63.84090909090909,
      "grad_norm": 0.2886304259300232,
      "learning_rate": 0.00038900497503583764,
      "loss": 2.2125,
      "step": 28090
    },
    {
      "epoch": 63.86363636363637,
      "grad_norm": 0.14842462539672852,
      "learning_rate": 0.0003889300350320998,
      "loss": 2.2,
      "step": 28100
    },
    {
      "epoch": 63.88636363636363,
      "grad_norm": 0.258015900850296,
      "learning_rate": 0.00038885507696243893,
      "loss": 2.204,
      "step": 28110
    },
    {
      "epoch": 63.90909090909091,
      "grad_norm": 0.7276510000228882,
      "learning_rate": 0.00038878010083660255,
      "loss": 2.2047,
      "step": 28120
    },
    {
      "epoch": 63.93181818181818,
      "grad_norm": 0.1820095181465149,
      "learning_rate": 0.00038870510666434005,
      "loss": 2.1953,
      "step": 28130
    },
    {
      "epoch": 63.95454545454545,
      "grad_norm": 0.2170279324054718,
      "learning_rate": 0.0003886300944554034,
      "loss": 2.2073,
      "step": 28140
    },
    {
      "epoch": 63.97727272727273,
      "grad_norm": 0.4561828672885895,
      "learning_rate": 0.0003885550642195471,
      "loss": 2.2043,
      "step": 28150
    },
    {
      "epoch": 64.0,
      "grad_norm": 0.2473207712173462,
      "learning_rate": 0.0003884800159665276,
      "loss": 2.2065,
      "step": 28160
    },
    {
      "epoch": 64.0,
      "eval_loss": 1.1002048254013062,
      "eval_runtime": 8.897,
      "eval_samples_per_second": 3420.355,
      "eval_steps_per_second": 13.375,
      "step": 28160
    },
    {
      "epoch": 64.02272727272727,
      "grad_norm": 0.24005359411239624,
      "learning_rate": 0.0003884049497061039,
      "loss": 2.1981,
      "step": 28170
    },
    {
      "epoch": 64.04545454545455,
      "grad_norm": 0.1584748774766922,
      "learning_rate": 0.0003883298654480374,
      "loss": 2.2066,
      "step": 28180
    },
    {
      "epoch": 64.06818181818181,
      "grad_norm": 0.16900721192359924,
      "learning_rate": 0.00038825476320209173,
      "loss": 2.2003,
      "step": 28190
    },
    {
      "epoch": 64.0909090909091,
      "grad_norm": 0.12759050726890564,
      "learning_rate": 0.0003881796429780329,
      "loss": 2.2017,
      "step": 28200
    },
    {
      "epoch": 64.11363636363636,
      "grad_norm": 0.3078545331954956,
      "learning_rate": 0.0003881045047856292,
      "loss": 2.202,
      "step": 28210
    },
    {
      "epoch": 64.13636363636364,
      "grad_norm": 0.32133808732032776,
      "learning_rate": 0.00038802934863465133,
      "loss": 2.2073,
      "step": 28220
    },
    {
      "epoch": 64.1590909090909,
      "grad_norm": 1.3724396228790283,
      "learning_rate": 0.0003879541745348725,
      "loss": 2.2069,
      "step": 28230
    },
    {
      "epoch": 64.18181818181819,
      "grad_norm": 0.13717836141586304,
      "learning_rate": 0.00038787898249606766,
      "loss": 2.2046,
      "step": 28240
    },
    {
      "epoch": 64.20454545454545,
      "grad_norm": 0.416770339012146,
      "learning_rate": 0.00038780377252801485,
      "loss": 2.1924,
      "step": 28250
    },
    {
      "epoch": 64.22727272727273,
      "grad_norm": 2.10772705078125,
      "learning_rate": 0.0003877285446404939,
      "loss": 2.1938,
      "step": 28260
    },
    {
      "epoch": 64.25,
      "grad_norm": 0.19755592942237854,
      "learning_rate": 0.0003876532988432872,
      "loss": 2.2012,
      "step": 28270
    },
    {
      "epoch": 64.27272727272727,
      "grad_norm": 0.1660095900297165,
      "learning_rate": 0.0003875780351461795,
      "loss": 2.2067,
      "step": 28280
    },
    {
      "epoch": 64.29545454545455,
      "grad_norm": 0.25198444724082947,
      "learning_rate": 0.0003875027535589576,
      "loss": 2.1935,
      "step": 28290
    },
    {
      "epoch": 64.31818181818181,
      "grad_norm": 0.1440589725971222,
      "learning_rate": 0.00038742745409141104,
      "loss": 2.1997,
      "step": 28300
    },
    {
      "epoch": 64.3409090909091,
      "grad_norm": 0.2163085788488388,
      "learning_rate": 0.00038735213675333136,
      "loss": 2.2056,
      "step": 28310
    },
    {
      "epoch": 64.36363636363636,
      "grad_norm": 0.3378250300884247,
      "learning_rate": 0.0003872768015545126,
      "loss": 2.2032,
      "step": 28320
    },
    {
      "epoch": 64.38636363636364,
      "grad_norm": 0.43876293301582336,
      "learning_rate": 0.0003872014485047509,
      "loss": 2.1912,
      "step": 28330
    },
    {
      "epoch": 64.4090909090909,
      "grad_norm": 0.12637749314308167,
      "learning_rate": 0.00038712607761384504,
      "loss": 2.2036,
      "step": 28340
    },
    {
      "epoch": 64.43181818181819,
      "grad_norm": 0.27561140060424805,
      "learning_rate": 0.00038705068889159595,
      "loss": 2.2018,
      "step": 28350
    },
    {
      "epoch": 64.45454545454545,
      "grad_norm": 0.6555271744728088,
      "learning_rate": 0.00038697528234780677,
      "loss": 2.2033,
      "step": 28360
    },
    {
      "epoch": 64.47727272727273,
      "grad_norm": 0.6619555950164795,
      "learning_rate": 0.00038689985799228324,
      "loss": 2.2047,
      "step": 28370
    },
    {
      "epoch": 64.5,
      "grad_norm": 1.3370965719223022,
      "learning_rate": 0.0003868244158348331,
      "loss": 2.1901,
      "step": 28380
    },
    {
      "epoch": 64.52272727272727,
      "grad_norm": 0.30362004041671753,
      "learning_rate": 0.0003867489558852666,
      "loss": 2.2093,
      "step": 28390
    },
    {
      "epoch": 64.54545454545455,
      "grad_norm": 0.20232413709163666,
      "learning_rate": 0.00038667347815339636,
      "loss": 2.2015,
      "step": 28400
    },
    {
      "epoch": 64.56818181818181,
      "grad_norm": 0.4133921265602112,
      "learning_rate": 0.000386597982649037,
      "loss": 2.1988,
      "step": 28410
    },
    {
      "epoch": 64.5909090909091,
      "grad_norm": 0.2348792999982834,
      "learning_rate": 0.00038652246938200585,
      "loss": 2.1938,
      "step": 28420
    },
    {
      "epoch": 64.61363636363636,
      "grad_norm": 0.23382675647735596,
      "learning_rate": 0.00038644693836212236,
      "loss": 2.2144,
      "step": 28430
    },
    {
      "epoch": 64.63636363636364,
      "grad_norm": 0.21327099204063416,
      "learning_rate": 0.00038637138959920814,
      "loss": 2.2045,
      "step": 28440
    },
    {
      "epoch": 64.6590909090909,
      "grad_norm": 0.19203615188598633,
      "learning_rate": 0.0003862958231030874,
      "loss": 2.212,
      "step": 28450
    },
    {
      "epoch": 64.68181818181819,
      "grad_norm": 0.22979536652565002,
      "learning_rate": 0.00038622023888358647,
      "loss": 2.2109,
      "step": 28460
    },
    {
      "epoch": 64.70454545454545,
      "grad_norm": 0.1669263392686844,
      "learning_rate": 0.00038614463695053403,
      "loss": 2.1885,
      "step": 28470
    },
    {
      "epoch": 64.72727272727273,
      "grad_norm": 0.4016273617744446,
      "learning_rate": 0.00038606901731376105,
      "loss": 2.2024,
      "step": 28480
    },
    {
      "epoch": 64.75,
      "grad_norm": 0.16706524789333344,
      "learning_rate": 0.0003859933799831008,
      "loss": 2.1968,
      "step": 28490
    },
    {
      "epoch": 64.77272727272727,
      "grad_norm": 0.2820478081703186,
      "learning_rate": 0.0003859177249683889,
      "loss": 2.1946,
      "step": 28500
    },
    {
      "epoch": 64.79545454545455,
      "grad_norm": 0.2960872948169708,
      "learning_rate": 0.0003858420522794631,
      "loss": 2.2047,
      "step": 28510
    },
    {
      "epoch": 64.81818181818181,
      "grad_norm": 0.8077630996704102,
      "learning_rate": 0.0003857663619261639,
      "loss": 2.1907,
      "step": 28520
    },
    {
      "epoch": 64.8409090909091,
      "grad_norm": 0.3579515814781189,
      "learning_rate": 0.0003856906539183335,
      "loss": 2.192,
      "step": 28530
    },
    {
      "epoch": 64.86363636363636,
      "grad_norm": 0.31065690517425537,
      "learning_rate": 0.0003856149282658167,
      "loss": 2.2041,
      "step": 28540
    },
    {
      "epoch": 64.88636363636364,
      "grad_norm": 0.2023524045944214,
      "learning_rate": 0.00038553918497846067,
      "loss": 2.2019,
      "step": 28550
    },
    {
      "epoch": 64.9090909090909,
      "grad_norm": 0.2812522351741791,
      "learning_rate": 0.0003854634240661148,
      "loss": 2.2013,
      "step": 28560
    },
    {
      "epoch": 64.93181818181819,
      "grad_norm": 0.1688038557767868,
      "learning_rate": 0.0003853876455386306,
      "loss": 2.207,
      "step": 28570
    },
    {
      "epoch": 64.95454545454545,
      "grad_norm": 0.21363498270511627,
      "learning_rate": 0.00038531184940586205,
      "loss": 2.2024,
      "step": 28580
    },
    {
      "epoch": 64.97727272727273,
      "grad_norm": 0.16608649492263794,
      "learning_rate": 0.00038523603567766553,
      "loss": 2.2049,
      "step": 28590
    },
    {
      "epoch": 65.0,
      "grad_norm": 0.2988486886024475,
      "learning_rate": 0.0003851602043638994,
      "loss": 2.2036,
      "step": 28600
    },
    {
      "epoch": 65.0,
      "eval_loss": 1.0998157262802124,
      "eval_runtime": 8.729,
      "eval_samples_per_second": 3486.192,
      "eval_steps_per_second": 13.633,
      "step": 28600
    },
    {
      "epoch": 65.02272727272727,
      "grad_norm": 0.20480677485466003,
      "learning_rate": 0.00038508435547442453,
      "loss": 2.2091,
      "step": 28610
    },
    {
      "epoch": 65.04545454545455,
      "grad_norm": 0.21927443146705627,
      "learning_rate": 0.00038500848901910407,
      "loss": 2.1961,
      "step": 28620
    },
    {
      "epoch": 65.06818181818181,
      "grad_norm": 0.3805682361125946,
      "learning_rate": 0.0003849326050078033,
      "loss": 2.1898,
      "step": 28630
    },
    {
      "epoch": 65.0909090909091,
      "grad_norm": 2.363307237625122,
      "learning_rate": 0.00038485670345038994,
      "loss": 2.1883,
      "step": 28640
    },
    {
      "epoch": 65.11363636363636,
      "grad_norm": 0.26412907242774963,
      "learning_rate": 0.0003847807843567339,
      "loss": 2.1873,
      "step": 28650
    },
    {
      "epoch": 65.13636363636364,
      "grad_norm": 0.19152966141700745,
      "learning_rate": 0.0003847048477367074,
      "loss": 2.2029,
      "step": 28660
    },
    {
      "epoch": 65.1590909090909,
      "grad_norm": 0.24366548657417297,
      "learning_rate": 0.00038462889360018504,
      "loss": 2.1953,
      "step": 28670
    },
    {
      "epoch": 65.18181818181819,
      "grad_norm": 0.39550358057022095,
      "learning_rate": 0.0003845529219570435,
      "loss": 2.1863,
      "step": 28680
    },
    {
      "epoch": 65.20454545454545,
      "grad_norm": 0.22540785372257233,
      "learning_rate": 0.0003844769328171619,
      "loss": 2.1937,
      "step": 28690
    },
    {
      "epoch": 65.22727272727273,
      "grad_norm": 0.21873971819877625,
      "learning_rate": 0.00038440092619042147,
      "loss": 2.2028,
      "step": 28700
    },
    {
      "epoch": 65.25,
      "grad_norm": 0.2492017298936844,
      "learning_rate": 0.000384324902086706,
      "loss": 2.2073,
      "step": 28710
    },
    {
      "epoch": 65.27272727272727,
      "grad_norm": 0.1502695381641388,
      "learning_rate": 0.00038424886051590117,
      "loss": 2.2086,
      "step": 28720
    },
    {
      "epoch": 65.29545454545455,
      "grad_norm": 0.1652173101902008,
      "learning_rate": 0.00038417280148789527,
      "loss": 2.2067,
      "step": 28730
    },
    {
      "epoch": 65.31818181818181,
      "grad_norm": 0.2468457818031311,
      "learning_rate": 0.0003840967250125786,
      "loss": 2.1884,
      "step": 28740
    },
    {
      "epoch": 65.3409090909091,
      "grad_norm": 0.30279725790023804,
      "learning_rate": 0.000384020631099844,
      "loss": 2.2022,
      "step": 28750
    },
    {
      "epoch": 65.36363636363636,
      "grad_norm": 0.3788924515247345,
      "learning_rate": 0.00038394451975958633,
      "loss": 2.1991,
      "step": 28760
    },
    {
      "epoch": 65.38636363636364,
      "grad_norm": 0.3267499804496765,
      "learning_rate": 0.0003838683910017028,
      "loss": 2.2067,
      "step": 28770
    },
    {
      "epoch": 65.4090909090909,
      "grad_norm": 0.3297277092933655,
      "learning_rate": 0.00038379224483609285,
      "loss": 2.1978,
      "step": 28780
    },
    {
      "epoch": 65.43181818181819,
      "grad_norm": 0.17914874851703644,
      "learning_rate": 0.00038371608127265845,
      "loss": 2.197,
      "step": 28790
    },
    {
      "epoch": 65.45454545454545,
      "grad_norm": 0.16499072313308716,
      "learning_rate": 0.0003836399003213035,
      "loss": 2.2016,
      "step": 28800
    },
    {
      "epoch": 65.47727272727273,
      "grad_norm": 0.2069048434495926,
      "learning_rate": 0.00038356370199193416,
      "loss": 2.2072,
      "step": 28810
    },
    {
      "epoch": 65.5,
      "grad_norm": 0.21265409886837006,
      "learning_rate": 0.0003834874862944591,
      "loss": 2.1947,
      "step": 28820
    },
    {
      "epoch": 65.52272727272727,
      "grad_norm": 0.15660396218299866,
      "learning_rate": 0.000383411253238789,
      "loss": 2.2038,
      "step": 28830
    },
    {
      "epoch": 65.54545454545455,
      "grad_norm": 0.20582012832164764,
      "learning_rate": 0.000383335002834837,
      "loss": 2.1979,
      "step": 28840
    },
    {
      "epoch": 65.56818181818181,
      "grad_norm": 0.14711320400238037,
      "learning_rate": 0.00038325873509251847,
      "loss": 2.199,
      "step": 28850
    },
    {
      "epoch": 65.5909090909091,
      "grad_norm": 0.6872014999389648,
      "learning_rate": 0.00038318245002175085,
      "loss": 2.1905,
      "step": 28860
    },
    {
      "epoch": 65.61363636363636,
      "grad_norm": 0.20772847533226013,
      "learning_rate": 0.0003831061476324539,
      "loss": 2.2032,
      "step": 28870
    },
    {
      "epoch": 65.63636363636364,
      "grad_norm": 0.17807990312576294,
      "learning_rate": 0.00038302982793454987,
      "loss": 2.2061,
      "step": 28880
    },
    {
      "epoch": 65.6590909090909,
      "grad_norm": 0.14587026834487915,
      "learning_rate": 0.0003829534909379629,
      "loss": 2.2004,
      "step": 28890
    },
    {
      "epoch": 65.68181818181819,
      "grad_norm": 0.23882323503494263,
      "learning_rate": 0.00038287713665261983,
      "loss": 2.2056,
      "step": 28900
    },
    {
      "epoch": 65.70454545454545,
      "grad_norm": 0.2864723801612854,
      "learning_rate": 0.00038280076508844917,
      "loss": 2.2049,
      "step": 28910
    },
    {
      "epoch": 65.72727272727273,
      "grad_norm": 0.26404374837875366,
      "learning_rate": 0.00038272437625538213,
      "loss": 2.2027,
      "step": 28920
    },
    {
      "epoch": 65.75,
      "grad_norm": 0.5556021928787231,
      "learning_rate": 0.000382647970163352,
      "loss": 2.1941,
      "step": 28930
    },
    {
      "epoch": 65.77272727272727,
      "grad_norm": 0.23192168772220612,
      "learning_rate": 0.0003825715468222942,
      "loss": 2.2005,
      "step": 28940
    },
    {
      "epoch": 65.79545454545455,
      "grad_norm": 0.19029280543327332,
      "learning_rate": 0.0003824951062421468,
      "loss": 2.2016,
      "step": 28950
    },
    {
      "epoch": 65.81818181818181,
      "grad_norm": 0.16291461884975433,
      "learning_rate": 0.00038241864843284966,
      "loss": 2.2044,
      "step": 28960
    },
    {
      "epoch": 65.8409090909091,
      "grad_norm": 0.19611918926239014,
      "learning_rate": 0.00038234217340434506,
      "loss": 2.209,
      "step": 28970
    },
    {
      "epoch": 65.86363636363636,
      "grad_norm": 0.18299096822738647,
      "learning_rate": 0.0003822656811665776,
      "loss": 2.2122,
      "step": 28980
    },
    {
      "epoch": 65.88636363636364,
      "grad_norm": 0.24466805160045624,
      "learning_rate": 0.0003821891717294938,
      "loss": 2.2094,
      "step": 28990
    },
    {
      "epoch": 65.9090909090909,
      "grad_norm": 0.1963156759738922,
      "learning_rate": 0.0003821126451030429,
      "loss": 2.2028,
      "step": 29000
    },
    {
      "epoch": 65.93181818181819,
      "grad_norm": 0.8556608557701111,
      "learning_rate": 0.00038203610129717615,
      "loss": 2.198,
      "step": 29010
    },
    {
      "epoch": 65.95454545454545,
      "grad_norm": 0.2348545342683792,
      "learning_rate": 0.00038195954032184675,
      "loss": 2.2016,
      "step": 29020
    },
    {
      "epoch": 65.97727272727273,
      "grad_norm": 0.18627062439918518,
      "learning_rate": 0.0003818829621870107,
      "loss": 2.2039,
      "step": 29030
    },
    {
      "epoch": 66.0,
      "grad_norm": 0.5332903861999512,
      "learning_rate": 0.00038180636690262563,
      "loss": 2.2078,
      "step": 29040
    },
    {
      "epoch": 66.0,
      "eval_loss": 1.0998170375823975,
      "eval_runtime": 8.9993,
      "eval_samples_per_second": 3381.504,
      "eval_steps_per_second": 13.223,
      "step": 29040
    },
    {
      "epoch": 66.02272727272727,
      "grad_norm": 0.19296404719352722,
      "learning_rate": 0.0003817297544786519,
      "loss": 2.1961,
      "step": 29050
    },
    {
      "epoch": 66.04545454545455,
      "grad_norm": 0.2040926218032837,
      "learning_rate": 0.00038165312492505183,
      "loss": 2.1961,
      "step": 29060
    },
    {
      "epoch": 66.06818181818181,
      "grad_norm": 0.2822340428829193,
      "learning_rate": 0.00038157647825178996,
      "loss": 2.187,
      "step": 29070
    },
    {
      "epoch": 66.0909090909091,
      "grad_norm": 0.14315247535705566,
      "learning_rate": 0.0003814998144688333,
      "loss": 2.2091,
      "step": 29080
    },
    {
      "epoch": 66.11363636363636,
      "grad_norm": 0.28765323758125305,
      "learning_rate": 0.00038142313358615075,
      "loss": 2.1954,
      "step": 29090
    },
    {
      "epoch": 66.13636363636364,
      "grad_norm": 0.45049723982810974,
      "learning_rate": 0.0003813464356137137,
      "loss": 2.1959,
      "step": 29100
    },
    {
      "epoch": 66.1590909090909,
      "grad_norm": 0.6059975028038025,
      "learning_rate": 0.00038126972056149556,
      "loss": 2.2051,
      "step": 29110
    },
    {
      "epoch": 66.18181818181819,
      "grad_norm": 0.3002888560295105,
      "learning_rate": 0.000381192988439472,
      "loss": 2.2098,
      "step": 29120
    },
    {
      "epoch": 66.20454545454545,
      "grad_norm": 0.25884580612182617,
      "learning_rate": 0.0003811162392576213,
      "loss": 2.2019,
      "step": 29130
    },
    {
      "epoch": 66.22727272727273,
      "grad_norm": 0.2955671548843384,
      "learning_rate": 0.00038103947302592324,
      "loss": 2.2048,
      "step": 29140
    },
    {
      "epoch": 66.25,
      "grad_norm": 0.17400547862052917,
      "learning_rate": 0.00038096268975436044,
      "loss": 2.1948,
      "step": 29150
    },
    {
      "epoch": 66.27272727272727,
      "grad_norm": 0.16693277657032013,
      "learning_rate": 0.0003808858894529174,
      "loss": 2.1948,
      "step": 29160
    },
    {
      "epoch": 66.29545454545455,
      "grad_norm": 0.29873692989349365,
      "learning_rate": 0.0003808090721315809,
      "loss": 2.1919,
      "step": 29170
    },
    {
      "epoch": 66.31818181818181,
      "grad_norm": 0.26764196157455444,
      "learning_rate": 0.0003807322378003401,
      "loss": 2.2082,
      "step": 29180
    },
    {
      "epoch": 66.3409090909091,
      "grad_norm": 0.18115846812725067,
      "learning_rate": 0.00038065538646918606,
      "loss": 2.2054,
      "step": 29190
    },
    {
      "epoch": 66.36363636363636,
      "grad_norm": 0.23521678149700165,
      "learning_rate": 0.0003805785181481123,
      "loss": 2.209,
      "step": 29200
    },
    {
      "epoch": 66.38636363636364,
      "grad_norm": 0.2325671762228012,
      "learning_rate": 0.00038050163284711466,
      "loss": 2.1967,
      "step": 29210
    },
    {
      "epoch": 66.4090909090909,
      "grad_norm": 0.48027747869491577,
      "learning_rate": 0.0003804247305761908,
      "loss": 2.2085,
      "step": 29220
    },
    {
      "epoch": 66.43181818181819,
      "grad_norm": 0.3429323434829712,
      "learning_rate": 0.0003803478113453408,
      "loss": 2.199,
      "step": 29230
    },
    {
      "epoch": 66.45454545454545,
      "grad_norm": 0.2600264847278595,
      "learning_rate": 0.00038027087516456705,
      "loss": 2.2,
      "step": 29240
    },
    {
      "epoch": 66.47727272727273,
      "grad_norm": 0.25400298833847046,
      "learning_rate": 0.00038019392204387384,
      "loss": 2.1947,
      "step": 29250
    },
    {
      "epoch": 66.5,
      "grad_norm": 0.23293434083461761,
      "learning_rate": 0.0003801169519932681,
      "loss": 2.2106,
      "step": 29260
    },
    {
      "epoch": 66.52272727272727,
      "grad_norm": 0.21001043915748596,
      "learning_rate": 0.0003800399650227585,
      "loss": 2.1994,
      "step": 29270
    },
    {
      "epoch": 66.54545454545455,
      "grad_norm": 0.39846834540367126,
      "learning_rate": 0.00037996296114235637,
      "loss": 2.2063,
      "step": 29280
    },
    {
      "epoch": 66.56818181818181,
      "grad_norm": 0.1683979481458664,
      "learning_rate": 0.0003798859403620747,
      "loss": 2.2003,
      "step": 29290
    },
    {
      "epoch": 66.5909090909091,
      "grad_norm": 0.18579863011837006,
      "learning_rate": 0.0003798089026919292,
      "loss": 2.1937,
      "step": 29300
    },
    {
      "epoch": 66.61363636363636,
      "grad_norm": 0.2448330968618393,
      "learning_rate": 0.00037973184814193745,
      "loss": 2.1932,
      "step": 29310
    },
    {
      "epoch": 66.63636363636364,
      "grad_norm": 0.19860157370567322,
      "learning_rate": 0.00037965477672211934,
      "loss": 2.1959,
      "step": 29320
    },
    {
      "epoch": 66.6590909090909,
      "grad_norm": 0.16076447069644928,
      "learning_rate": 0.00037957768844249694,
      "loss": 2.1905,
      "step": 29330
    },
    {
      "epoch": 66.68181818181819,
      "grad_norm": 0.1941993534564972,
      "learning_rate": 0.00037950058331309456,
      "loss": 2.2021,
      "step": 29340
    },
    {
      "epoch": 66.70454545454545,
      "grad_norm": 0.2628978192806244,
      "learning_rate": 0.0003794234613439387,
      "loss": 2.1957,
      "step": 29350
    },
    {
      "epoch": 66.72727272727273,
      "grad_norm": 0.19968511164188385,
      "learning_rate": 0.00037934632254505774,
      "loss": 2.1846,
      "step": 29360
    },
    {
      "epoch": 66.75,
      "grad_norm": 0.3151729106903076,
      "learning_rate": 0.00037926916692648273,
      "loss": 2.1935,
      "step": 29370
    },
    {
      "epoch": 66.77272727272727,
      "grad_norm": 0.22381620109081268,
      "learning_rate": 0.0003791919944982467,
      "loss": 2.2194,
      "step": 29380
    },
    {
      "epoch": 66.79545454545455,
      "grad_norm": 0.19462381303310394,
      "learning_rate": 0.0003791148052703848,
      "loss": 2.2036,
      "step": 29390
    },
    {
      "epoch": 66.81818181818181,
      "grad_norm": 0.1799500286579132,
      "learning_rate": 0.00037903759925293437,
      "loss": 2.1988,
      "step": 29400
    },
    {
      "epoch": 66.8409090909091,
      "grad_norm": 0.3626481890678406,
      "learning_rate": 0.00037896037645593496,
      "loss": 2.2115,
      "step": 29410
    },
    {
      "epoch": 66.86363636363636,
      "grad_norm": 0.2052336186170578,
      "learning_rate": 0.00037888313688942845,
      "loss": 2.2059,
      "step": 29420
    },
    {
      "epoch": 66.88636363636364,
      "grad_norm": 0.18856260180473328,
      "learning_rate": 0.00037880588056345865,
      "loss": 2.212,
      "step": 29430
    },
    {
      "epoch": 66.9090909090909,
      "grad_norm": 0.20624101161956787,
      "learning_rate": 0.00037872860748807183,
      "loss": 2.2065,
      "step": 29440
    },
    {
      "epoch": 66.93181818181819,
      "grad_norm": 0.21409408748149872,
      "learning_rate": 0.00037865131767331607,
      "loss": 2.1996,
      "step": 29450
    },
    {
      "epoch": 66.95454545454545,
      "grad_norm": 0.3163796663284302,
      "learning_rate": 0.00037857401112924207,
      "loss": 2.197,
      "step": 29460
    },
    {
      "epoch": 66.97727272727273,
      "grad_norm": 0.25940316915512085,
      "learning_rate": 0.0003784966878659023,
      "loss": 2.2012,
      "step": 29470
    },
    {
      "epoch": 67.0,
      "grad_norm": 0.3806673288345337,
      "learning_rate": 0.0003784193478933516,
      "loss": 2.1906,
      "step": 29480
    },
    {
      "epoch": 67.0,
      "eval_loss": 1.0999817848205566,
      "eval_runtime": 8.7732,
      "eval_samples_per_second": 3468.647,
      "eval_steps_per_second": 13.564,
      "step": 29480
    },
    {
      "epoch": 67.02272727272727,
      "grad_norm": 0.18813349306583405,
      "learning_rate": 0.00037834199122164705,
      "loss": 2.2002,
      "step": 29490
    },
    {
      "epoch": 67.04545454545455,
      "grad_norm": 0.38070371747016907,
      "learning_rate": 0.00037826461786084767,
      "loss": 2.2044,
      "step": 29500
    },
    {
      "epoch": 67.06818181818181,
      "grad_norm": 0.17431019246578217,
      "learning_rate": 0.00037818722782101497,
      "loss": 2.1978,
      "step": 29510
    },
    {
      "epoch": 67.0909090909091,
      "grad_norm": 0.279369056224823,
      "learning_rate": 0.00037810982111221225,
      "loss": 2.195,
      "step": 29520
    },
    {
      "epoch": 67.11363636363636,
      "grad_norm": 0.17273591458797455,
      "learning_rate": 0.0003780323977445054,
      "loss": 2.1987,
      "step": 29530
    },
    {
      "epoch": 67.13636363636364,
      "grad_norm": 0.3136334717273712,
      "learning_rate": 0.00037795495772796205,
      "loss": 2.195,
      "step": 29540
    },
    {
      "epoch": 67.1590909090909,
      "grad_norm": 0.18890169262886047,
      "learning_rate": 0.00037787750107265227,
      "loss": 2.2017,
      "step": 29550
    },
    {
      "epoch": 67.18181818181819,
      "grad_norm": 0.3779754340648651,
      "learning_rate": 0.0003778000277886483,
      "loss": 2.2064,
      "step": 29560
    },
    {
      "epoch": 67.20454545454545,
      "grad_norm": 0.2737627923488617,
      "learning_rate": 0.0003777225378860244,
      "loss": 2.1979,
      "step": 29570
    },
    {
      "epoch": 67.22727272727273,
      "grad_norm": 0.29804009199142456,
      "learning_rate": 0.00037764503137485705,
      "loss": 2.1968,
      "step": 29580
    },
    {
      "epoch": 67.25,
      "grad_norm": 0.22671197354793549,
      "learning_rate": 0.000377567508265225,
      "loss": 2.1939,
      "step": 29590
    },
    {
      "epoch": 67.27272727272727,
      "grad_norm": 0.6031516790390015,
      "learning_rate": 0.00037748996856720875,
      "loss": 2.1922,
      "step": 29600
    },
    {
      "epoch": 67.29545454545455,
      "grad_norm": 0.15792471170425415,
      "learning_rate": 0.0003774124122908916,
      "loss": 2.1996,
      "step": 29610
    },
    {
      "epoch": 67.31818181818181,
      "grad_norm": 0.8141547441482544,
      "learning_rate": 0.0003773348394463586,
      "loss": 2.1956,
      "step": 29620
    },
    {
      "epoch": 67.3409090909091,
      "grad_norm": 0.22393402457237244,
      "learning_rate": 0.00037725725004369683,
      "loss": 2.1955,
      "step": 29630
    },
    {
      "epoch": 67.36363636363636,
      "grad_norm": 0.24298012256622314,
      "learning_rate": 0.00037717964409299595,
      "loss": 2.2055,
      "step": 29640
    },
    {
      "epoch": 67.38636363636364,
      "grad_norm": 0.153050497174263,
      "learning_rate": 0.00037710202160434736,
      "loss": 2.203,
      "step": 29650
    },
    {
      "epoch": 67.4090909090909,
      "grad_norm": 0.24918517470359802,
      "learning_rate": 0.0003770243825878449,
      "loss": 2.1851,
      "step": 29660
    },
    {
      "epoch": 67.43181818181819,
      "grad_norm": 0.4152210056781769,
      "learning_rate": 0.00037694672705358446,
      "loss": 2.1991,
      "step": 29670
    },
    {
      "epoch": 67.45454545454545,
      "grad_norm": 0.26923927664756775,
      "learning_rate": 0.0003768690550116639,
      "loss": 2.1889,
      "step": 29680
    },
    {
      "epoch": 67.47727272727273,
      "grad_norm": 2.398557662963867,
      "learning_rate": 0.0003767913664721836,
      "loss": 2.2039,
      "step": 29690
    },
    {
      "epoch": 67.5,
      "grad_norm": 0.7136917114257812,
      "learning_rate": 0.0003767136614452458,
      "loss": 2.217,
      "step": 29700
    },
    {
      "epoch": 67.52272727272727,
      "grad_norm": 0.3597274720668793,
      "learning_rate": 0.0003766359399409549,
      "loss": 2.2049,
      "step": 29710
    },
    {
      "epoch": 67.54545454545455,
      "grad_norm": 0.3256399631500244,
      "learning_rate": 0.00037655820196941746,
      "loss": 2.1999,
      "step": 29720
    },
    {
      "epoch": 67.56818181818181,
      "grad_norm": 0.36159369349479675,
      "learning_rate": 0.0003764804475407424,
      "loss": 2.2056,
      "step": 29730
    },
    {
      "epoch": 67.5909090909091,
      "grad_norm": 0.3041461110115051,
      "learning_rate": 0.0003764026766650405,
      "loss": 2.2076,
      "step": 29740
    },
    {
      "epoch": 67.61363636363636,
      "grad_norm": 0.15416420996189117,
      "learning_rate": 0.0003763248893524247,
      "loss": 2.1932,
      "step": 29750
    },
    {
      "epoch": 67.63636363636364,
      "grad_norm": 0.16377438604831696,
      "learning_rate": 0.00037624708561301035,
      "loss": 2.1914,
      "step": 29760
    },
    {
      "epoch": 67.6590909090909,
      "grad_norm": 0.18554672598838806,
      "learning_rate": 0.0003761692654569147,
      "loss": 2.211,
      "step": 29770
    },
    {
      "epoch": 67.68181818181819,
      "grad_norm": 0.5654928684234619,
      "learning_rate": 0.0003760914288942571,
      "loss": 2.1863,
      "step": 29780
    },
    {
      "epoch": 67.70454545454545,
      "grad_norm": 0.2772725224494934,
      "learning_rate": 0.00037601357593515907,
      "loss": 2.1946,
      "step": 29790
    },
    {
      "epoch": 67.72727272727273,
      "grad_norm": 0.2505163550376892,
      "learning_rate": 0.0003759357065897444,
      "loss": 2.2054,
      "step": 29800
    },
    {
      "epoch": 67.75,
      "grad_norm": 0.38959938287734985,
      "learning_rate": 0.000375857820868139,
      "loss": 2.1962,
      "step": 29810
    },
    {
      "epoch": 67.77272727272727,
      "grad_norm": 0.1889212727546692,
      "learning_rate": 0.00037577991878047067,
      "loss": 2.1932,
      "step": 29820
    },
    {
      "epoch": 67.79545454545455,
      "grad_norm": 0.3174355924129486,
      "learning_rate": 0.00037570200033686953,
      "loss": 2.1928,
      "step": 29830
    },
    {
      "epoch": 67.81818181818181,
      "grad_norm": 0.17084775865077972,
      "learning_rate": 0.00037562406554746786,
      "loss": 2.2089,
      "step": 29840
    },
    {
      "epoch": 67.8409090909091,
      "grad_norm": 0.4342440366744995,
      "learning_rate": 0.0003755461144223999,
      "loss": 2.2005,
      "step": 29850
    },
    {
      "epoch": 67.86363636363636,
      "grad_norm": 0.2697969079017639,
      "learning_rate": 0.0003754681469718022,
      "loss": 2.2034,
      "step": 29860
    },
    {
      "epoch": 67.88636363636364,
      "grad_norm": 0.18125979602336884,
      "learning_rate": 0.0003753901632058133,
      "loss": 2.2092,
      "step": 29870
    },
    {
      "epoch": 67.9090909090909,
      "grad_norm": 0.17386135458946228,
      "learning_rate": 0.0003753121631345739,
      "loss": 2.2059,
      "step": 29880
    },
    {
      "epoch": 67.93181818181819,
      "grad_norm": 0.21660944819450378,
      "learning_rate": 0.00037523414676822697,
      "loss": 2.2083,
      "step": 29890
    },
    {
      "epoch": 67.95454545454545,
      "grad_norm": 0.20332232117652893,
      "learning_rate": 0.00037515611411691717,
      "loss": 2.2016,
      "step": 29900
    },
    {
      "epoch": 67.97727272727273,
      "grad_norm": 0.2744937539100647,
      "learning_rate": 0.00037507806519079183,
      "loss": 2.2022,
      "step": 29910
    },
    {
      "epoch": 68.0,
      "grad_norm": 0.23587369918823242,
      "learning_rate": 0.000375,
      "loss": 2.206,
      "step": 29920
    },
    {
      "epoch": 68.0,
      "eval_loss": 1.1003788709640503,
      "eval_runtime": 8.9224,
      "eval_samples_per_second": 3410.643,
      "eval_steps_per_second": 13.337,
      "step": 29920
    },
    {
      "epoch": 68.02272727272727,
      "grad_norm": 0.5383339524269104,
      "learning_rate": 0.000374921918554693,
      "loss": 2.2076,
      "step": 29930
    },
    {
      "epoch": 68.04545454545455,
      "grad_norm": 0.19983616471290588,
      "learning_rate": 0.0003748438208650242,
      "loss": 2.1985,
      "step": 29940
    },
    {
      "epoch": 68.06818181818181,
      "grad_norm": 0.19034001231193542,
      "learning_rate": 0.0003747657069411492,
      "loss": 2.1992,
      "step": 29950
    },
    {
      "epoch": 68.0909090909091,
      "grad_norm": 0.28852102160453796,
      "learning_rate": 0.00037468757679322555,
      "loss": 2.1928,
      "step": 29960
    },
    {
      "epoch": 68.11363636363636,
      "grad_norm": 0.2299850434064865,
      "learning_rate": 0.0003746094304314131,
      "loss": 2.2074,
      "step": 29970
    },
    {
      "epoch": 68.13636363636364,
      "grad_norm": 0.17357030510902405,
      "learning_rate": 0.0003745312678658736,
      "loss": 2.2033,
      "step": 29980
    },
    {
      "epoch": 68.1590909090909,
      "grad_norm": 0.4765220284461975,
      "learning_rate": 0.00037445308910677097,
      "loss": 2.2056,
      "step": 29990
    },
    {
      "epoch": 68.18181818181819,
      "grad_norm": 0.20780551433563232,
      "learning_rate": 0.0003743748941642714,
      "loss": 2.1938,
      "step": 30000
    },
    {
      "epoch": 68.20454545454545,
      "grad_norm": 0.15610626339912415,
      "learning_rate": 0.0003742966830485429,
      "loss": 2.187,
      "step": 30010
    },
    {
      "epoch": 68.22727272727273,
      "grad_norm": 0.18936611711978912,
      "learning_rate": 0.000374218455769756,
      "loss": 2.2077,
      "step": 30020
    },
    {
      "epoch": 68.25,
      "grad_norm": 0.15390874445438385,
      "learning_rate": 0.00037414021233808285,
      "loss": 2.2028,
      "step": 30030
    },
    {
      "epoch": 68.27272727272727,
      "grad_norm": 0.1793724149465561,
      "learning_rate": 0.00037406195276369793,
      "loss": 2.1907,
      "step": 30040
    },
    {
      "epoch": 68.29545454545455,
      "grad_norm": 0.2784619629383087,
      "learning_rate": 0.00037398367705677787,
      "loss": 2.1992,
      "step": 30050
    },
    {
      "epoch": 68.31818181818181,
      "grad_norm": 0.19729861617088318,
      "learning_rate": 0.0003739053852275013,
      "loss": 2.1989,
      "step": 30060
    },
    {
      "epoch": 68.3409090909091,
      "grad_norm": 0.21298864483833313,
      "learning_rate": 0.00037382707728604917,
      "loss": 2.1981,
      "step": 30070
    },
    {
      "epoch": 68.36363636363636,
      "grad_norm": 0.32406991720199585,
      "learning_rate": 0.00037374875324260406,
      "loss": 2.1939,
      "step": 30080
    },
    {
      "epoch": 68.38636363636364,
      "grad_norm": 0.2213105708360672,
      "learning_rate": 0.00037367041310735105,
      "loss": 2.1995,
      "step": 30090
    },
    {
      "epoch": 68.4090909090909,
      "grad_norm": 0.32683274149894714,
      "learning_rate": 0.00037359205689047726,
      "loss": 2.2003,
      "step": 30100
    },
    {
      "epoch": 68.43181818181819,
      "grad_norm": 0.16172125935554504,
      "learning_rate": 0.00037351368460217173,
      "loss": 2.1998,
      "step": 30110
    },
    {
      "epoch": 68.45454545454545,
      "grad_norm": 0.7147670388221741,
      "learning_rate": 0.00037343529625262565,
      "loss": 2.2091,
      "step": 30120
    },
    {
      "epoch": 68.47727272727273,
      "grad_norm": 0.18328922986984253,
      "learning_rate": 0.0003733568918520325,
      "loss": 2.2023,
      "step": 30130
    },
    {
      "epoch": 68.5,
      "grad_norm": 0.20598575472831726,
      "learning_rate": 0.0003732784714105876,
      "loss": 2.1983,
      "step": 30140
    },
    {
      "epoch": 68.52272727272727,
      "grad_norm": 0.2614271640777588,
      "learning_rate": 0.00037320003493848837,
      "loss": 2.1953,
      "step": 30150
    },
    {
      "epoch": 68.54545454545455,
      "grad_norm": 0.2934618294239044,
      "learning_rate": 0.00037312158244593444,
      "loss": 2.2073,
      "step": 30160
    },
    {
      "epoch": 68.56818181818181,
      "grad_norm": 0.477016806602478,
      "learning_rate": 0.0003730431139431275,
      "loss": 2.203,
      "step": 30170
    },
    {
      "epoch": 68.5909090909091,
      "grad_norm": 0.18211522698402405,
      "learning_rate": 0.0003729646294402712,
      "loss": 2.1998,
      "step": 30180
    },
    {
      "epoch": 68.61363636363636,
      "grad_norm": 0.32928186655044556,
      "learning_rate": 0.00037288612894757143,
      "loss": 2.203,
      "step": 30190
    },
    {
      "epoch": 68.63636363636364,
      "grad_norm": 0.24939122796058655,
      "learning_rate": 0.00037280761247523623,
      "loss": 2.1993,
      "step": 30200
    },
    {
      "epoch": 68.6590909090909,
      "grad_norm": 0.3739861249923706,
      "learning_rate": 0.00037272908003347526,
      "loss": 2.1923,
      "step": 30210
    },
    {
      "epoch": 68.68181818181819,
      "grad_norm": 0.37788766622543335,
      "learning_rate": 0.00037265053163250086,
      "loss": 2.1944,
      "step": 30220
    },
    {
      "epoch": 68.70454545454545,
      "grad_norm": 0.26872628927230835,
      "learning_rate": 0.00037257196728252695,
      "loss": 2.2035,
      "step": 30230
    },
    {
      "epoch": 68.72727272727273,
      "grad_norm": 0.32364922761917114,
      "learning_rate": 0.0003724933869937698,
      "loss": 2.212,
      "step": 30240
    },
    {
      "epoch": 68.75,
      "grad_norm": 0.34087738394737244,
      "learning_rate": 0.0003724147907764478,
      "loss": 2.1971,
      "step": 30250
    },
    {
      "epoch": 68.77272727272727,
      "grad_norm": 0.1761125922203064,
      "learning_rate": 0.0003723361786407812,
      "loss": 2.1925,
      "step": 30260
    },
    {
      "epoch": 68.79545454545455,
      "grad_norm": 0.17343494296073914,
      "learning_rate": 0.00037225755059699245,
      "loss": 2.2037,
      "step": 30270
    },
    {
      "epoch": 68.81818181818181,
      "grad_norm": 0.1455356925725937,
      "learning_rate": 0.0003721789066553059,
      "loss": 2.1832,
      "step": 30280
    },
    {
      "epoch": 68.8409090909091,
      "grad_norm": 0.2862800359725952,
      "learning_rate": 0.00037210024682594834,
      "loss": 2.1933,
      "step": 30290
    },
    {
      "epoch": 68.86363636363636,
      "grad_norm": 0.2300395667552948,
      "learning_rate": 0.00037202157111914817,
      "loss": 2.1978,
      "step": 30300
    },
    {
      "epoch": 68.88636363636364,
      "grad_norm": 0.14889350533485413,
      "learning_rate": 0.00037194287954513616,
      "loss": 2.1912,
      "step": 30310
    },
    {
      "epoch": 68.9090909090909,
      "grad_norm": 0.43716830015182495,
      "learning_rate": 0.00037186417211414516,
      "loss": 2.2017,
      "step": 30320
    },
    {
      "epoch": 68.93181818181819,
      "grad_norm": 0.32442381978034973,
      "learning_rate": 0.00037178544883640984,
      "loss": 2.2126,
      "step": 30330
    },
    {
      "epoch": 68.95454545454545,
      "grad_norm": 0.20755472779273987,
      "learning_rate": 0.0003717067097221671,
      "loss": 2.2004,
      "step": 30340
    },
    {
      "epoch": 68.97727272727273,
      "grad_norm": 0.1558408886194229,
      "learning_rate": 0.0003716279547816559,
      "loss": 2.2016,
      "step": 30350
    },
    {
      "epoch": 69.0,
      "grad_norm": 0.19879630208015442,
      "learning_rate": 0.0003715491840251172,
      "loss": 2.2027,
      "step": 30360
    },
    {
      "epoch": 69.0,
      "eval_loss": 1.0998173952102661,
      "eval_runtime": 8.678,
      "eval_samples_per_second": 3506.674,
      "eval_steps_per_second": 13.713,
      "step": 30360
    },
    {
      "epoch": 69.02272727272727,
      "grad_norm": 0.18249809741973877,
      "learning_rate": 0.00037147039746279406,
      "loss": 2.1987,
      "step": 30370
    },
    {
      "epoch": 69.04545454545455,
      "grad_norm": 0.38108181953430176,
      "learning_rate": 0.00037139159510493153,
      "loss": 2.1957,
      "step": 30380
    },
    {
      "epoch": 69.06818181818181,
      "grad_norm": 0.1983823925256729,
      "learning_rate": 0.0003713127769617769,
      "loss": 2.1961,
      "step": 30390
    },
    {
      "epoch": 69.0909090909091,
      "grad_norm": 0.412168949842453,
      "learning_rate": 0.0003712339430435792,
      "loss": 2.1874,
      "step": 30400
    },
    {
      "epoch": 69.11363636363636,
      "grad_norm": 0.8539243936538696,
      "learning_rate": 0.00037115509336058973,
      "loss": 2.2003,
      "step": 30410
    },
    {
      "epoch": 69.13636363636364,
      "grad_norm": 0.2794211208820343,
      "learning_rate": 0.00037107622792306196,
      "loss": 2.1943,
      "step": 30420
    },
    {
      "epoch": 69.1590909090909,
      "grad_norm": 1.16416335105896,
      "learning_rate": 0.00037099734674125097,
      "loss": 2.1966,
      "step": 30430
    },
    {
      "epoch": 69.18181818181819,
      "grad_norm": 0.694496750831604,
      "learning_rate": 0.0003709184498254144,
      "loss": 2.1925,
      "step": 30440
    },
    {
      "epoch": 69.20454545454545,
      "grad_norm": 0.14753961563110352,
      "learning_rate": 0.00037083953718581165,
      "loss": 2.1971,
      "step": 30450
    },
    {
      "epoch": 69.22727272727273,
      "grad_norm": 0.6788473129272461,
      "learning_rate": 0.0003707606088327041,
      "loss": 2.1924,
      "step": 30460
    },
    {
      "epoch": 69.25,
      "grad_norm": 0.3828502893447876,
      "learning_rate": 0.0003706816647763555,
      "loss": 2.198,
      "step": 30470
    },
    {
      "epoch": 69.27272727272727,
      "grad_norm": 0.33619365096092224,
      "learning_rate": 0.00037060270502703115,
      "loss": 2.1941,
      "step": 30480
    },
    {
      "epoch": 69.29545454545455,
      "grad_norm": 0.35094815492630005,
      "learning_rate": 0.0003705237295949988,
      "loss": 2.2017,
      "step": 30490
    },
    {
      "epoch": 69.31818181818181,
      "grad_norm": 0.1487308144569397,
      "learning_rate": 0.00037044473849052816,
      "loss": 2.2032,
      "step": 30500
    },
    {
      "epoch": 69.3409090909091,
      "grad_norm": 0.43451958894729614,
      "learning_rate": 0.00037036573172389095,
      "loss": 2.1944,
      "step": 30510
    },
    {
      "epoch": 69.36363636363636,
      "grad_norm": 0.22855831682682037,
      "learning_rate": 0.0003702867093053608,
      "loss": 2.204,
      "step": 30520
    },
    {
      "epoch": 69.38636363636364,
      "grad_norm": 0.33952051401138306,
      "learning_rate": 0.0003702076712452135,
      "loss": 2.1938,
      "step": 30530
    },
    {
      "epoch": 69.4090909090909,
      "grad_norm": 0.17032991349697113,
      "learning_rate": 0.00037012861755372686,
      "loss": 2.2057,
      "step": 30540
    },
    {
      "epoch": 69.43181818181819,
      "grad_norm": 0.28663644194602966,
      "learning_rate": 0.0003700495482411807,
      "loss": 2.1846,
      "step": 30550
    },
    {
      "epoch": 69.45454545454545,
      "grad_norm": 0.3410150706768036,
      "learning_rate": 0.00036997046331785696,
      "loss": 2.1954,
      "step": 30560
    },
    {
      "epoch": 69.47727272727273,
      "grad_norm": 0.3372148871421814,
      "learning_rate": 0.0003698913627940395,
      "loss": 2.2081,
      "step": 30570
    },
    {
      "epoch": 69.5,
      "grad_norm": 0.1860814094543457,
      "learning_rate": 0.0003698122466800142,
      "loss": 2.2067,
      "step": 30580
    },
    {
      "epoch": 69.52272727272727,
      "grad_norm": 0.5073545575141907,
      "learning_rate": 0.0003697331149860691,
      "loss": 2.197,
      "step": 30590
    },
    {
      "epoch": 69.54545454545455,
      "grad_norm": 0.18175779283046722,
      "learning_rate": 0.0003696539677224941,
      "loss": 2.206,
      "step": 30600
    },
    {
      "epoch": 69.56818181818181,
      "grad_norm": 0.2144797146320343,
      "learning_rate": 0.0003695748048995812,
      "loss": 2.1939,
      "step": 30610
    },
    {
      "epoch": 69.5909090909091,
      "grad_norm": 0.42176538705825806,
      "learning_rate": 0.0003694956265276245,
      "loss": 2.1936,
      "step": 30620
    },
    {
      "epoch": 69.61363636363636,
      "grad_norm": 0.511772871017456,
      "learning_rate": 0.00036941643261692,
      "loss": 2.196,
      "step": 30630
    },
    {
      "epoch": 69.63636363636364,
      "grad_norm": 0.36180850863456726,
      "learning_rate": 0.0003693372231777658,
      "loss": 2.2016,
      "step": 30640
    },
    {
      "epoch": 69.6590909090909,
      "grad_norm": 0.19301998615264893,
      "learning_rate": 0.0003692579982204619,
      "loss": 2.1915,
      "step": 30650
    },
    {
      "epoch": 69.68181818181819,
      "grad_norm": 0.20033438503742218,
      "learning_rate": 0.0003691787577553105,
      "loss": 2.1991,
      "step": 30660
    },
    {
      "epoch": 69.70454545454545,
      "grad_norm": 0.21267849206924438,
      "learning_rate": 0.0003690995017926157,
      "loss": 2.1831,
      "step": 30670
    },
    {
      "epoch": 69.72727272727273,
      "grad_norm": 0.32101529836654663,
      "learning_rate": 0.00036902023034268363,
      "loss": 2.1995,
      "step": 30680
    },
    {
      "epoch": 69.75,
      "grad_norm": 0.2118995487689972,
      "learning_rate": 0.0003689409434158224,
      "loss": 2.2029,
      "step": 30690
    },
    {
      "epoch": 69.77272727272727,
      "grad_norm": 0.4548003673553467,
      "learning_rate": 0.00036886164102234225,
      "loss": 2.2061,
      "step": 30700
    },
    {
      "epoch": 69.79545454545455,
      "grad_norm": 1.2194591760635376,
      "learning_rate": 0.00036878232317255535,
      "loss": 2.2054,
      "step": 30710
    },
    {
      "epoch": 69.81818181818181,
      "grad_norm": 0.20955675840377808,
      "learning_rate": 0.0003687029898767758,
      "loss": 2.2034,
      "step": 30720
    },
    {
      "epoch": 69.8409090909091,
      "grad_norm": 0.22625818848609924,
      "learning_rate": 0.0003686236411453199,
      "loss": 2.2158,
      "step": 30730
    },
    {
      "epoch": 69.86363636363636,
      "grad_norm": 0.36057400703430176,
      "learning_rate": 0.00036854427698850575,
      "loss": 2.1974,
      "step": 30740
    },
    {
      "epoch": 69.88636363636364,
      "grad_norm": 0.20488305389881134,
      "learning_rate": 0.0003684648974166536,
      "loss": 2.2105,
      "step": 30750
    },
    {
      "epoch": 69.9090909090909,
      "grad_norm": 0.5061398148536682,
      "learning_rate": 0.00036838550244008573,
      "loss": 2.2002,
      "step": 30760
    },
    {
      "epoch": 69.93181818181819,
      "grad_norm": 0.35145673155784607,
      "learning_rate": 0.0003683060920691263,
      "loss": 2.2147,
      "step": 30770
    },
    {
      "epoch": 69.95454545454545,
      "grad_norm": 0.15172626078128815,
      "learning_rate": 0.00036822666631410153,
      "loss": 2.2036,
      "step": 30780
    },
    {
      "epoch": 69.97727272727273,
      "grad_norm": 0.27387475967407227,
      "learning_rate": 0.0003681472251853395,
      "loss": 2.2014,
      "step": 30790
    },
    {
      "epoch": 70.0,
      "grad_norm": 0.3414675295352936,
      "learning_rate": 0.0003680677686931707,
      "loss": 2.2068,
      "step": 30800
    },
    {
      "epoch": 70.0,
      "eval_loss": 1.099814534187317,
      "eval_runtime": 8.9143,
      "eval_samples_per_second": 3413.746,
      "eval_steps_per_second": 13.349,
      "step": 30800
    },
    {
      "epoch": 70.02272727272727,
      "grad_norm": 0.22840134799480438,
      "learning_rate": 0.0003679882968479271,
      "loss": 2.1926,
      "step": 30810
    },
    {
      "epoch": 70.04545454545455,
      "grad_norm": 0.3426913619041443,
      "learning_rate": 0.0003679088096599431,
      "loss": 2.2015,
      "step": 30820
    },
    {
      "epoch": 70.06818181818181,
      "grad_norm": 1.4102420806884766,
      "learning_rate": 0.0003678293071395548,
      "loss": 2.204,
      "step": 30830
    },
    {
      "epoch": 70.0909090909091,
      "grad_norm": 0.519490122795105,
      "learning_rate": 0.00036774978929710034,
      "loss": 2.1895,
      "step": 30840
    },
    {
      "epoch": 70.11363636363636,
      "grad_norm": 0.21248915791511536,
      "learning_rate": 0.00036767025614292003,
      "loss": 2.2073,
      "step": 30850
    },
    {
      "epoch": 70.13636363636364,
      "grad_norm": 0.18459931015968323,
      "learning_rate": 0.00036759070768735603,
      "loss": 2.2082,
      "step": 30860
    },
    {
      "epoch": 70.1590909090909,
      "grad_norm": 0.2669191360473633,
      "learning_rate": 0.0003675111439407525,
      "loss": 2.1988,
      "step": 30870
    },
    {
      "epoch": 70.18181818181819,
      "grad_norm": 0.17287349700927734,
      "learning_rate": 0.0003674315649134556,
      "loss": 2.1932,
      "step": 30880
    },
    {
      "epoch": 70.20454545454545,
      "grad_norm": 0.5024542212486267,
      "learning_rate": 0.00036735197061581344,
      "loss": 2.2135,
      "step": 30890
    },
    {
      "epoch": 70.22727272727273,
      "grad_norm": 0.4004012942314148,
      "learning_rate": 0.00036727236105817617,
      "loss": 2.1872,
      "step": 30900
    },
    {
      "epoch": 70.25,
      "grad_norm": 0.28632715344429016,
      "learning_rate": 0.00036719273625089593,
      "loss": 2.1959,
      "step": 30910
    },
    {
      "epoch": 70.27272727272727,
      "grad_norm": 0.4167174994945526,
      "learning_rate": 0.00036711309620432686,
      "loss": 2.2097,
      "step": 30920
    },
    {
      "epoch": 70.29545454545455,
      "grad_norm": 0.24696174263954163,
      "learning_rate": 0.000367033440928825,
      "loss": 2.2007,
      "step": 30930
    },
    {
      "epoch": 70.31818181818181,
      "grad_norm": 0.25958284735679626,
      "learning_rate": 0.0003669537704347484,
      "loss": 2.1996,
      "step": 30940
    },
    {
      "epoch": 70.3409090909091,
      "grad_norm": 0.19607435166835785,
      "learning_rate": 0.0003668740847324571,
      "loss": 2.1951,
      "step": 30950
    },
    {
      "epoch": 70.36363636363636,
      "grad_norm": 0.1328362673521042,
      "learning_rate": 0.00036679438383231314,
      "loss": 2.2109,
      "step": 30960
    },
    {
      "epoch": 70.38636363636364,
      "grad_norm": 0.37882164120674133,
      "learning_rate": 0.00036671466774468043,
      "loss": 2.1986,
      "step": 30970
    },
    {
      "epoch": 70.4090909090909,
      "grad_norm": 0.5391792058944702,
      "learning_rate": 0.0003666349364799251,
      "loss": 2.2074,
      "step": 30980
    },
    {
      "epoch": 70.43181818181819,
      "grad_norm": 0.23457343876361847,
      "learning_rate": 0.00036655519004841496,
      "loss": 2.1987,
      "step": 30990
    },
    {
      "epoch": 70.45454545454545,
      "grad_norm": 0.12015306949615479,
      "learning_rate": 0.00036647542846052005,
      "loss": 2.2011,
      "step": 31000
    },
    {
      "epoch": 70.47727272727273,
      "grad_norm": 0.2100140005350113,
      "learning_rate": 0.0003663956517266122,
      "loss": 2.1881,
      "step": 31010
    },
    {
      "epoch": 70.5,
      "grad_norm": 0.14345812797546387,
      "learning_rate": 0.0003663158598570652,
      "loss": 2.2039,
      "step": 31020
    },
    {
      "epoch": 70.52272727272727,
      "grad_norm": 0.22335714101791382,
      "learning_rate": 0.0003662360528622549,
      "loss": 2.2001,
      "step": 31030
    },
    {
      "epoch": 70.54545454545455,
      "grad_norm": 0.2114819884300232,
      "learning_rate": 0.0003661562307525591,
      "loss": 2.2048,
      "step": 31040
    },
    {
      "epoch": 70.56818181818181,
      "grad_norm": 0.15425537526607513,
      "learning_rate": 0.0003660763935383576,
      "loss": 2.1959,
      "step": 31050
    },
    {
      "epoch": 70.5909090909091,
      "grad_norm": 0.2378462255001068,
      "learning_rate": 0.00036599654123003225,
      "loss": 2.1969,
      "step": 31060
    },
    {
      "epoch": 70.61363636363636,
      "grad_norm": 0.2813733220100403,
      "learning_rate": 0.0003659166738379665,
      "loss": 2.1869,
      "step": 31070
    },
    {
      "epoch": 70.63636363636364,
      "grad_norm": 0.23319216072559357,
      "learning_rate": 0.00036583679137254597,
      "loss": 2.2033,
      "step": 31080
    },
    {
      "epoch": 70.6590909090909,
      "grad_norm": 0.2749640941619873,
      "learning_rate": 0.0003657568938441585,
      "loss": 2.198,
      "step": 31090
    },
    {
      "epoch": 70.68181818181819,
      "grad_norm": 0.1857207715511322,
      "learning_rate": 0.0003656769812631935,
      "loss": 2.1947,
      "step": 31100
    },
    {
      "epoch": 70.70454545454545,
      "grad_norm": 0.26215583086013794,
      "learning_rate": 0.00036559705364004255,
      "loss": 2.2063,
      "step": 31110
    },
    {
      "epoch": 70.72727272727273,
      "grad_norm": 0.16345880925655365,
      "learning_rate": 0.00036551711098509903,
      "loss": 2.2021,
      "step": 31120
    },
    {
      "epoch": 70.75,
      "grad_norm": 0.25041213631629944,
      "learning_rate": 0.0003654371533087585,
      "loss": 2.1958,
      "step": 31130
    },
    {
      "epoch": 70.77272727272727,
      "grad_norm": 0.14624299108982086,
      "learning_rate": 0.0003653571806214183,
      "loss": 2.1935,
      "step": 31140
    },
    {
      "epoch": 70.79545454545455,
      "grad_norm": 0.16401472687721252,
      "learning_rate": 0.00036527719293347774,
      "loss": 2.2054,
      "step": 31150
    },
    {
      "epoch": 70.81818181818181,
      "grad_norm": 0.402563214302063,
      "learning_rate": 0.00036519719025533817,
      "loss": 2.1934,
      "step": 31160
    },
    {
      "epoch": 70.8409090909091,
      "grad_norm": 0.2172631472349167,
      "learning_rate": 0.0003651171725974026,
      "loss": 2.1963,
      "step": 31170
    },
    {
      "epoch": 70.86363636363636,
      "grad_norm": 0.16566582024097443,
      "learning_rate": 0.00036503713997007657,
      "loss": 2.1991,
      "step": 31180
    },
    {
      "epoch": 70.88636363636364,
      "grad_norm": 0.1477779746055603,
      "learning_rate": 0.00036495709238376704,
      "loss": 2.1927,
      "step": 31190
    },
    {
      "epoch": 70.9090909090909,
      "grad_norm": 0.48233872652053833,
      "learning_rate": 0.00036487702984888313,
      "loss": 2.2013,
      "step": 31200
    },
    {
      "epoch": 70.93181818181819,
      "grad_norm": 0.34668847918510437,
      "learning_rate": 0.00036479695237583577,
      "loss": 2.2135,
      "step": 31210
    },
    {
      "epoch": 70.95454545454545,
      "grad_norm": 0.1459922045469284,
      "learning_rate": 0.0003647168599750379,
      "loss": 2.1985,
      "step": 31220
    },
    {
      "epoch": 70.97727272727273,
      "grad_norm": 0.12555105984210968,
      "learning_rate": 0.0003646367526569046,
      "loss": 2.1912,
      "step": 31230
    },
    {
      "epoch": 71.0,
      "grad_norm": 0.4342396855354309,
      "learning_rate": 0.0003645566304318526,
      "loss": 2.1994,
      "step": 31240
    },
    {
      "epoch": 71.0,
      "eval_loss": 1.0999683141708374,
      "eval_runtime": 8.7585,
      "eval_samples_per_second": 3474.469,
      "eval_steps_per_second": 13.587,
      "step": 31240
    },
    {
      "epoch": 71.02272727272727,
      "grad_norm": 0.7552376985549927,
      "learning_rate": 0.00036447649331030073,
      "loss": 2.1877,
      "step": 31250
    },
    {
      "epoch": 71.04545454545455,
      "grad_norm": 0.1899220496416092,
      "learning_rate": 0.0003643963413026696,
      "loss": 2.201,
      "step": 31260
    },
    {
      "epoch": 71.06818181818181,
      "grad_norm": 0.3981536626815796,
      "learning_rate": 0.00036431617441938204,
      "loss": 2.209,
      "step": 31270
    },
    {
      "epoch": 71.0909090909091,
      "grad_norm": 0.20235727727413177,
      "learning_rate": 0.0003642359926708625,
      "loss": 2.1922,
      "step": 31280
    },
    {
      "epoch": 71.11363636363636,
      "grad_norm": 0.24633103609085083,
      "learning_rate": 0.00036415579606753753,
      "loss": 2.195,
      "step": 31290
    },
    {
      "epoch": 71.13636363636364,
      "grad_norm": 0.34802505373954773,
      "learning_rate": 0.00036407558461983556,
      "loss": 2.194,
      "step": 31300
    },
    {
      "epoch": 71.1590909090909,
      "grad_norm": 0.18797177076339722,
      "learning_rate": 0.0003639953583381872,
      "loss": 2.1893,
      "step": 31310
    },
    {
      "epoch": 71.18181818181819,
      "grad_norm": 0.13389478623867035,
      "learning_rate": 0.0003639151172330245,
      "loss": 2.1839,
      "step": 31320
    },
    {
      "epoch": 71.20454545454545,
      "grad_norm": 0.8590036034584045,
      "learning_rate": 0.0003638348613147817,
      "loss": 2.2001,
      "step": 31330
    },
    {
      "epoch": 71.22727272727273,
      "grad_norm": 0.20107300579547882,
      "learning_rate": 0.0003637545905938951,
      "loss": 2.1968,
      "step": 31340
    },
    {
      "epoch": 71.25,
      "grad_norm": 0.23209641873836517,
      "learning_rate": 0.00036367430508080277,
      "loss": 2.2033,
      "step": 31350
    },
    {
      "epoch": 71.27272727272727,
      "grad_norm": 0.24488529562950134,
      "learning_rate": 0.0003635940047859447,
      "loss": 2.1952,
      "step": 31360
    },
    {
      "epoch": 71.29545454545455,
      "grad_norm": 1.7944262027740479,
      "learning_rate": 0.00036351368971976295,
      "loss": 2.2121,
      "step": 31370
    },
    {
      "epoch": 71.31818181818181,
      "grad_norm": 0.23341429233551025,
      "learning_rate": 0.00036343335989270117,
      "loss": 2.1932,
      "step": 31380
    },
    {
      "epoch": 71.3409090909091,
      "grad_norm": 0.14564703404903412,
      "learning_rate": 0.0003633530153152052,
      "loss": 2.1911,
      "step": 31390
    },
    {
      "epoch": 71.36363636363636,
      "grad_norm": 0.132892444729805,
      "learning_rate": 0.0003632726559977228,
      "loss": 2.1976,
      "step": 31400
    },
    {
      "epoch": 71.38636363636364,
      "grad_norm": 0.18316692113876343,
      "learning_rate": 0.00036319228195070354,
      "loss": 2.2059,
      "step": 31410
    },
    {
      "epoch": 71.4090909090909,
      "grad_norm": 0.154888316988945,
      "learning_rate": 0.000363111893184599,
      "loss": 2.1939,
      "step": 31420
    },
    {
      "epoch": 71.43181818181819,
      "grad_norm": 0.14325721561908722,
      "learning_rate": 0.0003630314897098626,
      "loss": 2.1938,
      "step": 31430
    },
    {
      "epoch": 71.45454545454545,
      "grad_norm": 0.15459312498569489,
      "learning_rate": 0.0003629510715369497,
      "loss": 2.2012,
      "step": 31440
    },
    {
      "epoch": 71.47727272727273,
      "grad_norm": 0.15970316529273987,
      "learning_rate": 0.0003628706386763174,
      "loss": 2.2072,
      "step": 31450
    },
    {
      "epoch": 71.5,
      "grad_norm": 0.2007106989622116,
      "learning_rate": 0.0003627901911384252,
      "loss": 2.203,
      "step": 31460
    },
    {
      "epoch": 71.52272727272727,
      "grad_norm": 0.16075372695922852,
      "learning_rate": 0.0003627097289337339,
      "loss": 2.1916,
      "step": 31470
    },
    {
      "epoch": 71.54545454545455,
      "grad_norm": 0.14400088787078857,
      "learning_rate": 0.0003626292520727067,
      "loss": 2.1947,
      "step": 31480
    },
    {
      "epoch": 71.56818181818181,
      "grad_norm": 0.18442131578922272,
      "learning_rate": 0.0003625487605658084,
      "loss": 2.2013,
      "step": 31490
    },
    {
      "epoch": 71.5909090909091,
      "grad_norm": 0.18053320050239563,
      "learning_rate": 0.00036246825442350577,
      "loss": 2.1988,
      "step": 31500
    },
    {
      "epoch": 71.61363636363636,
      "grad_norm": 0.10828851908445358,
      "learning_rate": 0.0003623877336562676,
      "loss": 2.1959,
      "step": 31510
    },
    {
      "epoch": 71.63636363636364,
      "grad_norm": 0.13174830377101898,
      "learning_rate": 0.0003623071982745644,
      "loss": 2.2029,
      "step": 31520
    },
    {
      "epoch": 71.6590909090909,
      "grad_norm": 0.1350923776626587,
      "learning_rate": 0.00036222664828886877,
      "loss": 2.1986,
      "step": 31530
    },
    {
      "epoch": 71.68181818181819,
      "grad_norm": 0.13244351744651794,
      "learning_rate": 0.0003621460837096551,
      "loss": 2.1908,
      "step": 31540
    },
    {
      "epoch": 71.70454545454545,
      "grad_norm": 0.20809534192085266,
      "learning_rate": 0.00036206550454739964,
      "loss": 2.1958,
      "step": 31550
    },
    {
      "epoch": 71.72727272727273,
      "grad_norm": 0.15840771794319153,
      "learning_rate": 0.00036198491081258067,
      "loss": 2.2001,
      "step": 31560
    },
    {
      "epoch": 71.75,
      "grad_norm": 0.22433854639530182,
      "learning_rate": 0.0003619043025156781,
      "loss": 2.1924,
      "step": 31570
    },
    {
      "epoch": 71.77272727272727,
      "grad_norm": 0.13871654868125916,
      "learning_rate": 0.0003618236796671742,
      "loss": 2.203,
      "step": 31580
    },
    {
      "epoch": 71.79545454545455,
      "grad_norm": 0.14928536117076874,
      "learning_rate": 0.00036174304227755275,
      "loss": 2.2175,
      "step": 31590
    },
    {
      "epoch": 71.81818181818181,
      "grad_norm": 0.7475195527076721,
      "learning_rate": 0.00036166239035729944,
      "loss": 2.2046,
      "step": 31600
    },
    {
      "epoch": 71.8409090909091,
      "grad_norm": 0.21001340448856354,
      "learning_rate": 0.000361581723916902,
      "loss": 2.189,
      "step": 31610
    },
    {
      "epoch": 71.86363636363636,
      "grad_norm": 0.23653030395507812,
      "learning_rate": 0.00036150104296684993,
      "loss": 2.1963,
      "step": 31620
    },
    {
      "epoch": 71.88636363636364,
      "grad_norm": 0.12675321102142334,
      "learning_rate": 0.0003614203475176347,
      "loss": 2.2035,
      "step": 31630
    },
    {
      "epoch": 71.9090909090909,
      "grad_norm": 0.12506666779518127,
      "learning_rate": 0.0003613396375797497,
      "loss": 2.2054,
      "step": 31640
    },
    {
      "epoch": 71.93181818181819,
      "grad_norm": 0.1431242674589157,
      "learning_rate": 0.00036125891316369,
      "loss": 2.1948,
      "step": 31650
    },
    {
      "epoch": 71.95454545454545,
      "grad_norm": 0.20111003518104553,
      "learning_rate": 0.0003611781742799528,
      "loss": 2.1916,
      "step": 31660
    },
    {
      "epoch": 71.97727272727273,
      "grad_norm": 0.1267344057559967,
      "learning_rate": 0.00036109742093903705,
      "loss": 2.205,
      "step": 31670
    },
    {
      "epoch": 72.0,
      "grad_norm": 0.7929547429084778,
      "learning_rate": 0.00036101665315144355,
      "loss": 2.2017,
      "step": 31680
    },
    {
      "epoch": 72.0,
      "eval_loss": 1.0999454259872437,
      "eval_runtime": 8.7071,
      "eval_samples_per_second": 3494.946,
      "eval_steps_per_second": 13.667,
      "step": 31680
    },
    {
      "epoch": 72.02272727272727,
      "grad_norm": 0.10067339986562729,
      "learning_rate": 0.0003609358709276751,
      "loss": 2.1993,
      "step": 31690
    },
    {
      "epoch": 72.04545454545455,
      "grad_norm": 0.1943168193101883,
      "learning_rate": 0.0003608550742782362,
      "loss": 2.1949,
      "step": 31700
    },
    {
      "epoch": 72.06818181818181,
      "grad_norm": 0.159242182970047,
      "learning_rate": 0.00036077426321363346,
      "loss": 2.191,
      "step": 31710
    },
    {
      "epoch": 72.0909090909091,
      "grad_norm": 0.18769247829914093,
      "learning_rate": 0.0003606934377443752,
      "loss": 2.203,
      "step": 31720
    },
    {
      "epoch": 72.11363636363636,
      "grad_norm": 0.18471825122833252,
      "learning_rate": 0.00036061259788097163,
      "loss": 2.1958,
      "step": 31730
    },
    {
      "epoch": 72.13636363636364,
      "grad_norm": 0.13027553260326385,
      "learning_rate": 0.0003605317436339349,
      "loss": 2.1978,
      "step": 31740
    },
    {
      "epoch": 72.1590909090909,
      "grad_norm": 0.2006557285785675,
      "learning_rate": 0.0003604508750137789,
      "loss": 2.1929,
      "step": 31750
    },
    {
      "epoch": 72.18181818181819,
      "grad_norm": 0.15383371710777283,
      "learning_rate": 0.0003603699920310195,
      "loss": 2.2079,
      "step": 31760
    },
    {
      "epoch": 72.20454545454545,
      "grad_norm": 0.1476515233516693,
      "learning_rate": 0.0003602890946961743,
      "loss": 2.1835,
      "step": 31770
    },
    {
      "epoch": 72.22727272727273,
      "grad_norm": 0.11902357637882233,
      "learning_rate": 0.00036020818301976324,
      "loss": 2.2021,
      "step": 31780
    },
    {
      "epoch": 72.25,
      "grad_norm": 0.19602270424365997,
      "learning_rate": 0.00036012725701230734,
      "loss": 2.2072,
      "step": 31790
    },
    {
      "epoch": 72.27272727272727,
      "grad_norm": 0.2262011617422104,
      "learning_rate": 0.0003600463166843301,
      "loss": 2.1998,
      "step": 31800
    },
    {
      "epoch": 72.29545454545455,
      "grad_norm": 0.1860193908214569,
      "learning_rate": 0.0003599653620463568,
      "loss": 2.1997,
      "step": 31810
    },
    {
      "epoch": 72.31818181818181,
      "grad_norm": 0.17209245264530182,
      "learning_rate": 0.00035988439310891417,
      "loss": 2.1976,
      "step": 31820
    },
    {
      "epoch": 72.3409090909091,
      "grad_norm": 0.5049098134040833,
      "learning_rate": 0.00035980340988253136,
      "loss": 2.2034,
      "step": 31830
    },
    {
      "epoch": 72.36363636363636,
      "grad_norm": 0.14055989682674408,
      "learning_rate": 0.0003597224123777389,
      "loss": 2.1906,
      "step": 31840
    },
    {
      "epoch": 72.38636363636364,
      "grad_norm": 0.11986148357391357,
      "learning_rate": 0.00035964140060506963,
      "loss": 2.1981,
      "step": 31850
    },
    {
      "epoch": 72.4090909090909,
      "grad_norm": 0.28609347343444824,
      "learning_rate": 0.00035956037457505785,
      "loss": 2.1894,
      "step": 31860
    },
    {
      "epoch": 72.43181818181819,
      "grad_norm": 0.14323177933692932,
      "learning_rate": 0.00035947933429823987,
      "loss": 2.193,
      "step": 31870
    },
    {
      "epoch": 72.45454545454545,
      "grad_norm": 0.16884073615074158,
      "learning_rate": 0.00035939827978515395,
      "loss": 2.1875,
      "step": 31880
    },
    {
      "epoch": 72.47727272727273,
      "grad_norm": 0.29200857877731323,
      "learning_rate": 0.00035931721104633997,
      "loss": 2.189,
      "step": 31890
    },
    {
      "epoch": 72.5,
      "grad_norm": 0.26112622022628784,
      "learning_rate": 0.0003592361280923399,
      "loss": 2.1997,
      "step": 31900
    },
    {
      "epoch": 72.52272727272727,
      "grad_norm": 0.15236738324165344,
      "learning_rate": 0.0003591550309336974,
      "loss": 2.2062,
      "step": 31910
    },
    {
      "epoch": 72.54545454545455,
      "grad_norm": 0.18554964661598206,
      "learning_rate": 0.0003590739195809581,
      "loss": 2.2061,
      "step": 31920
    },
    {
      "epoch": 72.56818181818181,
      "grad_norm": 0.31225141882896423,
      "learning_rate": 0.00035899279404466935,
      "loss": 2.1956,
      "step": 31930
    },
    {
      "epoch": 72.5909090909091,
      "grad_norm": 0.1667257845401764,
      "learning_rate": 0.0003589116543353804,
      "loss": 2.1993,
      "step": 31940
    },
    {
      "epoch": 72.61363636363636,
      "grad_norm": 0.16316662728786469,
      "learning_rate": 0.00035883050046364224,
      "loss": 2.1929,
      "step": 31950
    },
    {
      "epoch": 72.63636363636364,
      "grad_norm": 0.15121109783649445,
      "learning_rate": 0.00035874933244000803,
      "loss": 2.1949,
      "step": 31960
    },
    {
      "epoch": 72.6590909090909,
      "grad_norm": 0.14455030858516693,
      "learning_rate": 0.0003586681502750324,
      "loss": 2.1946,
      "step": 31970
    },
    {
      "epoch": 72.68181818181819,
      "grad_norm": 0.14634652435779572,
      "learning_rate": 0.000358586953979272,
      "loss": 2.2007,
      "step": 31980
    },
    {
      "epoch": 72.70454545454545,
      "grad_norm": 0.31713271141052246,
      "learning_rate": 0.00035850574356328523,
      "loss": 2.2029,
      "step": 31990
    },
    {
      "epoch": 72.72727272727273,
      "grad_norm": 0.12845449149608612,
      "learning_rate": 0.0003584245190376324,
      "loss": 2.1955,
      "step": 32000
    },
    {
      "epoch": 72.75,
      "grad_norm": 0.17010557651519775,
      "learning_rate": 0.0003583432804128757,
      "loss": 2.1944,
      "step": 32010
    },
    {
      "epoch": 72.77272727272727,
      "grad_norm": 0.139534592628479,
      "learning_rate": 0.00035826202769957905,
      "loss": 2.2009,
      "step": 32020
    },
    {
      "epoch": 72.79545454545455,
      "grad_norm": 0.15313443541526794,
      "learning_rate": 0.0003581807609083082,
      "loss": 2.1912,
      "step": 32030
    },
    {
      "epoch": 72.81818181818181,
      "grad_norm": 0.21089449524879456,
      "learning_rate": 0.0003580994800496308,
      "loss": 2.1964,
      "step": 32040
    },
    {
      "epoch": 72.8409090909091,
      "grad_norm": 0.3317314684391022,
      "learning_rate": 0.0003580181851341163,
      "loss": 2.2072,
      "step": 32050
    },
    {
      "epoch": 72.86363636363636,
      "grad_norm": 0.1367441713809967,
      "learning_rate": 0.000357936876172336,
      "loss": 2.2061,
      "step": 32060
    },
    {
      "epoch": 72.88636363636364,
      "grad_norm": 0.12463167309761047,
      "learning_rate": 0.000357855553174863,
      "loss": 2.1927,
      "step": 32070
    },
    {
      "epoch": 72.9090909090909,
      "grad_norm": 0.4524343013763428,
      "learning_rate": 0.0003577742161522721,
      "loss": 2.1969,
      "step": 32080
    },
    {
      "epoch": 72.93181818181819,
      "grad_norm": 0.20607371628284454,
      "learning_rate": 0.0003576928651151402,
      "loss": 2.2037,
      "step": 32090
    },
    {
      "epoch": 72.95454545454545,
      "grad_norm": 0.14572228491306305,
      "learning_rate": 0.00035761150007404573,
      "loss": 2.2025,
      "step": 32100
    },
    {
      "epoch": 72.97727272727273,
      "grad_norm": 0.28555935621261597,
      "learning_rate": 0.00035753012103956926,
      "loss": 2.1851,
      "step": 32110
    },
    {
      "epoch": 73.0,
      "grad_norm": 0.3307974934577942,
      "learning_rate": 0.00035744872802229296,
      "loss": 2.2074,
      "step": 32120
    },
    {
      "epoch": 73.0,
      "eval_loss": 1.0994428396224976,
      "eval_runtime": 8.7369,
      "eval_samples_per_second": 3483.031,
      "eval_steps_per_second": 13.62,
      "step": 32120
    },
    {
      "epoch": 73.02272727272727,
      "grad_norm": 0.1757580190896988,
      "learning_rate": 0.00035736732103280075,
      "loss": 2.1953,
      "step": 32130
    },
    {
      "epoch": 73.04545454545455,
      "grad_norm": 0.12859217822551727,
      "learning_rate": 0.00035728590008167856,
      "loss": 2.2124,
      "step": 32140
    },
    {
      "epoch": 73.06818181818181,
      "grad_norm": 0.13372719287872314,
      "learning_rate": 0.000357204465179514,
      "loss": 2.199,
      "step": 32150
    },
    {
      "epoch": 73.0909090909091,
      "grad_norm": 0.12928850948810577,
      "learning_rate": 0.00035712301633689667,
      "loss": 2.1843,
      "step": 32160
    },
    {
      "epoch": 73.11363636363636,
      "grad_norm": 0.16115590929985046,
      "learning_rate": 0.0003570415535644178,
      "loss": 2.1897,
      "step": 32170
    },
    {
      "epoch": 73.13636363636364,
      "grad_norm": 0.15967091917991638,
      "learning_rate": 0.0003569600768726704,
      "loss": 2.1969,
      "step": 32180
    },
    {
      "epoch": 73.1590909090909,
      "grad_norm": 0.1777348667383194,
      "learning_rate": 0.0003568785862722496,
      "loss": 2.1873,
      "step": 32190
    },
    {
      "epoch": 73.18181818181819,
      "grad_norm": 0.1630595624446869,
      "learning_rate": 0.0003567970817737518,
      "loss": 2.2015,
      "step": 32200
    },
    {
      "epoch": 73.20454545454545,
      "grad_norm": 0.14607910811901093,
      "learning_rate": 0.0003567155633877758,
      "loss": 2.1952,
      "step": 32210
    },
    {
      "epoch": 73.22727272727273,
      "grad_norm": 0.14146378636360168,
      "learning_rate": 0.00035663403112492185,
      "loss": 2.2034,
      "step": 32220
    },
    {
      "epoch": 73.25,
      "grad_norm": 0.13126763701438904,
      "learning_rate": 0.00035655248499579206,
      "loss": 2.1941,
      "step": 32230
    },
    {
      "epoch": 73.27272727272727,
      "grad_norm": 0.16610689461231232,
      "learning_rate": 0.00035647092501099045,
      "loss": 2.1957,
      "step": 32240
    },
    {
      "epoch": 73.29545454545455,
      "grad_norm": 0.32460343837738037,
      "learning_rate": 0.0003563893511811226,
      "loss": 2.2021,
      "step": 32250
    },
    {
      "epoch": 73.31818181818181,
      "grad_norm": 0.37910905480384827,
      "learning_rate": 0.0003563077635167963,
      "loss": 2.2016,
      "step": 32260
    },
    {
      "epoch": 73.3409090909091,
      "grad_norm": 0.2998301684856415,
      "learning_rate": 0.00035622616202862073,
      "loss": 2.2082,
      "step": 32270
    },
    {
      "epoch": 73.36363636363636,
      "grad_norm": 0.17313456535339355,
      "learning_rate": 0.000356144546727207,
      "loss": 2.1983,
      "step": 32280
    },
    {
      "epoch": 73.38636363636364,
      "grad_norm": 0.1761922836303711,
      "learning_rate": 0.0003560629176231682,
      "loss": 2.1968,
      "step": 32290
    },
    {
      "epoch": 73.4090909090909,
      "grad_norm": 0.3998013734817505,
      "learning_rate": 0.0003559812747271189,
      "loss": 2.1984,
      "step": 32300
    },
    {
      "epoch": 73.43181818181819,
      "grad_norm": 0.13472720980644226,
      "learning_rate": 0.00035589961804967574,
      "loss": 2.1968,
      "step": 32310
    },
    {
      "epoch": 73.45454545454545,
      "grad_norm": 0.3347858786582947,
      "learning_rate": 0.00035581794760145696,
      "loss": 2.185,
      "step": 32320
    },
    {
      "epoch": 73.47727272727273,
      "grad_norm": 0.34337344765663147,
      "learning_rate": 0.0003557362633930827,
      "loss": 2.1937,
      "step": 32330
    },
    {
      "epoch": 73.5,
      "grad_norm": 0.146736741065979,
      "learning_rate": 0.00035565456543517487,
      "loss": 2.1968,
      "step": 32340
    },
    {
      "epoch": 73.52272727272727,
      "grad_norm": 0.27012309432029724,
      "learning_rate": 0.00035557285373835715,
      "loss": 2.1944,
      "step": 32350
    },
    {
      "epoch": 73.54545454545455,
      "grad_norm": 0.10040536522865295,
      "learning_rate": 0.00035549112831325503,
      "loss": 2.2004,
      "step": 32360
    },
    {
      "epoch": 73.56818181818181,
      "grad_norm": 0.21929794549942017,
      "learning_rate": 0.0003554093891704957,
      "loss": 2.1889,
      "step": 32370
    },
    {
      "epoch": 73.5909090909091,
      "grad_norm": 0.1355215609073639,
      "learning_rate": 0.00035532763632070826,
      "loss": 2.1882,
      "step": 32380
    },
    {
      "epoch": 73.61363636363636,
      "grad_norm": 0.18875958025455475,
      "learning_rate": 0.0003552458697745236,
      "loss": 2.1955,
      "step": 32390
    },
    {
      "epoch": 73.63636363636364,
      "grad_norm": 0.4692002534866333,
      "learning_rate": 0.00035516408954257414,
      "loss": 2.1915,
      "step": 32400
    },
    {
      "epoch": 73.6590909090909,
      "grad_norm": 0.22457732260227203,
      "learning_rate": 0.00035508229563549433,
      "loss": 2.2119,
      "step": 32410
    },
    {
      "epoch": 73.68181818181819,
      "grad_norm": 0.2301958203315735,
      "learning_rate": 0.0003550004880639205,
      "loss": 2.204,
      "step": 32420
    },
    {
      "epoch": 73.70454545454545,
      "grad_norm": 0.19212450087070465,
      "learning_rate": 0.0003549186668384903,
      "loss": 2.1909,
      "step": 32430
    },
    {
      "epoch": 73.72727272727273,
      "grad_norm": 0.22381414473056793,
      "learning_rate": 0.00035483683196984374,
      "loss": 2.1927,
      "step": 32440
    },
    {
      "epoch": 73.75,
      "grad_norm": 0.1140206977725029,
      "learning_rate": 0.00035475498346862217,
      "loss": 2.2012,
      "step": 32450
    },
    {
      "epoch": 73.77272727272727,
      "grad_norm": 0.15053194761276245,
      "learning_rate": 0.0003546731213454688,
      "loss": 2.2012,
      "step": 32460
    },
    {
      "epoch": 73.79545454545455,
      "grad_norm": 0.12580814957618713,
      "learning_rate": 0.0003545912456110287,
      "loss": 2.2015,
      "step": 32470
    },
    {
      "epoch": 73.81818181818181,
      "grad_norm": 0.2756812274456024,
      "learning_rate": 0.00035450935627594877,
      "loss": 2.1976,
      "step": 32480
    },
    {
      "epoch": 73.8409090909091,
      "grad_norm": 0.12103505432605743,
      "learning_rate": 0.00035442745335087744,
      "loss": 2.1926,
      "step": 32490
    },
    {
      "epoch": 73.86363636363636,
      "grad_norm": 0.15685150027275085,
      "learning_rate": 0.0003543455368464652,
      "loss": 2.1922,
      "step": 32500
    },
    {
      "epoch": 73.88636363636364,
      "grad_norm": 0.1824832260608673,
      "learning_rate": 0.00035426360677336393,
      "loss": 2.1978,
      "step": 32510
    },
    {
      "epoch": 73.9090909090909,
      "grad_norm": 0.1191529780626297,
      "learning_rate": 0.0003541816631422277,
      "loss": 2.1918,
      "step": 32520
    },
    {
      "epoch": 73.93181818181819,
      "grad_norm": 0.1297568529844284,
      "learning_rate": 0.0003540997059637121,
      "loss": 2.2009,
      "step": 32530
    },
    {
      "epoch": 73.95454545454545,
      "grad_norm": 0.2076001912355423,
      "learning_rate": 0.00035401773524847455,
      "loss": 2.197,
      "step": 32540
    },
    {
      "epoch": 73.97727272727273,
      "grad_norm": 0.15281015634536743,
      "learning_rate": 0.0003539357510071741,
      "loss": 2.2106,
      "step": 32550
    },
    {
      "epoch": 74.0,
      "grad_norm": 0.3469758629798889,
      "learning_rate": 0.00035385375325047166,
      "loss": 2.2082,
      "step": 32560
    },
    {
      "epoch": 74.0,
      "eval_loss": 1.0996490716934204,
      "eval_runtime": 8.9529,
      "eval_samples_per_second": 3399.007,
      "eval_steps_per_second": 13.292,
      "step": 32560
    },
    {
      "epoch": 74.02272727272727,
      "grad_norm": 0.1906566023826599,
      "learning_rate": 0.00035377174198902996,
      "loss": 2.195,
      "step": 32570
    },
    {
      "epoch": 74.04545454545455,
      "grad_norm": 0.12907223403453827,
      "learning_rate": 0.00035368971723351343,
      "loss": 2.197,
      "step": 32580
    },
    {
      "epoch": 74.06818181818181,
      "grad_norm": 0.13134823739528656,
      "learning_rate": 0.0003536076789945883,
      "loss": 2.188,
      "step": 32590
    },
    {
      "epoch": 74.0909090909091,
      "grad_norm": 0.12545862793922424,
      "learning_rate": 0.0003535256272829224,
      "loss": 2.1914,
      "step": 32600
    },
    {
      "epoch": 74.11363636363636,
      "grad_norm": 0.42472147941589355,
      "learning_rate": 0.00035344356210918553,
      "loss": 2.1889,
      "step": 32610
    },
    {
      "epoch": 74.13636363636364,
      "grad_norm": 0.1409222036600113,
      "learning_rate": 0.0003533614834840491,
      "loss": 2.1976,
      "step": 32620
    },
    {
      "epoch": 74.1590909090909,
      "grad_norm": 0.19424866139888763,
      "learning_rate": 0.00035327939141818613,
      "loss": 2.1931,
      "step": 32630
    },
    {
      "epoch": 74.18181818181819,
      "grad_norm": 0.1562526524066925,
      "learning_rate": 0.00035319728592227167,
      "loss": 2.1965,
      "step": 32640
    },
    {
      "epoch": 74.20454545454545,
      "grad_norm": 0.394375205039978,
      "learning_rate": 0.00035311516700698255,
      "loss": 2.1985,
      "step": 32650
    },
    {
      "epoch": 74.22727272727273,
      "grad_norm": 0.23250582814216614,
      "learning_rate": 0.00035303303468299697,
      "loss": 2.201,
      "step": 32660
    },
    {
      "epoch": 74.25,
      "grad_norm": 0.49080532789230347,
      "learning_rate": 0.0003529508889609952,
      "loss": 2.1977,
      "step": 32670
    },
    {
      "epoch": 74.27272727272727,
      "grad_norm": 0.1638721078634262,
      "learning_rate": 0.0003528687298516591,
      "loss": 2.1956,
      "step": 32680
    },
    {
      "epoch": 74.29545454545455,
      "grad_norm": 0.33509981632232666,
      "learning_rate": 0.00035278655736567245,
      "loss": 2.2012,
      "step": 32690
    },
    {
      "epoch": 74.31818181818181,
      "grad_norm": 0.13958051800727844,
      "learning_rate": 0.00035270437151372047,
      "loss": 2.2012,
      "step": 32700
    },
    {
      "epoch": 74.3409090909091,
      "grad_norm": 0.2172749638557434,
      "learning_rate": 0.00035262217230649034,
      "loss": 2.1943,
      "step": 32710
    },
    {
      "epoch": 74.36363636363636,
      "grad_norm": 0.11552882939577103,
      "learning_rate": 0.000352539959754671,
      "loss": 2.1888,
      "step": 32720
    },
    {
      "epoch": 74.38636363636364,
      "grad_norm": 0.45412540435791016,
      "learning_rate": 0.0003524577338689531,
      "loss": 2.1929,
      "step": 32730
    },
    {
      "epoch": 74.4090909090909,
      "grad_norm": 0.25899454951286316,
      "learning_rate": 0.00035237549466002885,
      "loss": 2.181,
      "step": 32740
    },
    {
      "epoch": 74.43181818181819,
      "grad_norm": 0.26241743564605713,
      "learning_rate": 0.00035229324213859227,
      "loss": 2.1927,
      "step": 32750
    },
    {
      "epoch": 74.45454545454545,
      "grad_norm": 0.2693856656551361,
      "learning_rate": 0.0003522109763153392,
      "loss": 2.1943,
      "step": 32760
    },
    {
      "epoch": 74.47727272727273,
      "grad_norm": 0.11033140122890472,
      "learning_rate": 0.0003521286972009673,
      "loss": 2.1984,
      "step": 32770
    },
    {
      "epoch": 74.5,
      "grad_norm": 0.1631891280412674,
      "learning_rate": 0.0003520464048061758,
      "loss": 2.2073,
      "step": 32780
    },
    {
      "epoch": 74.52272727272727,
      "grad_norm": 0.20265360176563263,
      "learning_rate": 0.0003519640991416655,
      "loss": 2.212,
      "step": 32790
    },
    {
      "epoch": 74.54545454545455,
      "grad_norm": 0.1415031999349594,
      "learning_rate": 0.00035188178021813924,
      "loss": 2.2032,
      "step": 32800
    },
    {
      "epoch": 74.56818181818181,
      "grad_norm": 0.1524353325366974,
      "learning_rate": 0.0003517994480463015,
      "loss": 2.1857,
      "step": 32810
    },
    {
      "epoch": 74.5909090909091,
      "grad_norm": 0.12495549023151398,
      "learning_rate": 0.0003517171026368583,
      "loss": 2.203,
      "step": 32820
    },
    {
      "epoch": 74.61363636363636,
      "grad_norm": 0.1154157742857933,
      "learning_rate": 0.00035163474400051764,
      "loss": 2.199,
      "step": 32830
    },
    {
      "epoch": 74.63636363636364,
      "grad_norm": 0.4169781804084778,
      "learning_rate": 0.00035155237214798906,
      "loss": 2.1969,
      "step": 32840
    },
    {
      "epoch": 74.6590909090909,
      "grad_norm": 0.1938711404800415,
      "learning_rate": 0.000351469987089984,
      "loss": 2.1887,
      "step": 32850
    },
    {
      "epoch": 74.68181818181819,
      "grad_norm": 0.1531301885843277,
      "learning_rate": 0.0003513875888372152,
      "loss": 2.1972,
      "step": 32860
    },
    {
      "epoch": 74.70454545454545,
      "grad_norm": 0.2563149034976959,
      "learning_rate": 0.0003513051774003977,
      "loss": 2.1982,
      "step": 32870
    },
    {
      "epoch": 74.72727272727273,
      "grad_norm": 0.15605862438678741,
      "learning_rate": 0.00035122275279024783,
      "loss": 2.2,
      "step": 32880
    },
    {
      "epoch": 74.75,
      "grad_norm": 0.1750013679265976,
      "learning_rate": 0.0003511403150174838,
      "loss": 2.2002,
      "step": 32890
    },
    {
      "epoch": 74.77272727272727,
      "grad_norm": 0.1053600087761879,
      "learning_rate": 0.0003510578640928255,
      "loss": 2.199,
      "step": 32900
    },
    {
      "epoch": 74.79545454545455,
      "grad_norm": 0.11791671812534332,
      "learning_rate": 0.0003509754000269945,
      "loss": 2.2091,
      "step": 32910
    },
    {
      "epoch": 74.81818181818181,
      "grad_norm": 0.21879220008850098,
      "learning_rate": 0.0003508929228307142,
      "loss": 2.2116,
      "step": 32920
    },
    {
      "epoch": 74.8409090909091,
      "grad_norm": 0.14734520018100739,
      "learning_rate": 0.00035081043251470944,
      "loss": 2.1937,
      "step": 32930
    },
    {
      "epoch": 74.86363636363636,
      "grad_norm": 0.13385526835918427,
      "learning_rate": 0.0003507279290897071,
      "loss": 2.1957,
      "step": 32940
    },
    {
      "epoch": 74.88636363636364,
      "grad_norm": 0.17265313863754272,
      "learning_rate": 0.00035064541256643546,
      "loss": 2.1992,
      "step": 32950
    },
    {
      "epoch": 74.9090909090909,
      "grad_norm": 0.2605728209018707,
      "learning_rate": 0.00035056288295562475,
      "loss": 2.1961,
      "step": 32960
    },
    {
      "epoch": 74.93181818181819,
      "grad_norm": 0.1433340162038803,
      "learning_rate": 0.0003504803402680069,
      "loss": 2.1976,
      "step": 32970
    },
    {
      "epoch": 74.95454545454545,
      "grad_norm": 0.4758528769016266,
      "learning_rate": 0.0003503977845143153,
      "loss": 2.2041,
      "step": 32980
    },
    {
      "epoch": 74.97727272727273,
      "grad_norm": 0.13269884884357452,
      "learning_rate": 0.00035031521570528515,
      "loss": 2.1842,
      "step": 32990
    },
    {
      "epoch": 75.0,
      "grad_norm": 0.24776196479797363,
      "learning_rate": 0.0003502326338516534,
      "loss": 2.1818,
      "step": 33000
    },
    {
      "epoch": 75.0,
      "eval_loss": 1.09931480884552,
      "eval_runtime": 8.7077,
      "eval_samples_per_second": 3494.738,
      "eval_steps_per_second": 13.666,
      "step": 33000
    },
    {
      "epoch": 75.02272727272727,
      "grad_norm": 0.21848450601100922,
      "learning_rate": 0.00035015003896415875,
      "loss": 2.1871,
      "step": 33010
    },
    {
      "epoch": 75.04545454545455,
      "grad_norm": 0.17886343598365784,
      "learning_rate": 0.00035006743105354157,
      "loss": 2.1879,
      "step": 33020
    },
    {
      "epoch": 75.06818181818181,
      "grad_norm": 0.16673564910888672,
      "learning_rate": 0.00034998481013054367,
      "loss": 2.1988,
      "step": 33030
    },
    {
      "epoch": 75.0909090909091,
      "grad_norm": 0.5626837015151978,
      "learning_rate": 0.00034990217620590894,
      "loss": 2.1908,
      "step": 33040
    },
    {
      "epoch": 75.11363636363636,
      "grad_norm": 0.2830633521080017,
      "learning_rate": 0.00034981952929038265,
      "loss": 2.1917,
      "step": 33050
    },
    {
      "epoch": 75.13636363636364,
      "grad_norm": 0.12141743302345276,
      "learning_rate": 0.0003497368693947119,
      "loss": 2.1997,
      "step": 33060
    },
    {
      "epoch": 75.1590909090909,
      "grad_norm": 0.13992559909820557,
      "learning_rate": 0.00034965419652964555,
      "loss": 2.1973,
      "step": 33070
    },
    {
      "epoch": 75.18181818181819,
      "grad_norm": 0.1188652291893959,
      "learning_rate": 0.00034957151070593394,
      "loss": 2.1951,
      "step": 33080
    },
    {
      "epoch": 75.20454545454545,
      "grad_norm": 0.16007842123508453,
      "learning_rate": 0.00034948881193432935,
      "loss": 2.1902,
      "step": 33090
    },
    {
      "epoch": 75.22727272727273,
      "grad_norm": 0.1667417734861374,
      "learning_rate": 0.00034940610022558556,
      "loss": 2.1866,
      "step": 33100
    },
    {
      "epoch": 75.25,
      "grad_norm": 0.18581664562225342,
      "learning_rate": 0.00034932337559045795,
      "loss": 2.1954,
      "step": 33110
    },
    {
      "epoch": 75.27272727272727,
      "grad_norm": 0.26209983229637146,
      "learning_rate": 0.0003492406380397039,
      "loss": 2.1982,
      "step": 33120
    },
    {
      "epoch": 75.29545454545455,
      "grad_norm": 0.21920283138751984,
      "learning_rate": 0.0003491578875840822,
      "loss": 2.2095,
      "step": 33130
    },
    {
      "epoch": 75.31818181818181,
      "grad_norm": 0.16822510957717896,
      "learning_rate": 0.0003490751242343533,
      "loss": 2.2066,
      "step": 33140
    },
    {
      "epoch": 75.3409090909091,
      "grad_norm": 0.6163597106933594,
      "learning_rate": 0.0003489923480012796,
      "loss": 2.1916,
      "step": 33150
    },
    {
      "epoch": 75.36363636363636,
      "grad_norm": 0.18653684854507446,
      "learning_rate": 0.00034890955889562493,
      "loss": 2.194,
      "step": 33160
    },
    {
      "epoch": 75.38636363636364,
      "grad_norm": 0.16723071038722992,
      "learning_rate": 0.00034882675692815486,
      "loss": 2.2047,
      "step": 33170
    },
    {
      "epoch": 75.4090909090909,
      "grad_norm": 0.14021986722946167,
      "learning_rate": 0.0003487439421096366,
      "loss": 2.2017,
      "step": 33180
    },
    {
      "epoch": 75.43181818181819,
      "grad_norm": 0.17304395139217377,
      "learning_rate": 0.00034866111445083905,
      "loss": 2.1972,
      "step": 33190
    },
    {
      "epoch": 75.45454545454545,
      "grad_norm": 0.12819340825080872,
      "learning_rate": 0.00034857827396253294,
      "loss": 2.1835,
      "step": 33200
    },
    {
      "epoch": 75.47727272727273,
      "grad_norm": 0.17021024227142334,
      "learning_rate": 0.0003484954206554904,
      "loss": 2.197,
      "step": 33210
    },
    {
      "epoch": 75.5,
      "grad_norm": 0.28469353914260864,
      "learning_rate": 0.00034841255454048535,
      "loss": 2.1957,
      "step": 33220
    },
    {
      "epoch": 75.52272727272727,
      "grad_norm": 0.3201523423194885,
      "learning_rate": 0.00034832967562829347,
      "loss": 2.2,
      "step": 33230
    },
    {
      "epoch": 75.54545454545455,
      "grad_norm": 0.1678634136915207,
      "learning_rate": 0.0003482467839296919,
      "loss": 2.1848,
      "step": 33240
    },
    {
      "epoch": 75.56818181818181,
      "grad_norm": 0.31340134143829346,
      "learning_rate": 0.00034816387945545965,
      "loss": 2.1948,
      "step": 33250
    },
    {
      "epoch": 75.5909090909091,
      "grad_norm": 0.11469555646181107,
      "learning_rate": 0.0003480809622163772,
      "loss": 2.1931,
      "step": 33260
    },
    {
      "epoch": 75.61363636363636,
      "grad_norm": 0.19699442386627197,
      "learning_rate": 0.0003479980322232269,
      "loss": 2.2089,
      "step": 33270
    },
    {
      "epoch": 75.63636363636364,
      "grad_norm": 0.5010890960693359,
      "learning_rate": 0.0003479150894867926,
      "loss": 2.201,
      "step": 33280
    },
    {
      "epoch": 75.6590909090909,
      "grad_norm": 0.14397406578063965,
      "learning_rate": 0.0003478321340178598,
      "loss": 2.2026,
      "step": 33290
    },
    {
      "epoch": 75.68181818181819,
      "grad_norm": 0.14923301339149475,
      "learning_rate": 0.00034774916582721574,
      "loss": 2.1975,
      "step": 33300
    },
    {
      "epoch": 75.70454545454545,
      "grad_norm": 0.13127218186855316,
      "learning_rate": 0.0003476661849256493,
      "loss": 2.2013,
      "step": 33310
    },
    {
      "epoch": 75.72727272727273,
      "grad_norm": 0.13111291825771332,
      "learning_rate": 0.000347583191323951,
      "loss": 2.2072,
      "step": 33320
    },
    {
      "epoch": 75.75,
      "grad_norm": 0.136016383767128,
      "learning_rate": 0.00034750018503291297,
      "loss": 2.2002,
      "step": 33330
    },
    {
      "epoch": 75.77272727272727,
      "grad_norm": 0.38513192534446716,
      "learning_rate": 0.0003474171660633291,
      "loss": 2.21,
      "step": 33340
    },
    {
      "epoch": 75.79545454545455,
      "grad_norm": 0.13228146731853485,
      "learning_rate": 0.00034733413442599473,
      "loss": 2.1876,
      "step": 33350
    },
    {
      "epoch": 75.81818181818181,
      "grad_norm": 0.165791317820549,
      "learning_rate": 0.0003472510901317071,
      "loss": 2.1984,
      "step": 33360
    },
    {
      "epoch": 75.8409090909091,
      "grad_norm": 0.2799180746078491,
      "learning_rate": 0.00034716803319126475,
      "loss": 2.1849,
      "step": 33370
    },
    {
      "epoch": 75.86363636363636,
      "grad_norm": 0.13242654502391815,
      "learning_rate": 0.00034708496361546845,
      "loss": 2.2059,
      "step": 33380
    },
    {
      "epoch": 75.88636363636364,
      "grad_norm": 0.16690433025360107,
      "learning_rate": 0.0003470018814151199,
      "loss": 2.1919,
      "step": 33390
    },
    {
      "epoch": 75.9090909090909,
      "grad_norm": 0.388138085603714,
      "learning_rate": 0.0003469187866010231,
      "loss": 2.1945,
      "step": 33400
    },
    {
      "epoch": 75.93181818181819,
      "grad_norm": 0.2046222984790802,
      "learning_rate": 0.000346835679183983,
      "loss": 2.1996,
      "step": 33410
    },
    {
      "epoch": 75.95454545454545,
      "grad_norm": 0.15794990956783295,
      "learning_rate": 0.0003467525591748068,
      "loss": 2.1947,
      "step": 33420
    },
    {
      "epoch": 75.97727272727273,
      "grad_norm": 0.2063208669424057,
      "learning_rate": 0.00034666942658430317,
      "loss": 2.1908,
      "step": 33430
    },
    {
      "epoch": 76.0,
      "grad_norm": 0.21347615122795105,
      "learning_rate": 0.00034658628142328216,
      "loss": 2.196,
      "step": 33440
    },
    {
      "epoch": 76.0,
      "eval_loss": 1.0994784832000732,
      "eval_runtime": 8.8712,
      "eval_samples_per_second": 3430.303,
      "eval_steps_per_second": 13.414,
      "step": 33440
    },
    {
      "epoch": 76.02272727272727,
      "grad_norm": 0.21940504014492035,
      "learning_rate": 0.00034650312370255575,
      "loss": 2.1941,
      "step": 33450
    },
    {
      "epoch": 76.04545454545455,
      "grad_norm": 0.2158844918012619,
      "learning_rate": 0.0003464199534329375,
      "loss": 2.197,
      "step": 33460
    },
    {
      "epoch": 76.06818181818181,
      "grad_norm": 0.1181345134973526,
      "learning_rate": 0.0003463367706252424,
      "loss": 2.1824,
      "step": 33470
    },
    {
      "epoch": 76.0909090909091,
      "grad_norm": 0.21305882930755615,
      "learning_rate": 0.0003462535752902874,
      "loss": 2.1838,
      "step": 33480
    },
    {
      "epoch": 76.11363636363636,
      "grad_norm": 0.20567595958709717,
      "learning_rate": 0.00034617036743889076,
      "loss": 2.2025,
      "step": 33490
    },
    {
      "epoch": 76.13636363636364,
      "grad_norm": 0.17846150696277618,
      "learning_rate": 0.0003460871470818725,
      "loss": 2.1929,
      "step": 33500
    },
    {
      "epoch": 76.1590909090909,
      "grad_norm": 0.1597880721092224,
      "learning_rate": 0.0003460039142300544,
      "loss": 2.196,
      "step": 33510
    },
    {
      "epoch": 76.18181818181819,
      "grad_norm": 0.13583141565322876,
      "learning_rate": 0.0003459206688942596,
      "loss": 2.1882,
      "step": 33520
    },
    {
      "epoch": 76.20454545454545,
      "grad_norm": 0.21262991428375244,
      "learning_rate": 0.0003458374110853131,
      "loss": 2.1903,
      "step": 33530
    },
    {
      "epoch": 76.22727272727273,
      "grad_norm": 0.20359548926353455,
      "learning_rate": 0.0003457541408140413,
      "loss": 2.1978,
      "step": 33540
    },
    {
      "epoch": 76.25,
      "grad_norm": 0.2266531139612198,
      "learning_rate": 0.0003456708580912725,
      "loss": 2.1927,
      "step": 33550
    },
    {
      "epoch": 76.27272727272727,
      "grad_norm": 0.1653016358613968,
      "learning_rate": 0.00034558756292783635,
      "loss": 2.1939,
      "step": 33560
    },
    {
      "epoch": 76.29545454545455,
      "grad_norm": 0.21443374454975128,
      "learning_rate": 0.0003455042553345642,
      "loss": 2.1993,
      "step": 33570
    },
    {
      "epoch": 76.31818181818181,
      "grad_norm": 0.21893559396266937,
      "learning_rate": 0.00034542093532228917,
      "loss": 2.1965,
      "step": 33580
    },
    {
      "epoch": 76.3409090909091,
      "grad_norm": 0.14526760578155518,
      "learning_rate": 0.0003453376029018459,
      "loss": 2.1984,
      "step": 33590
    },
    {
      "epoch": 76.36363636363636,
      "grad_norm": 0.26341018080711365,
      "learning_rate": 0.0003452542580840704,
      "loss": 2.1959,
      "step": 33600
    },
    {
      "epoch": 76.38636363636364,
      "grad_norm": 0.19975048303604126,
      "learning_rate": 0.0003451709008798007,
      "loss": 2.2019,
      "step": 33610
    },
    {
      "epoch": 76.4090909090909,
      "grad_norm": 0.2526552379131317,
      "learning_rate": 0.0003450875312998761,
      "loss": 2.2,
      "step": 33620
    },
    {
      "epoch": 76.43181818181819,
      "grad_norm": 0.10279282927513123,
      "learning_rate": 0.0003450041493551379,
      "loss": 2.2081,
      "step": 33630
    },
    {
      "epoch": 76.45454545454545,
      "grad_norm": 0.1435042917728424,
      "learning_rate": 0.0003449207550564285,
      "loss": 2.1894,
      "step": 33640
    },
    {
      "epoch": 76.47727272727273,
      "grad_norm": 0.36434999108314514,
      "learning_rate": 0.0003448373484145923,
      "loss": 2.1961,
      "step": 33650
    },
    {
      "epoch": 76.5,
      "grad_norm": 0.1474657952785492,
      "learning_rate": 0.0003447539294404751,
      "loss": 2.2043,
      "step": 33660
    },
    {
      "epoch": 76.52272727272727,
      "grad_norm": 0.16638195514678955,
      "learning_rate": 0.00034467049814492444,
      "loss": 2.2,
      "step": 33670
    },
    {
      "epoch": 76.54545454545455,
      "grad_norm": 0.6090824604034424,
      "learning_rate": 0.0003445870545387895,
      "loss": 2.197,
      "step": 33680
    },
    {
      "epoch": 76.56818181818181,
      "grad_norm": 0.41262879967689514,
      "learning_rate": 0.0003445035986329207,
      "loss": 2.1941,
      "step": 33690
    },
    {
      "epoch": 76.5909090909091,
      "grad_norm": 0.1706685870885849,
      "learning_rate": 0.00034442013043817053,
      "loss": 2.185,
      "step": 33700
    },
    {
      "epoch": 76.61363636363636,
      "grad_norm": 0.16527439653873444,
      "learning_rate": 0.0003443366499653929,
      "loss": 2.2014,
      "step": 33710
    },
    {
      "epoch": 76.63636363636364,
      "grad_norm": 0.17781871557235718,
      "learning_rate": 0.00034425315722544315,
      "loss": 2.1953,
      "step": 33720
    },
    {
      "epoch": 76.6590909090909,
      "grad_norm": 0.34863734245300293,
      "learning_rate": 0.0003441696522291784,
      "loss": 2.1977,
      "step": 33730
    },
    {
      "epoch": 76.68181818181819,
      "grad_norm": 0.17190702259540558,
      "learning_rate": 0.0003440861349874573,
      "loss": 2.1876,
      "step": 33740
    },
    {
      "epoch": 76.70454545454545,
      "grad_norm": 0.1583636999130249,
      "learning_rate": 0.00034400260551114013,
      "loss": 2.1915,
      "step": 33750
    },
    {
      "epoch": 76.72727272727273,
      "grad_norm": 0.14563943445682526,
      "learning_rate": 0.00034391906381108883,
      "loss": 2.2085,
      "step": 33760
    },
    {
      "epoch": 76.75,
      "grad_norm": 0.14048007130622864,
      "learning_rate": 0.0003438355098981667,
      "loss": 2.1923,
      "step": 33770
    },
    {
      "epoch": 76.77272727272727,
      "grad_norm": 0.24878552556037903,
      "learning_rate": 0.00034375194378323884,
      "loss": 2.1907,
      "step": 33780
    },
    {
      "epoch": 76.79545454545455,
      "grad_norm": 0.19307532906532288,
      "learning_rate": 0.0003436683654771718,
      "loss": 2.1965,
      "step": 33790
    },
    {
      "epoch": 76.81818181818181,
      "grad_norm": 0.19513878226280212,
      "learning_rate": 0.00034358477499083374,
      "loss": 2.211,
      "step": 33800
    },
    {
      "epoch": 76.8409090909091,
      "grad_norm": 0.1726873368024826,
      "learning_rate": 0.00034350117233509467,
      "loss": 2.1967,
      "step": 33810
    },
    {
      "epoch": 76.86363636363636,
      "grad_norm": 0.24458390474319458,
      "learning_rate": 0.00034341755752082567,
      "loss": 2.1972,
      "step": 33820
    },
    {
      "epoch": 76.88636363636364,
      "grad_norm": 0.372414231300354,
      "learning_rate": 0.00034333393055889993,
      "loss": 2.1964,
      "step": 33830
    },
    {
      "epoch": 76.9090909090909,
      "grad_norm": 0.16697879135608673,
      "learning_rate": 0.0003432502914601919,
      "loss": 2.196,
      "step": 33840
    },
    {
      "epoch": 76.93181818181819,
      "grad_norm": 0.18966683745384216,
      "learning_rate": 0.0003431666402355775,
      "loss": 2.2065,
      "step": 33850
    },
    {
      "epoch": 76.95454545454545,
      "grad_norm": 0.3471986651420593,
      "learning_rate": 0.0003430829768959347,
      "loss": 2.1884,
      "step": 33860
    },
    {
      "epoch": 76.97727272727273,
      "grad_norm": 0.35750091075897217,
      "learning_rate": 0.00034299930145214246,
      "loss": 2.1879,
      "step": 33870
    },
    {
      "epoch": 77.0,
      "grad_norm": 0.34720712900161743,
      "learning_rate": 0.00034291561391508186,
      "loss": 2.1963,
      "step": 33880
    },
    {
      "epoch": 77.0,
      "eval_loss": 1.0991181135177612,
      "eval_runtime": 8.7538,
      "eval_samples_per_second": 3476.324,
      "eval_steps_per_second": 13.594,
      "step": 33880
    },
    {
      "epoch": 77.02272727272727,
      "grad_norm": 0.22293519973754883,
      "learning_rate": 0.0003428319142956353,
      "loss": 2.1951,
      "step": 33890
    },
    {
      "epoch": 77.04545454545455,
      "grad_norm": 0.22737973928451538,
      "learning_rate": 0.00034274820260468653,
      "loss": 2.1925,
      "step": 33900
    },
    {
      "epoch": 77.06818181818181,
      "grad_norm": 0.20169097185134888,
      "learning_rate": 0.0003426644788531213,
      "loss": 2.1901,
      "step": 33910
    },
    {
      "epoch": 77.0909090909091,
      "grad_norm": 0.13510365784168243,
      "learning_rate": 0.0003425807430518266,
      "loss": 2.203,
      "step": 33920
    },
    {
      "epoch": 77.11363636363636,
      "grad_norm": 0.12355178594589233,
      "learning_rate": 0.0003424969952116912,
      "loss": 2.2022,
      "step": 33930
    },
    {
      "epoch": 77.13636363636364,
      "grad_norm": 0.2496332824230194,
      "learning_rate": 0.0003424132353436053,
      "loss": 2.2063,
      "step": 33940
    },
    {
      "epoch": 77.1590909090909,
      "grad_norm": 0.21481995284557343,
      "learning_rate": 0.00034232946345846073,
      "loss": 2.1805,
      "step": 33950
    },
    {
      "epoch": 77.18181818181819,
      "grad_norm": 0.19399075210094452,
      "learning_rate": 0.00034224567956715084,
      "loss": 2.196,
      "step": 33960
    },
    {
      "epoch": 77.20454545454545,
      "grad_norm": 0.14507336914539337,
      "learning_rate": 0.0003421618836805705,
      "loss": 2.1923,
      "step": 33970
    },
    {
      "epoch": 77.22727272727273,
      "grad_norm": 0.23967529833316803,
      "learning_rate": 0.00034207807580961634,
      "loss": 2.1915,
      "step": 33980
    },
    {
      "epoch": 77.25,
      "grad_norm": 0.24081860482692719,
      "learning_rate": 0.0003419942559651863,
      "loss": 2.1815,
      "step": 33990
    },
    {
      "epoch": 77.27272727272727,
      "grad_norm": 0.2233092486858368,
      "learning_rate": 0.00034191042415818,
      "loss": 2.1833,
      "step": 34000
    },
    {
      "epoch": 77.29545454545455,
      "grad_norm": 0.25406938791275024,
      "learning_rate": 0.0003418265803994987,
      "loss": 2.2037,
      "step": 34010
    },
    {
      "epoch": 77.31818181818181,
      "grad_norm": 0.18642765283584595,
      "learning_rate": 0.000341742724700045,
      "loss": 2.1996,
      "step": 34020
    },
    {
      "epoch": 77.3409090909091,
      "grad_norm": 0.1726880520582199,
      "learning_rate": 0.00034165885707072326,
      "loss": 2.1996,
      "step": 34030
    },
    {
      "epoch": 77.36363636363636,
      "grad_norm": 0.49882054328918457,
      "learning_rate": 0.0003415749775224393,
      "loss": 2.2037,
      "step": 34040
    },
    {
      "epoch": 77.38636363636364,
      "grad_norm": 0.299228698015213,
      "learning_rate": 0.0003414910860661003,
      "loss": 2.1939,
      "step": 34050
    },
    {
      "epoch": 77.4090909090909,
      "grad_norm": 0.3572429418563843,
      "learning_rate": 0.0003414071827126154,
      "loss": 2.2109,
      "step": 34060
    },
    {
      "epoch": 77.43181818181819,
      "grad_norm": 0.10883087664842606,
      "learning_rate": 0.000341323267472895,
      "loss": 2.1939,
      "step": 34070
    },
    {
      "epoch": 77.45454545454545,
      "grad_norm": 0.12992466986179352,
      "learning_rate": 0.0003412393403578511,
      "loss": 2.1961,
      "step": 34080
    },
    {
      "epoch": 77.47727272727273,
      "grad_norm": 0.5580127239227295,
      "learning_rate": 0.0003411554013783973,
      "loss": 2.1983,
      "step": 34090
    },
    {
      "epoch": 77.5,
      "grad_norm": 0.2068876177072525,
      "learning_rate": 0.0003410714505454486,
      "loss": 2.1774,
      "step": 34100
    },
    {
      "epoch": 77.52272727272727,
      "grad_norm": 0.1950986385345459,
      "learning_rate": 0.0003409874878699217,
      "loss": 2.1861,
      "step": 34110
    },
    {
      "epoch": 77.54545454545455,
      "grad_norm": 0.2713286578655243,
      "learning_rate": 0.0003409035133627348,
      "loss": 2.1946,
      "step": 34120
    },
    {
      "epoch": 77.56818181818181,
      "grad_norm": 0.19628284871578217,
      "learning_rate": 0.00034081952703480763,
      "loss": 2.1894,
      "step": 34130
    },
    {
      "epoch": 77.5909090909091,
      "grad_norm": 0.2029615342617035,
      "learning_rate": 0.0003407355288970615,
      "loss": 2.2066,
      "step": 34140
    },
    {
      "epoch": 77.61363636363636,
      "grad_norm": 0.2098640501499176,
      "learning_rate": 0.00034065151896041894,
      "loss": 2.1967,
      "step": 34150
    },
    {
      "epoch": 77.63636363636364,
      "grad_norm": 0.22778472304344177,
      "learning_rate": 0.0003405674972358046,
      "loss": 2.2026,
      "step": 34160
    },
    {
      "epoch": 77.6590909090909,
      "grad_norm": 0.1723403036594391,
      "learning_rate": 0.00034048346373414406,
      "loss": 2.2106,
      "step": 34170
    },
    {
      "epoch": 77.68181818181819,
      "grad_norm": 0.15920092165470123,
      "learning_rate": 0.000340399418466365,
      "loss": 2.1962,
      "step": 34180
    },
    {
      "epoch": 77.70454545454545,
      "grad_norm": 0.19082634150981903,
      "learning_rate": 0.0003403153614433961,
      "loss": 2.1932,
      "step": 34190
    },
    {
      "epoch": 77.72727272727273,
      "grad_norm": 0.1538296937942505,
      "learning_rate": 0.00034023129267616794,
      "loss": 2.1949,
      "step": 34200
    },
    {
      "epoch": 77.75,
      "grad_norm": 0.20271383225917816,
      "learning_rate": 0.00034014721217561243,
      "loss": 2.1941,
      "step": 34210
    },
    {
      "epoch": 77.77272727272727,
      "grad_norm": 0.26810672879219055,
      "learning_rate": 0.000340063119952663,
      "loss": 2.1943,
      "step": 34220
    },
    {
      "epoch": 77.79545454545455,
      "grad_norm": 0.25160685181617737,
      "learning_rate": 0.00033997901601825494,
      "loss": 2.1918,
      "step": 34230
    },
    {
      "epoch": 77.81818181818181,
      "grad_norm": 0.18361437320709229,
      "learning_rate": 0.0003398949003833246,
      "loss": 2.1938,
      "step": 34240
    },
    {
      "epoch": 77.8409090909091,
      "grad_norm": 0.1596294790506363,
      "learning_rate": 0.00033981077305880996,
      "loss": 2.1901,
      "step": 34250
    },
    {
      "epoch": 77.86363636363636,
      "grad_norm": 0.36330732703208923,
      "learning_rate": 0.0003397266340556509,
      "loss": 2.1967,
      "step": 34260
    },
    {
      "epoch": 77.88636363636364,
      "grad_norm": 0.18023425340652466,
      "learning_rate": 0.00033964248338478833,
      "loss": 2.1828,
      "step": 34270
    },
    {
      "epoch": 77.9090909090909,
      "grad_norm": 0.19802799820899963,
      "learning_rate": 0.0003395583210571648,
      "loss": 2.2026,
      "step": 34280
    },
    {
      "epoch": 77.93181818181819,
      "grad_norm": 0.1832638829946518,
      "learning_rate": 0.0003394741470837246,
      "loss": 2.1969,
      "step": 34290
    },
    {
      "epoch": 77.95454545454545,
      "grad_norm": 0.15041029453277588,
      "learning_rate": 0.00033938996147541334,
      "loss": 2.1868,
      "step": 34300
    },
    {
      "epoch": 77.97727272727273,
      "grad_norm": 0.21920624375343323,
      "learning_rate": 0.00033930576424317827,
      "loss": 2.1974,
      "step": 34310
    },
    {
      "epoch": 78.0,
      "grad_norm": 0.2527529001235962,
      "learning_rate": 0.00033922155539796797,
      "loss": 2.197,
      "step": 34320
    },
    {
      "epoch": 78.0,
      "eval_loss": 1.0991050004959106,
      "eval_runtime": 8.8769,
      "eval_samples_per_second": 3428.12,
      "eval_steps_per_second": 13.406,
      "step": 34320
    },
    {
      "epoch": 78.02272727272727,
      "grad_norm": 0.1509620100259781,
      "learning_rate": 0.00033913733495073267,
      "loss": 2.1918,
      "step": 34330
    },
    {
      "epoch": 78.04545454545455,
      "grad_norm": 0.25251907110214233,
      "learning_rate": 0.0003390531029124241,
      "loss": 2.1894,
      "step": 34340
    },
    {
      "epoch": 78.06818181818181,
      "grad_norm": 0.1759963035583496,
      "learning_rate": 0.00033896885929399537,
      "loss": 2.1875,
      "step": 34350
    },
    {
      "epoch": 78.0909090909091,
      "grad_norm": 0.34058794379234314,
      "learning_rate": 0.0003388846041064012,
      "loss": 2.1976,
      "step": 34360
    },
    {
      "epoch": 78.11363636363636,
      "grad_norm": 0.28595468401908875,
      "learning_rate": 0.00033880033736059795,
      "loss": 2.1881,
      "step": 34370
    },
    {
      "epoch": 78.13636363636364,
      "grad_norm": 0.3895009160041809,
      "learning_rate": 0.00033871605906754326,
      "loss": 2.1997,
      "step": 34380
    },
    {
      "epoch": 78.1590909090909,
      "grad_norm": 0.2499161809682846,
      "learning_rate": 0.00033863176923819635,
      "loss": 2.1901,
      "step": 34390
    },
    {
      "epoch": 78.18181818181819,
      "grad_norm": 1.0812926292419434,
      "learning_rate": 0.00033854746788351787,
      "loss": 2.1883,
      "step": 34400
    },
    {
      "epoch": 78.20454545454545,
      "grad_norm": 0.23945291340351105,
      "learning_rate": 0.0003384631550144701,
      "loss": 2.185,
      "step": 34410
    },
    {
      "epoch": 78.22727272727273,
      "grad_norm": 0.20188476145267487,
      "learning_rate": 0.00033837883064201683,
      "loss": 2.1894,
      "step": 34420
    },
    {
      "epoch": 78.25,
      "grad_norm": 0.24991893768310547,
      "learning_rate": 0.0003382944947771232,
      "loss": 2.2076,
      "step": 34430
    },
    {
      "epoch": 78.27272727272727,
      "grad_norm": 0.16680355370044708,
      "learning_rate": 0.00033821014743075595,
      "loss": 2.1963,
      "step": 34440
    },
    {
      "epoch": 78.29545454545455,
      "grad_norm": 0.3053037226200104,
      "learning_rate": 0.0003381257886138832,
      "loss": 2.1951,
      "step": 34450
    },
    {
      "epoch": 78.31818181818181,
      "grad_norm": 0.19977833330631256,
      "learning_rate": 0.00033804141833747477,
      "loss": 2.2045,
      "step": 34460
    },
    {
      "epoch": 78.3409090909091,
      "grad_norm": 0.22322988510131836,
      "learning_rate": 0.00033795703661250177,
      "loss": 2.1967,
      "step": 34470
    },
    {
      "epoch": 78.36363636363636,
      "grad_norm": 0.2578172981739044,
      "learning_rate": 0.00033787264344993685,
      "loss": 2.207,
      "step": 34480
    },
    {
      "epoch": 78.38636363636364,
      "grad_norm": 0.11429788172245026,
      "learning_rate": 0.0003377882388607542,
      "loss": 2.1902,
      "step": 34490
    },
    {
      "epoch": 78.4090909090909,
      "grad_norm": 0.22634586691856384,
      "learning_rate": 0.0003377038228559295,
      "loss": 2.1871,
      "step": 34500
    },
    {
      "epoch": 78.43181818181819,
      "grad_norm": 0.1711856573820114,
      "learning_rate": 0.0003376193954464398,
      "loss": 2.1889,
      "step": 34510
    },
    {
      "epoch": 78.45454545454545,
      "grad_norm": 0.2380482703447342,
      "learning_rate": 0.00033753495664326386,
      "loss": 2.2072,
      "step": 34520
    },
    {
      "epoch": 78.47727272727273,
      "grad_norm": 0.1559821516275406,
      "learning_rate": 0.0003374505064573815,
      "loss": 2.1883,
      "step": 34530
    },
    {
      "epoch": 78.5,
      "grad_norm": 0.19817355275154114,
      "learning_rate": 0.0003373660448997746,
      "loss": 2.2011,
      "step": 34540
    },
    {
      "epoch": 78.52272727272727,
      "grad_norm": 0.36924272775650024,
      "learning_rate": 0.000337281571981426,
      "loss": 2.1875,
      "step": 34550
    },
    {
      "epoch": 78.54545454545455,
      "grad_norm": 0.4101180136203766,
      "learning_rate": 0.00033719708771332035,
      "loss": 2.196,
      "step": 34560
    },
    {
      "epoch": 78.56818181818181,
      "grad_norm": 0.1750824749469757,
      "learning_rate": 0.00033711259210644366,
      "loss": 2.1963,
      "step": 34570
    },
    {
      "epoch": 78.5909090909091,
      "grad_norm": 0.24339355528354645,
      "learning_rate": 0.00033702808517178343,
      "loss": 2.193,
      "step": 34580
    },
    {
      "epoch": 78.61363636363636,
      "grad_norm": 0.1306944191455841,
      "learning_rate": 0.0003369435669203284,
      "loss": 2.187,
      "step": 34590
    },
    {
      "epoch": 78.63636363636364,
      "grad_norm": 0.2796099781990051,
      "learning_rate": 0.00033685903736306924,
      "loss": 2.197,
      "step": 34600
    },
    {
      "epoch": 78.6590909090909,
      "grad_norm": 0.11152917891740799,
      "learning_rate": 0.00033677449651099765,
      "loss": 2.2019,
      "step": 34610
    },
    {
      "epoch": 78.68181818181819,
      "grad_norm": 0.15579698979854584,
      "learning_rate": 0.00033668994437510724,
      "loss": 2.2022,
      "step": 34620
    },
    {
      "epoch": 78.70454545454545,
      "grad_norm": 0.1577170491218567,
      "learning_rate": 0.0003366053809663927,
      "loss": 2.2035,
      "step": 34630
    },
    {
      "epoch": 78.72727272727273,
      "grad_norm": 0.1752513349056244,
      "learning_rate": 0.0003365208062958502,
      "loss": 2.1953,
      "step": 34640
    },
    {
      "epoch": 78.75,
      "grad_norm": 0.15293511748313904,
      "learning_rate": 0.0003364362203744777,
      "loss": 2.1898,
      "step": 34650
    },
    {
      "epoch": 78.77272727272727,
      "grad_norm": 0.18602314591407776,
      "learning_rate": 0.0003363516232132743,
      "loss": 2.1842,
      "step": 34660
    },
    {
      "epoch": 78.79545454545455,
      "grad_norm": 0.1694336086511612,
      "learning_rate": 0.00033626701482324084,
      "loss": 2.1983,
      "step": 34670
    },
    {
      "epoch": 78.81818181818181,
      "grad_norm": 0.2283979207277298,
      "learning_rate": 0.0003361823952153793,
      "loss": 2.2004,
      "step": 34680
    },
    {
      "epoch": 78.8409090909091,
      "grad_norm": 0.16298052668571472,
      "learning_rate": 0.00033609776440069345,
      "loss": 2.1979,
      "step": 34690
    },
    {
      "epoch": 78.86363636363636,
      "grad_norm": 0.14758893847465515,
      "learning_rate": 0.0003360131223901881,
      "loss": 2.1826,
      "step": 34700
    },
    {
      "epoch": 78.88636363636364,
      "grad_norm": 0.22690671682357788,
      "learning_rate": 0.00033592846919487006,
      "loss": 2.2007,
      "step": 34710
    },
    {
      "epoch": 78.9090909090909,
      "grad_norm": 0.2847656011581421,
      "learning_rate": 0.0003358438048257472,
      "loss": 2.1972,
      "step": 34720
    },
    {
      "epoch": 78.93181818181819,
      "grad_norm": 0.23367345333099365,
      "learning_rate": 0.00033575912929382877,
      "loss": 2.1974,
      "step": 34730
    },
    {
      "epoch": 78.95454545454545,
      "grad_norm": 0.2777364253997803,
      "learning_rate": 0.00033567444261012585,
      "loss": 2.1964,
      "step": 34740
    },
    {
      "epoch": 78.97727272727273,
      "grad_norm": 0.1555003672838211,
      "learning_rate": 0.00033558974478565076,
      "loss": 2.191,
      "step": 34750
    },
    {
      "epoch": 79.0,
      "grad_norm": 0.2046559602022171,
      "learning_rate": 0.0003355050358314172,
      "loss": 2.2037,
      "step": 34760
    },
    {
      "epoch": 79.0,
      "eval_loss": 1.0990439653396606,
      "eval_runtime": 8.7041,
      "eval_samples_per_second": 3496.165,
      "eval_steps_per_second": 13.672,
      "step": 34760
    },
    {
      "epoch": 79.02272727272727,
      "grad_norm": 0.2642152011394501,
      "learning_rate": 0.00033542031575844046,
      "loss": 2.1954,
      "step": 34770
    },
    {
      "epoch": 79.04545454545455,
      "grad_norm": 0.36585038900375366,
      "learning_rate": 0.00033533558457773705,
      "loss": 2.1959,
      "step": 34780
    },
    {
      "epoch": 79.06818181818181,
      "grad_norm": 0.11274909228086472,
      "learning_rate": 0.00033525084230032534,
      "loss": 2.1921,
      "step": 34790
    },
    {
      "epoch": 79.0909090909091,
      "grad_norm": 0.21389718353748322,
      "learning_rate": 0.00033516608893722474,
      "loss": 2.1922,
      "step": 34800
    },
    {
      "epoch": 79.11363636363636,
      "grad_norm": 0.9099605083465576,
      "learning_rate": 0.0003350813244994562,
      "loss": 2.1918,
      "step": 34810
    },
    {
      "epoch": 79.13636363636364,
      "grad_norm": 0.21842721104621887,
      "learning_rate": 0.0003349965489980423,
      "loss": 2.2052,
      "step": 34820
    },
    {
      "epoch": 79.1590909090909,
      "grad_norm": 0.19549508392810822,
      "learning_rate": 0.0003349117624440067,
      "loss": 2.1959,
      "step": 34830
    },
    {
      "epoch": 79.18181818181819,
      "grad_norm": 0.14459381997585297,
      "learning_rate": 0.00033482696484837495,
      "loss": 2.191,
      "step": 34840
    },
    {
      "epoch": 79.20454545454545,
      "grad_norm": 0.24834485352039337,
      "learning_rate": 0.0003347421562221737,
      "loss": 2.1857,
      "step": 34850
    },
    {
      "epoch": 79.22727272727273,
      "grad_norm": 0.43075108528137207,
      "learning_rate": 0.0003346573365764311,
      "loss": 2.1927,
      "step": 34860
    },
    {
      "epoch": 79.25,
      "grad_norm": 0.17058919370174408,
      "learning_rate": 0.00033457250592217687,
      "loss": 2.196,
      "step": 34870
    },
    {
      "epoch": 79.27272727272727,
      "grad_norm": 0.1815950572490692,
      "learning_rate": 0.000334487664270442,
      "loss": 2.2041,
      "step": 34880
    },
    {
      "epoch": 79.29545454545455,
      "grad_norm": 0.16413015127182007,
      "learning_rate": 0.0003344028116322589,
      "loss": 2.2068,
      "step": 34890
    },
    {
      "epoch": 79.31818181818181,
      "grad_norm": 0.12919127941131592,
      "learning_rate": 0.00033431794801866156,
      "loss": 2.1887,
      "step": 34900
    },
    {
      "epoch": 79.3409090909091,
      "grad_norm": 0.17934513092041016,
      "learning_rate": 0.00033423307344068525,
      "loss": 2.1912,
      "step": 34910
    },
    {
      "epoch": 79.36363636363636,
      "grad_norm": 0.13557355105876923,
      "learning_rate": 0.00033414818790936687,
      "loss": 2.1929,
      "step": 34920
    },
    {
      "epoch": 79.38636363636364,
      "grad_norm": 0.2825014293193817,
      "learning_rate": 0.00033406329143574446,
      "loss": 2.1944,
      "step": 34930
    },
    {
      "epoch": 79.4090909090909,
      "grad_norm": 0.24592973291873932,
      "learning_rate": 0.00033397838403085775,
      "loss": 2.2007,
      "step": 34940
    },
    {
      "epoch": 79.43181818181819,
      "grad_norm": 0.14811962842941284,
      "learning_rate": 0.00033389346570574766,
      "loss": 2.1986,
      "step": 34950
    },
    {
      "epoch": 79.45454545454545,
      "grad_norm": 0.1957959234714508,
      "learning_rate": 0.0003338085364714566,
      "loss": 2.2012,
      "step": 34960
    },
    {
      "epoch": 79.47727272727273,
      "grad_norm": 0.20504212379455566,
      "learning_rate": 0.0003337235963390286,
      "loss": 2.1976,
      "step": 34970
    },
    {
      "epoch": 79.5,
      "grad_norm": 0.2843160033226013,
      "learning_rate": 0.0003336386453195088,
      "loss": 2.1906,
      "step": 34980
    },
    {
      "epoch": 79.52272727272727,
      "grad_norm": 0.14571109414100647,
      "learning_rate": 0.0003335536834239441,
      "loss": 2.1994,
      "step": 34990
    },
    {
      "epoch": 79.54545454545455,
      "grad_norm": 0.1472737044095993,
      "learning_rate": 0.0003334687106633824,
      "loss": 2.1853,
      "step": 35000
    },
    {
      "epoch": 79.56818181818181,
      "grad_norm": 0.45888766646385193,
      "learning_rate": 0.00033338372704887334,
      "loss": 2.191,
      "step": 35010
    },
    {
      "epoch": 79.5909090909091,
      "grad_norm": 0.15339380502700806,
      "learning_rate": 0.00033329873259146773,
      "loss": 2.2047,
      "step": 35020
    },
    {
      "epoch": 79.61363636363636,
      "grad_norm": 0.12942206859588623,
      "learning_rate": 0.0003332137273022181,
      "loss": 2.1845,
      "step": 35030
    },
    {
      "epoch": 79.63636363636364,
      "grad_norm": 0.2514657974243164,
      "learning_rate": 0.00033312871119217815,
      "loss": 2.1945,
      "step": 35040
    },
    {
      "epoch": 79.6590909090909,
      "grad_norm": 0.14283722639083862,
      "learning_rate": 0.00033304368427240294,
      "loss": 2.2002,
      "step": 35050
    },
    {
      "epoch": 79.68181818181819,
      "grad_norm": 0.16004258394241333,
      "learning_rate": 0.0003329586465539492,
      "loss": 2.202,
      "step": 35060
    },
    {
      "epoch": 79.70454545454545,
      "grad_norm": 0.2989688515663147,
      "learning_rate": 0.0003328735980478748,
      "loss": 2.1932,
      "step": 35070
    },
    {
      "epoch": 79.72727272727273,
      "grad_norm": 0.18403446674346924,
      "learning_rate": 0.0003327885387652391,
      "loss": 2.1975,
      "step": 35080
    },
    {
      "epoch": 79.75,
      "grad_norm": 0.18427090346813202,
      "learning_rate": 0.000332703468717103,
      "loss": 2.194,
      "step": 35090
    },
    {
      "epoch": 79.77272727272727,
      "grad_norm": 0.27357017993927,
      "learning_rate": 0.0003326183879145286,
      "loss": 2.1866,
      "step": 35100
    },
    {
      "epoch": 79.79545454545455,
      "grad_norm": 0.4883839786052704,
      "learning_rate": 0.0003325332963685794,
      "loss": 2.1916,
      "step": 35110
    },
    {
      "epoch": 79.81818181818181,
      "grad_norm": 0.40954628586769104,
      "learning_rate": 0.00033244819409032053,
      "loss": 2.2041,
      "step": 35120
    },
    {
      "epoch": 79.8409090909091,
      "grad_norm": 0.1975550800561905,
      "learning_rate": 0.00033236308109081827,
      "loss": 2.1964,
      "step": 35130
    },
    {
      "epoch": 79.86363636363636,
      "grad_norm": 0.17714881896972656,
      "learning_rate": 0.0003322779573811404,
      "loss": 2.1897,
      "step": 35140
    },
    {
      "epoch": 79.88636363636364,
      "grad_norm": 0.1724480837583542,
      "learning_rate": 0.0003321928229723561,
      "loss": 2.1909,
      "step": 35150
    },
    {
      "epoch": 79.9090909090909,
      "grad_norm": 0.1636752188205719,
      "learning_rate": 0.0003321076778755358,
      "loss": 2.1865,
      "step": 35160
    },
    {
      "epoch": 79.93181818181819,
      "grad_norm": 0.3886908292770386,
      "learning_rate": 0.0003320225221017516,
      "loss": 2.1983,
      "step": 35170
    },
    {
      "epoch": 79.95454545454545,
      "grad_norm": 0.4120870530605316,
      "learning_rate": 0.00033193735566207675,
      "loss": 2.1996,
      "step": 35180
    },
    {
      "epoch": 79.97727272727273,
      "grad_norm": 0.2462073713541031,
      "learning_rate": 0.00033185217856758606,
      "loss": 2.1888,
      "step": 35190
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.45467519760131836,
      "learning_rate": 0.00033176699082935546,
      "loss": 2.1891,
      "step": 35200
    },
    {
      "epoch": 80.0,
      "eval_loss": 1.0992056131362915,
      "eval_runtime": 8.9114,
      "eval_samples_per_second": 3414.857,
      "eval_steps_per_second": 13.354,
      "step": 35200
    },
    {
      "epoch": 80.02272727272727,
      "grad_norm": 0.3251344859600067,
      "learning_rate": 0.00033168179245846247,
      "loss": 2.1932,
      "step": 35210
    },
    {
      "epoch": 80.04545454545455,
      "grad_norm": 0.11914538592100143,
      "learning_rate": 0.0003315965834659861,
      "loss": 2.1947,
      "step": 35220
    },
    {
      "epoch": 80.06818181818181,
      "grad_norm": 0.17655475437641144,
      "learning_rate": 0.00033151136386300645,
      "loss": 2.19,
      "step": 35230
    },
    {
      "epoch": 80.0909090909091,
      "grad_norm": 0.21485863626003265,
      "learning_rate": 0.00033142613366060516,
      "loss": 2.1981,
      "step": 35240
    },
    {
      "epoch": 80.11363636363636,
      "grad_norm": 0.28951579332351685,
      "learning_rate": 0.00033134089286986535,
      "loss": 2.1976,
      "step": 35250
    },
    {
      "epoch": 80.13636363636364,
      "grad_norm": 0.34433653950691223,
      "learning_rate": 0.0003312556415018713,
      "loss": 2.1926,
      "step": 35260
    },
    {
      "epoch": 80.1590909090909,
      "grad_norm": 0.16202211380004883,
      "learning_rate": 0.00033117037956770874,
      "loss": 2.1895,
      "step": 35270
    },
    {
      "epoch": 80.18181818181819,
      "grad_norm": 0.19967716932296753,
      "learning_rate": 0.0003310851070784649,
      "loss": 2.186,
      "step": 35280
    },
    {
      "epoch": 80.20454545454545,
      "grad_norm": 0.19517715275287628,
      "learning_rate": 0.0003309998240452282,
      "loss": 2.198,
      "step": 35290
    },
    {
      "epoch": 80.22727272727273,
      "grad_norm": 0.2231644243001938,
      "learning_rate": 0.0003309145304790886,
      "loss": 2.1883,
      "step": 35300
    },
    {
      "epoch": 80.25,
      "grad_norm": 0.18826720118522644,
      "learning_rate": 0.0003308292263911372,
      "loss": 2.1844,
      "step": 35310
    },
    {
      "epoch": 80.27272727272727,
      "grad_norm": 0.7779048085212708,
      "learning_rate": 0.0003307439117924668,
      "loss": 2.1896,
      "step": 35320
    },
    {
      "epoch": 80.29545454545455,
      "grad_norm": 0.24018417298793793,
      "learning_rate": 0.0003306585866941713,
      "loss": 2.1892,
      "step": 35330
    },
    {
      "epoch": 80.31818181818181,
      "grad_norm": 0.15261191129684448,
      "learning_rate": 0.00033057325110734593,
      "loss": 2.1979,
      "step": 35340
    },
    {
      "epoch": 80.3409090909091,
      "grad_norm": 0.22240671515464783,
      "learning_rate": 0.0003304879050430876,
      "loss": 2.2046,
      "step": 35350
    },
    {
      "epoch": 80.36363636363636,
      "grad_norm": 0.3237018287181854,
      "learning_rate": 0.0003304025485124942,
      "loss": 2.1859,
      "step": 35360
    },
    {
      "epoch": 80.38636363636364,
      "grad_norm": 0.18140597641468048,
      "learning_rate": 0.0003303171815266653,
      "loss": 2.1857,
      "step": 35370
    },
    {
      "epoch": 80.4090909090909,
      "grad_norm": 0.16919225454330444,
      "learning_rate": 0.0003302318040967016,
      "loss": 2.2068,
      "step": 35380
    },
    {
      "epoch": 80.43181818181819,
      "grad_norm": 0.1872374713420868,
      "learning_rate": 0.0003301464162337053,
      "loss": 2.1845,
      "step": 35390
    },
    {
      "epoch": 80.45454545454545,
      "grad_norm": 0.406070739030838,
      "learning_rate": 0.00033006101794877987,
      "loss": 2.2014,
      "step": 35400
    },
    {
      "epoch": 80.47727272727273,
      "grad_norm": 0.16918110847473145,
      "learning_rate": 0.00032997560925303017,
      "loss": 2.2015,
      "step": 35410
    },
    {
      "epoch": 80.5,
      "grad_norm": 0.1664735972881317,
      "learning_rate": 0.0003298901901575625,
      "loss": 2.1934,
      "step": 35420
    },
    {
      "epoch": 80.52272727272727,
      "grad_norm": 0.2264351099729538,
      "learning_rate": 0.0003298047606734844,
      "loss": 2.1898,
      "step": 35430
    },
    {
      "epoch": 80.54545454545455,
      "grad_norm": 0.18750260770320892,
      "learning_rate": 0.0003297193208119047,
      "loss": 2.1879,
      "step": 35440
    },
    {
      "epoch": 80.56818181818181,
      "grad_norm": 0.2233542799949646,
      "learning_rate": 0.0003296338705839337,
      "loss": 2.1982,
      "step": 35450
    },
    {
      "epoch": 80.5909090909091,
      "grad_norm": 0.18648211658000946,
      "learning_rate": 0.000329548410000683,
      "loss": 2.1993,
      "step": 35460
    },
    {
      "epoch": 80.61363636363636,
      "grad_norm": 0.2513607144355774,
      "learning_rate": 0.0003294629390732657,
      "loss": 2.1986,
      "step": 35470
    },
    {
      "epoch": 80.63636363636364,
      "grad_norm": 0.15188001096248627,
      "learning_rate": 0.000329377457812796,
      "loss": 2.2006,
      "step": 35480
    },
    {
      "epoch": 80.6590909090909,
      "grad_norm": 0.13369984924793243,
      "learning_rate": 0.00032929196623038954,
      "loss": 2.2027,
      "step": 35490
    },
    {
      "epoch": 80.68181818181819,
      "grad_norm": 0.21024690568447113,
      "learning_rate": 0.00032920646433716336,
      "loss": 2.1979,
      "step": 35500
    },
    {
      "epoch": 80.70454545454545,
      "grad_norm": 0.27554985880851746,
      "learning_rate": 0.00032912095214423565,
      "loss": 2.1968,
      "step": 35510
    },
    {
      "epoch": 80.72727272727273,
      "grad_norm": 0.18899938464164734,
      "learning_rate": 0.00032903542966272633,
      "loss": 2.1861,
      "step": 35520
    },
    {
      "epoch": 80.75,
      "grad_norm": 0.21988610923290253,
      "learning_rate": 0.00032894989690375627,
      "loss": 2.1946,
      "step": 35530
    },
    {
      "epoch": 80.77272727272727,
      "grad_norm": 0.13769669830799103,
      "learning_rate": 0.00032886435387844776,
      "loss": 2.1767,
      "step": 35540
    },
    {
      "epoch": 80.79545454545455,
      "grad_norm": 0.15755242109298706,
      "learning_rate": 0.00032877880059792465,
      "loss": 2.1972,
      "step": 35550
    },
    {
      "epoch": 80.81818181818181,
      "grad_norm": 0.18375606834888458,
      "learning_rate": 0.0003286932370733118,
      "loss": 2.2042,
      "step": 35560
    },
    {
      "epoch": 80.8409090909091,
      "grad_norm": 0.20030555129051208,
      "learning_rate": 0.00032860766331573564,
      "loss": 2.1859,
      "step": 35570
    },
    {
      "epoch": 80.86363636363636,
      "grad_norm": 0.2503253221511841,
      "learning_rate": 0.0003285220793363239,
      "loss": 2.1981,
      "step": 35580
    },
    {
      "epoch": 80.88636363636364,
      "grad_norm": 0.18111146986484528,
      "learning_rate": 0.00032843648514620537,
      "loss": 2.1962,
      "step": 35590
    },
    {
      "epoch": 80.9090909090909,
      "grad_norm": 0.148964524269104,
      "learning_rate": 0.0003283508807565106,
      "loss": 2.1956,
      "step": 35600
    },
    {
      "epoch": 80.93181818181819,
      "grad_norm": 0.3115274906158447,
      "learning_rate": 0.0003282652661783713,
      "loss": 2.1961,
      "step": 35610
    },
    {
      "epoch": 80.95454545454545,
      "grad_norm": 0.16117657721042633,
      "learning_rate": 0.00032817964142292026,
      "loss": 2.2004,
      "step": 35620
    },
    {
      "epoch": 80.97727272727273,
      "grad_norm": 0.3016497492790222,
      "learning_rate": 0.0003280940065012919,
      "loss": 2.1905,
      "step": 35630
    },
    {
      "epoch": 81.0,
      "grad_norm": 0.44096481800079346,
      "learning_rate": 0.00032800836142462175,
      "loss": 2.2024,
      "step": 35640
    },
    {
      "epoch": 81.0,
      "eval_loss": 1.0994367599487305,
      "eval_runtime": 8.7205,
      "eval_samples_per_second": 3489.608,
      "eval_steps_per_second": 13.646,
      "step": 35640
    },
    {
      "epoch": 81.02272727272727,
      "grad_norm": 0.12411776930093765,
      "learning_rate": 0.000327922706204047,
      "loss": 2.1966,
      "step": 35650
    },
    {
      "epoch": 81.04545454545455,
      "grad_norm": 0.2298121601343155,
      "learning_rate": 0.0003278370408507056,
      "loss": 2.2015,
      "step": 35660
    },
    {
      "epoch": 81.06818181818181,
      "grad_norm": 0.643036425113678,
      "learning_rate": 0.0003277513653757374,
      "loss": 2.1918,
      "step": 35670
    },
    {
      "epoch": 81.0909090909091,
      "grad_norm": 0.34482187032699585,
      "learning_rate": 0.0003276656797902833,
      "loss": 2.1812,
      "step": 35680
    },
    {
      "epoch": 81.11363636363636,
      "grad_norm": 0.17535488307476044,
      "learning_rate": 0.0003275799841054853,
      "loss": 2.189,
      "step": 35690
    },
    {
      "epoch": 81.13636363636364,
      "grad_norm": 0.24459081888198853,
      "learning_rate": 0.00032749427833248706,
      "loss": 2.2032,
      "step": 35700
    },
    {
      "epoch": 81.1590909090909,
      "grad_norm": 0.37398940324783325,
      "learning_rate": 0.0003274085624824335,
      "loss": 2.1978,
      "step": 35710
    },
    {
      "epoch": 81.18181818181819,
      "grad_norm": 0.20202815532684326,
      "learning_rate": 0.00032732283656647067,
      "loss": 2.1921,
      "step": 35720
    },
    {
      "epoch": 81.20454545454545,
      "grad_norm": 0.17052017152309418,
      "learning_rate": 0.0003272371005957461,
      "loss": 2.1928,
      "step": 35730
    },
    {
      "epoch": 81.22727272727273,
      "grad_norm": 0.4208587408065796,
      "learning_rate": 0.0003271513545814086,
      "loss": 2.1935,
      "step": 35740
    },
    {
      "epoch": 81.25,
      "grad_norm": 0.17537716031074524,
      "learning_rate": 0.00032706559853460813,
      "loss": 2.1911,
      "step": 35750
    },
    {
      "epoch": 81.27272727272727,
      "grad_norm": 0.25460654497146606,
      "learning_rate": 0.0003269798324664962,
      "loss": 2.191,
      "step": 35760
    },
    {
      "epoch": 81.29545454545455,
      "grad_norm": 0.17010067403316498,
      "learning_rate": 0.00032689405638822535,
      "loss": 2.1845,
      "step": 35770
    },
    {
      "epoch": 81.31818181818181,
      "grad_norm": 0.1192445382475853,
      "learning_rate": 0.00032680827031094967,
      "loss": 2.197,
      "step": 35780
    },
    {
      "epoch": 81.3409090909091,
      "grad_norm": 0.22104133665561676,
      "learning_rate": 0.0003267224742458244,
      "loss": 2.1997,
      "step": 35790
    },
    {
      "epoch": 81.36363636363636,
      "grad_norm": 0.32219526171684265,
      "learning_rate": 0.0003266366682040063,
      "loss": 2.1845,
      "step": 35800
    },
    {
      "epoch": 81.38636363636364,
      "grad_norm": 0.1505701243877411,
      "learning_rate": 0.000326550852196653,
      "loss": 2.1976,
      "step": 35810
    },
    {
      "epoch": 81.4090909090909,
      "grad_norm": 1.268416404724121,
      "learning_rate": 0.00032646502623492375,
      "loss": 2.1889,
      "step": 35820
    },
    {
      "epoch": 81.43181818181819,
      "grad_norm": 0.234977588057518,
      "learning_rate": 0.00032637919032997917,
      "loss": 2.1995,
      "step": 35830
    },
    {
      "epoch": 81.45454545454545,
      "grad_norm": 0.20301413536071777,
      "learning_rate": 0.0003262933444929808,
      "loss": 2.1912,
      "step": 35840
    },
    {
      "epoch": 81.47727272727273,
      "grad_norm": 0.2914864718914032,
      "learning_rate": 0.0003262074887350919,
      "loss": 2.1836,
      "step": 35850
    },
    {
      "epoch": 81.5,
      "grad_norm": 0.26650092005729675,
      "learning_rate": 0.0003261216230674768,
      "loss": 2.181,
      "step": 35860
    },
    {
      "epoch": 81.52272727272727,
      "grad_norm": 0.306226909160614,
      "learning_rate": 0.000326035747501301,
      "loss": 2.1916,
      "step": 35870
    },
    {
      "epoch": 81.54545454545455,
      "grad_norm": 0.2269556224346161,
      "learning_rate": 0.00032594986204773156,
      "loss": 2.1933,
      "step": 35880
    },
    {
      "epoch": 81.56818181818181,
      "grad_norm": 0.216244176030159,
      "learning_rate": 0.0003258639667179366,
      "loss": 2.1888,
      "step": 35890
    },
    {
      "epoch": 81.5909090909091,
      "grad_norm": 0.6424205303192139,
      "learning_rate": 0.00032577806152308565,
      "loss": 2.1899,
      "step": 35900
    },
    {
      "epoch": 81.61363636363636,
      "grad_norm": 0.21229016780853271,
      "learning_rate": 0.0003256921464743495,
      "loss": 2.1941,
      "step": 35910
    },
    {
      "epoch": 81.63636363636364,
      "grad_norm": 0.14381444454193115,
      "learning_rate": 0.0003256062215829003,
      "loss": 2.1982,
      "step": 35920
    },
    {
      "epoch": 81.6590909090909,
      "grad_norm": 0.370156854391098,
      "learning_rate": 0.0003255202868599112,
      "loss": 2.2021,
      "step": 35930
    },
    {
      "epoch": 81.68181818181819,
      "grad_norm": 0.4522835612297058,
      "learning_rate": 0.00032543434231655687,
      "loss": 2.1972,
      "step": 35940
    },
    {
      "epoch": 81.70454545454545,
      "grad_norm": 0.13506940007209778,
      "learning_rate": 0.00032534838796401334,
      "loss": 2.1832,
      "step": 35950
    },
    {
      "epoch": 81.72727272727273,
      "grad_norm": 0.32189762592315674,
      "learning_rate": 0.0003252624238134577,
      "loss": 2.194,
      "step": 35960
    },
    {
      "epoch": 81.75,
      "grad_norm": 0.6621732115745544,
      "learning_rate": 0.00032517644987606825,
      "loss": 2.2007,
      "step": 35970
    },
    {
      "epoch": 81.77272727272727,
      "grad_norm": 0.2020515501499176,
      "learning_rate": 0.00032509046616302495,
      "loss": 2.1912,
      "step": 35980
    },
    {
      "epoch": 81.79545454545455,
      "grad_norm": 0.18679210543632507,
      "learning_rate": 0.0003250044726855087,
      "loss": 2.1954,
      "step": 35990
    },
    {
      "epoch": 81.81818181818181,
      "grad_norm": 0.9337456822395325,
      "learning_rate": 0.00032491846945470167,
      "loss": 2.1946,
      "step": 36000
    },
    {
      "epoch": 81.8409090909091,
      "grad_norm": 0.20216777920722961,
      "learning_rate": 0.0003248324564817875,
      "loss": 2.1905,
      "step": 36010
    },
    {
      "epoch": 81.86363636363636,
      "grad_norm": 0.16038358211517334,
      "learning_rate": 0.0003247464337779509,
      "loss": 2.1896,
      "step": 36020
    },
    {
      "epoch": 81.88636363636364,
      "grad_norm": 0.41337552666664124,
      "learning_rate": 0.0003246604013543779,
      "loss": 2.1918,
      "step": 36030
    },
    {
      "epoch": 81.9090909090909,
      "grad_norm": 0.773892343044281,
      "learning_rate": 0.00032457435922225605,
      "loss": 2.2073,
      "step": 36040
    },
    {
      "epoch": 81.93181818181819,
      "grad_norm": 0.22467274963855743,
      "learning_rate": 0.00032448830739277367,
      "loss": 2.2051,
      "step": 36050
    },
    {
      "epoch": 81.95454545454545,
      "grad_norm": 0.23699088394641876,
      "learning_rate": 0.00032440224587712075,
      "loss": 2.2031,
      "step": 36060
    },
    {
      "epoch": 81.97727272727273,
      "grad_norm": 0.18363825976848602,
      "learning_rate": 0.00032431617468648826,
      "loss": 2.1998,
      "step": 36070
    },
    {
      "epoch": 82.0,
      "grad_norm": 0.27406513690948486,
      "learning_rate": 0.00032423009383206875,
      "loss": 2.182,
      "step": 36080
    },
    {
      "epoch": 82.0,
      "eval_loss": 1.0989741086959839,
      "eval_runtime": 8.7516,
      "eval_samples_per_second": 3477.203,
      "eval_steps_per_second": 13.598,
      "step": 36080
    },
    {
      "epoch": 82.02272727272727,
      "grad_norm": 0.3002833425998688,
      "learning_rate": 0.0003241440033250557,
      "loss": 2.1846,
      "step": 36090
    },
    {
      "epoch": 82.04545454545455,
      "grad_norm": 0.14661598205566406,
      "learning_rate": 0.00032405790317664403,
      "loss": 2.1797,
      "step": 36100
    },
    {
      "epoch": 82.06818181818181,
      "grad_norm": 0.3651069700717926,
      "learning_rate": 0.00032397179339802994,
      "loss": 2.1963,
      "step": 36110
    },
    {
      "epoch": 82.0909090909091,
      "grad_norm": 0.178850919008255,
      "learning_rate": 0.0003238856740004107,
      "loss": 2.1895,
      "step": 36120
    },
    {
      "epoch": 82.11363636363636,
      "grad_norm": 0.20177528262138367,
      "learning_rate": 0.000323799544994985,
      "loss": 2.1811,
      "step": 36130
    },
    {
      "epoch": 82.13636363636364,
      "grad_norm": 0.34274226427078247,
      "learning_rate": 0.00032371340639295265,
      "loss": 2.1937,
      "step": 36140
    },
    {
      "epoch": 82.1590909090909,
      "grad_norm": 0.25136899948120117,
      "learning_rate": 0.0003236272582055149,
      "loss": 2.1904,
      "step": 36150
    },
    {
      "epoch": 82.18181818181819,
      "grad_norm": 0.2028762847185135,
      "learning_rate": 0.0003235411004438741,
      "loss": 2.1904,
      "step": 36160
    },
    {
      "epoch": 82.20454545454545,
      "grad_norm": 0.18426074087619781,
      "learning_rate": 0.0003234549331192338,
      "loss": 2.2016,
      "step": 36170
    },
    {
      "epoch": 82.22727272727273,
      "grad_norm": 0.3780826926231384,
      "learning_rate": 0.00032336875624279894,
      "loss": 2.1887,
      "step": 36180
    },
    {
      "epoch": 82.25,
      "grad_norm": 0.22659555077552795,
      "learning_rate": 0.0003232825698257755,
      "loss": 2.1953,
      "step": 36190
    },
    {
      "epoch": 82.27272727272727,
      "grad_norm": 0.17816175520420074,
      "learning_rate": 0.00032319637387937097,
      "loss": 2.1826,
      "step": 36200
    },
    {
      "epoch": 82.29545454545455,
      "grad_norm": 0.19603382050991058,
      "learning_rate": 0.0003231101684147939,
      "loss": 2.1994,
      "step": 36210
    },
    {
      "epoch": 82.31818181818181,
      "grad_norm": 0.26041150093078613,
      "learning_rate": 0.000323023953443254,
      "loss": 2.196,
      "step": 36220
    },
    {
      "epoch": 82.3409090909091,
      "grad_norm": 0.15160565078258514,
      "learning_rate": 0.0003229377289759626,
      "loss": 2.1903,
      "step": 36230
    },
    {
      "epoch": 82.36363636363636,
      "grad_norm": 0.2121201455593109,
      "learning_rate": 0.0003228514950241317,
      "loss": 2.1964,
      "step": 36240
    },
    {
      "epoch": 82.38636363636364,
      "grad_norm": 0.26683375239372253,
      "learning_rate": 0.000322765251598975,
      "loss": 2.1911,
      "step": 36250
    },
    {
      "epoch": 82.4090909090909,
      "grad_norm": 0.30721598863601685,
      "learning_rate": 0.00032267899871170715,
      "loss": 2.1857,
      "step": 36260
    },
    {
      "epoch": 82.43181818181819,
      "grad_norm": 0.22292502224445343,
      "learning_rate": 0.0003225927363735442,
      "loss": 2.2022,
      "step": 36270
    },
    {
      "epoch": 82.45454545454545,
      "grad_norm": 0.15938816964626312,
      "learning_rate": 0.00032250646459570345,
      "loss": 2.1989,
      "step": 36280
    },
    {
      "epoch": 82.47727272727273,
      "grad_norm": 0.19307014346122742,
      "learning_rate": 0.00032242018338940313,
      "loss": 2.1989,
      "step": 36290
    },
    {
      "epoch": 82.5,
      "grad_norm": 0.22704565525054932,
      "learning_rate": 0.00032233389276586325,
      "loss": 2.1892,
      "step": 36300
    },
    {
      "epoch": 82.52272727272727,
      "grad_norm": 0.19620713591575623,
      "learning_rate": 0.0003222475927363044,
      "loss": 2.1899,
      "step": 36310
    },
    {
      "epoch": 82.54545454545455,
      "grad_norm": 0.2739114463329315,
      "learning_rate": 0.0003221612833119488,
      "loss": 2.1871,
      "step": 36320
    },
    {
      "epoch": 82.56818181818181,
      "grad_norm": 0.3403335213661194,
      "learning_rate": 0.0003220749645040198,
      "loss": 2.1934,
      "step": 36330
    },
    {
      "epoch": 82.5909090909091,
      "grad_norm": 0.28370344638824463,
      "learning_rate": 0.00032198863632374196,
      "loss": 2.197,
      "step": 36340
    },
    {
      "epoch": 82.61363636363636,
      "grad_norm": 0.5616154074668884,
      "learning_rate": 0.00032190229878234115,
      "loss": 2.1866,
      "step": 36350
    },
    {
      "epoch": 82.63636363636364,
      "grad_norm": 0.3171466588973999,
      "learning_rate": 0.00032181595189104427,
      "loss": 2.1947,
      "step": 36360
    },
    {
      "epoch": 82.6590909090909,
      "grad_norm": 0.2381942719221115,
      "learning_rate": 0.0003217295956610795,
      "loss": 2.1855,
      "step": 36370
    },
    {
      "epoch": 82.68181818181819,
      "grad_norm": 0.4058211147785187,
      "learning_rate": 0.00032164323010367633,
      "loss": 2.211,
      "step": 36380
    },
    {
      "epoch": 82.70454545454545,
      "grad_norm": 0.528011679649353,
      "learning_rate": 0.00032155685523006545,
      "loss": 2.1927,
      "step": 36390
    },
    {
      "epoch": 82.72727272727273,
      "grad_norm": 0.16559526324272156,
      "learning_rate": 0.0003214704710514786,
      "loss": 2.1999,
      "step": 36400
    },
    {
      "epoch": 82.75,
      "grad_norm": 0.17409202456474304,
      "learning_rate": 0.0003213840775791489,
      "loss": 2.2054,
      "step": 36410
    },
    {
      "epoch": 82.77272727272727,
      "grad_norm": 0.25281426310539246,
      "learning_rate": 0.00032129767482431065,
      "loss": 2.1849,
      "step": 36420
    },
    {
      "epoch": 82.79545454545455,
      "grad_norm": 0.17194883525371552,
      "learning_rate": 0.00032121126279819934,
      "loss": 2.2006,
      "step": 36430
    },
    {
      "epoch": 82.81818181818181,
      "grad_norm": 0.17961867153644562,
      "learning_rate": 0.00032112484151205155,
      "loss": 2.1899,
      "step": 36440
    },
    {
      "epoch": 82.8409090909091,
      "grad_norm": 0.20008113980293274,
      "learning_rate": 0.0003210384109771052,
      "loss": 2.1911,
      "step": 36450
    },
    {
      "epoch": 82.86363636363636,
      "grad_norm": 0.35687732696533203,
      "learning_rate": 0.00032095197120459953,
      "loss": 2.2034,
      "step": 36460
    },
    {
      "epoch": 82.88636363636364,
      "grad_norm": 0.13978254795074463,
      "learning_rate": 0.0003208655222057747,
      "loss": 2.2025,
      "step": 36470
    },
    {
      "epoch": 82.9090909090909,
      "grad_norm": 1.7303162813186646,
      "learning_rate": 0.00032077906399187217,
      "loss": 2.1854,
      "step": 36480
    },
    {
      "epoch": 82.93181818181819,
      "grad_norm": 0.2460843026638031,
      "learning_rate": 0.00032069259657413474,
      "loss": 2.1999,
      "step": 36490
    },
    {
      "epoch": 82.95454545454545,
      "grad_norm": 0.11763320863246918,
      "learning_rate": 0.00032060611996380617,
      "loss": 2.1903,
      "step": 36500
    },
    {
      "epoch": 82.97727272727273,
      "grad_norm": 0.43126124143600464,
      "learning_rate": 0.0003205196341721317,
      "loss": 2.1845,
      "step": 36510
    },
    {
      "epoch": 83.0,
      "grad_norm": 0.31975802779197693,
      "learning_rate": 0.00032043313921035745,
      "loss": 2.1919,
      "step": 36520
    },
    {
      "epoch": 83.0,
      "eval_loss": 1.0990145206451416,
      "eval_runtime": 8.7169,
      "eval_samples_per_second": 3491.037,
      "eval_steps_per_second": 13.652,
      "step": 36520
    },
    {
      "epoch": 83.02272727272727,
      "grad_norm": 0.1680874228477478,
      "learning_rate": 0.00032034663508973097,
      "loss": 2.1877,
      "step": 36530
    },
    {
      "epoch": 83.04545454545455,
      "grad_norm": 0.15236152708530426,
      "learning_rate": 0.000320260121821501,
      "loss": 2.1964,
      "step": 36540
    },
    {
      "epoch": 83.06818181818181,
      "grad_norm": 0.1806318759918213,
      "learning_rate": 0.00032017359941691713,
      "loss": 2.1974,
      "step": 36550
    },
    {
      "epoch": 83.0909090909091,
      "grad_norm": 0.48478105664253235,
      "learning_rate": 0.00032008706788723076,
      "loss": 2.1975,
      "step": 36560
    },
    {
      "epoch": 83.11363636363636,
      "grad_norm": 0.24845299124717712,
      "learning_rate": 0.00032000052724369377,
      "loss": 2.189,
      "step": 36570
    },
    {
      "epoch": 83.13636363636364,
      "grad_norm": 0.14853639900684357,
      "learning_rate": 0.0003199139774975598,
      "loss": 2.1787,
      "step": 36580
    },
    {
      "epoch": 83.1590909090909,
      "grad_norm": 0.22582945227622986,
      "learning_rate": 0.00031982741866008334,
      "loss": 2.1918,
      "step": 36590
    },
    {
      "epoch": 83.18181818181819,
      "grad_norm": 0.2796008884906769,
      "learning_rate": 0.00031974085074252027,
      "loss": 2.1995,
      "step": 36600
    },
    {
      "epoch": 83.20454545454545,
      "grad_norm": 0.1539488434791565,
      "learning_rate": 0.00031965427375612736,
      "loss": 2.1846,
      "step": 36610
    },
    {
      "epoch": 83.22727272727273,
      "grad_norm": 0.2058613896369934,
      "learning_rate": 0.0003195676877121628,
      "loss": 2.1734,
      "step": 36620
    },
    {
      "epoch": 83.25,
      "grad_norm": 0.22072157263755798,
      "learning_rate": 0.0003194810926218861,
      "loss": 2.1877,
      "step": 36630
    },
    {
      "epoch": 83.27272727272727,
      "grad_norm": 0.3450469374656677,
      "learning_rate": 0.00031939448849655763,
      "loss": 2.1879,
      "step": 36640
    },
    {
      "epoch": 83.29545454545455,
      "grad_norm": 0.17514580488204956,
      "learning_rate": 0.00031930787534743886,
      "loss": 2.1848,
      "step": 36650
    },
    {
      "epoch": 83.31818181818181,
      "grad_norm": 0.1856006532907486,
      "learning_rate": 0.000319221253185793,
      "loss": 2.1867,
      "step": 36660
    },
    {
      "epoch": 83.3409090909091,
      "grad_norm": 0.3161715865135193,
      "learning_rate": 0.0003191346220228837,
      "loss": 2.1911,
      "step": 36670
    },
    {
      "epoch": 83.36363636363636,
      "grad_norm": 0.20961743593215942,
      "learning_rate": 0.00031904798186997637,
      "loss": 2.1831,
      "step": 36680
    },
    {
      "epoch": 83.38636363636364,
      "grad_norm": 0.19220300018787384,
      "learning_rate": 0.00031896133273833725,
      "loss": 2.1849,
      "step": 36690
    },
    {
      "epoch": 83.4090909090909,
      "grad_norm": 0.20750319957733154,
      "learning_rate": 0.00031887467463923393,
      "loss": 2.194,
      "step": 36700
    },
    {
      "epoch": 83.43181818181819,
      "grad_norm": 0.3012917935848236,
      "learning_rate": 0.0003187880075839351,
      "loss": 2.205,
      "step": 36710
    },
    {
      "epoch": 83.45454545454545,
      "grad_norm": 0.7894511818885803,
      "learning_rate": 0.00031870133158371053,
      "loss": 2.1851,
      "step": 36720
    },
    {
      "epoch": 83.47727272727273,
      "grad_norm": 0.18294519186019897,
      "learning_rate": 0.00031861464664983117,
      "loss": 2.1911,
      "step": 36730
    },
    {
      "epoch": 83.5,
      "grad_norm": 0.4317564070224762,
      "learning_rate": 0.00031852795279356945,
      "loss": 2.206,
      "step": 36740
    },
    {
      "epoch": 83.52272727272727,
      "grad_norm": 0.7117616534233093,
      "learning_rate": 0.0003184412500261984,
      "loss": 2.1906,
      "step": 36750
    },
    {
      "epoch": 83.54545454545455,
      "grad_norm": 0.18788009881973267,
      "learning_rate": 0.00031835453835899277,
      "loss": 2.2032,
      "step": 36760
    },
    {
      "epoch": 83.56818181818181,
      "grad_norm": 0.7305402159690857,
      "learning_rate": 0.00031826781780322803,
      "loss": 2.1938,
      "step": 36770
    },
    {
      "epoch": 83.5909090909091,
      "grad_norm": 0.2966860830783844,
      "learning_rate": 0.00031818108837018114,
      "loss": 2.1882,
      "step": 36780
    },
    {
      "epoch": 83.61363636363636,
      "grad_norm": 0.2101108580827713,
      "learning_rate": 0.00031809435007112985,
      "loss": 2.1973,
      "step": 36790
    },
    {
      "epoch": 83.63636363636364,
      "grad_norm": 0.25495240092277527,
      "learning_rate": 0.00031800760291735345,
      "loss": 2.1931,
      "step": 36800
    },
    {
      "epoch": 83.6590909090909,
      "grad_norm": 0.4563910961151123,
      "learning_rate": 0.0003179208469201321,
      "loss": 2.1942,
      "step": 36810
    },
    {
      "epoch": 83.68181818181819,
      "grad_norm": 0.4026612639427185,
      "learning_rate": 0.00031783408209074725,
      "loss": 2.2094,
      "step": 36820
    },
    {
      "epoch": 83.70454545454545,
      "grad_norm": 0.1792779117822647,
      "learning_rate": 0.0003177473084404815,
      "loss": 2.1964,
      "step": 36830
    },
    {
      "epoch": 83.72727272727273,
      "grad_norm": 0.13904047012329102,
      "learning_rate": 0.00031766052598061853,
      "loss": 2.201,
      "step": 36840
    },
    {
      "epoch": 83.75,
      "grad_norm": 0.34600648283958435,
      "learning_rate": 0.00031757373472244323,
      "loss": 2.1898,
      "step": 36850
    },
    {
      "epoch": 83.77272727272727,
      "grad_norm": 2.040032386779785,
      "learning_rate": 0.0003174869346772415,
      "loss": 2.1875,
      "step": 36860
    },
    {
      "epoch": 83.79545454545455,
      "grad_norm": 0.1475684940814972,
      "learning_rate": 0.00031740012585630055,
      "loss": 2.2012,
      "step": 36870
    },
    {
      "epoch": 83.81818181818181,
      "grad_norm": 0.23499535024166107,
      "learning_rate": 0.00031731330827090864,
      "loss": 2.1924,
      "step": 36880
    },
    {
      "epoch": 83.8409090909091,
      "grad_norm": 0.5434079766273499,
      "learning_rate": 0.0003172264819323553,
      "loss": 2.206,
      "step": 36890
    },
    {
      "epoch": 83.86363636363636,
      "grad_norm": 0.21159398555755615,
      "learning_rate": 0.0003171396468519309,
      "loss": 2.1969,
      "step": 36900
    },
    {
      "epoch": 83.88636363636364,
      "grad_norm": 0.24102170765399933,
      "learning_rate": 0.0003170528030409272,
      "loss": 2.1921,
      "step": 36910
    },
    {
      "epoch": 83.9090909090909,
      "grad_norm": 0.16675011813640594,
      "learning_rate": 0.00031696595051063716,
      "loss": 2.203,
      "step": 36920
    },
    {
      "epoch": 83.93181818181819,
      "grad_norm": 0.2534349858760834,
      "learning_rate": 0.0003168790892723546,
      "loss": 2.1909,
      "step": 36930
    },
    {
      "epoch": 83.95454545454545,
      "grad_norm": 0.3484763205051422,
      "learning_rate": 0.0003167922193373747,
      "loss": 2.1938,
      "step": 36940
    },
    {
      "epoch": 83.97727272727273,
      "grad_norm": 0.1849423199892044,
      "learning_rate": 0.0003167053407169936,
      "loss": 2.1958,
      "step": 36950
    },
    {
      "epoch": 84.0,
      "grad_norm": 0.7745547294616699,
      "learning_rate": 0.0003166184534225087,
      "loss": 2.1927,
      "step": 36960
    },
    {
      "epoch": 84.0,
      "eval_loss": 1.0991538763046265,
      "eval_runtime": 8.6809,
      "eval_samples_per_second": 3505.493,
      "eval_steps_per_second": 13.708,
      "step": 36960
    },
    {
      "epoch": 84.02272727272727,
      "grad_norm": 0.3209682106971741,
      "learning_rate": 0.0003165315574652187,
      "loss": 2.1872,
      "step": 36970
    },
    {
      "epoch": 84.04545454545455,
      "grad_norm": 0.24632133543491364,
      "learning_rate": 0.0003164446528564228,
      "loss": 2.1808,
      "step": 36980
    },
    {
      "epoch": 84.06818181818181,
      "grad_norm": 0.14878737926483154,
      "learning_rate": 0.0003163577396074221,
      "loss": 2.2009,
      "step": 36990
    },
    {
      "epoch": 84.0909090909091,
      "grad_norm": 0.18018484115600586,
      "learning_rate": 0.00031627081772951817,
      "loss": 2.1852,
      "step": 37000
    },
    {
      "epoch": 84.11363636363636,
      "grad_norm": 0.265784353017807,
      "learning_rate": 0.0003161838872340143,
      "loss": 2.191,
      "step": 37010
    },
    {
      "epoch": 84.13636363636364,
      "grad_norm": 0.20160305500030518,
      "learning_rate": 0.00031609694813221437,
      "loss": 2.1937,
      "step": 37020
    },
    {
      "epoch": 84.1590909090909,
      "grad_norm": 0.19631554186344147,
      "learning_rate": 0.00031601000043542365,
      "loss": 2.1906,
      "step": 37030
    },
    {
      "epoch": 84.18181818181819,
      "grad_norm": 0.1535925716161728,
      "learning_rate": 0.00031592304415494854,
      "loss": 2.1886,
      "step": 37040
    },
    {
      "epoch": 84.20454545454545,
      "grad_norm": 0.1809050589799881,
      "learning_rate": 0.0003158360793020964,
      "loss": 2.1895,
      "step": 37050
    },
    {
      "epoch": 84.22727272727273,
      "grad_norm": 0.29139694571495056,
      "learning_rate": 0.00031574910588817593,
      "loss": 2.1835,
      "step": 37060
    },
    {
      "epoch": 84.25,
      "grad_norm": 0.200238436460495,
      "learning_rate": 0.00031566212392449667,
      "loss": 2.189,
      "step": 37070
    },
    {
      "epoch": 84.27272727272727,
      "grad_norm": 0.31528520584106445,
      "learning_rate": 0.0003155751334223695,
      "loss": 2.18,
      "step": 37080
    },
    {
      "epoch": 84.29545454545455,
      "grad_norm": 0.22541594505310059,
      "learning_rate": 0.0003154881343931064,
      "loss": 2.1806,
      "step": 37090
    },
    {
      "epoch": 84.31818181818181,
      "grad_norm": 0.24552029371261597,
      "learning_rate": 0.00031540112684802024,
      "loss": 2.1965,
      "step": 37100
    },
    {
      "epoch": 84.3409090909091,
      "grad_norm": 0.18361300230026245,
      "learning_rate": 0.0003153141107984253,
      "loss": 2.1858,
      "step": 37110
    },
    {
      "epoch": 84.36363636363636,
      "grad_norm": 0.28077155351638794,
      "learning_rate": 0.00031522708625563665,
      "loss": 2.2011,
      "step": 37120
    },
    {
      "epoch": 84.38636363636364,
      "grad_norm": 0.27494293451309204,
      "learning_rate": 0.0003151400532309707,
      "loss": 2.1956,
      "step": 37130
    },
    {
      "epoch": 84.4090909090909,
      "grad_norm": 0.8203127384185791,
      "learning_rate": 0.00031505301173574484,
      "loss": 2.19,
      "step": 37140
    },
    {
      "epoch": 84.43181818181819,
      "grad_norm": 0.3793644905090332,
      "learning_rate": 0.0003149659617812777,
      "loss": 2.1866,
      "step": 37150
    },
    {
      "epoch": 84.45454545454545,
      "grad_norm": 0.2008201777935028,
      "learning_rate": 0.00031487890337888894,
      "loss": 2.2044,
      "step": 37160
    },
    {
      "epoch": 84.47727272727273,
      "grad_norm": 0.19173939526081085,
      "learning_rate": 0.0003147918365398991,
      "loss": 2.1971,
      "step": 37170
    },
    {
      "epoch": 84.5,
      "grad_norm": 0.22391277551651,
      "learning_rate": 0.00031470476127563017,
      "loss": 2.197,
      "step": 37180
    },
    {
      "epoch": 84.52272727272727,
      "grad_norm": 0.1780225783586502,
      "learning_rate": 0.0003146176775974051,
      "loss": 2.1909,
      "step": 37190
    },
    {
      "epoch": 84.54545454545455,
      "grad_norm": 0.19286465644836426,
      "learning_rate": 0.00031453058551654783,
      "loss": 2.1836,
      "step": 37200
    },
    {
      "epoch": 84.56818181818181,
      "grad_norm": 0.18056467175483704,
      "learning_rate": 0.00031444348504438354,
      "loss": 2.2127,
      "step": 37210
    },
    {
      "epoch": 84.5909090909091,
      "grad_norm": 0.1753373146057129,
      "learning_rate": 0.0003143563761922384,
      "loss": 2.2023,
      "step": 37220
    },
    {
      "epoch": 84.61363636363636,
      "grad_norm": 0.22448010742664337,
      "learning_rate": 0.00031426925897143967,
      "loss": 2.1935,
      "step": 37230
    },
    {
      "epoch": 84.63636363636364,
      "grad_norm": 0.26362794637680054,
      "learning_rate": 0.0003141821333933158,
      "loss": 2.1977,
      "step": 37240
    },
    {
      "epoch": 84.6590909090909,
      "grad_norm": 0.1391487717628479,
      "learning_rate": 0.0003140949994691962,
      "loss": 2.1942,
      "step": 37250
    },
    {
      "epoch": 84.68181818181819,
      "grad_norm": 0.4560099244117737,
      "learning_rate": 0.00031400785721041153,
      "loss": 2.1892,
      "step": 37260
    },
    {
      "epoch": 84.70454545454545,
      "grad_norm": 0.18123602867126465,
      "learning_rate": 0.00031392070662829334,
      "loss": 2.1973,
      "step": 37270
    },
    {
      "epoch": 84.72727272727273,
      "grad_norm": 0.119949109852314,
      "learning_rate": 0.00031383354773417444,
      "loss": 2.1945,
      "step": 37280
    },
    {
      "epoch": 84.75,
      "grad_norm": 0.12815171480178833,
      "learning_rate": 0.0003137463805393885,
      "loss": 2.1891,
      "step": 37290
    },
    {
      "epoch": 84.77272727272727,
      "grad_norm": 0.1510898321866989,
      "learning_rate": 0.00031365920505527055,
      "loss": 2.1908,
      "step": 37300
    },
    {
      "epoch": 84.79545454545455,
      "grad_norm": 0.14499686658382416,
      "learning_rate": 0.00031357202129315645,
      "loss": 2.1951,
      "step": 37310
    },
    {
      "epoch": 84.81818181818181,
      "grad_norm": 0.192730113863945,
      "learning_rate": 0.0003134848292643833,
      "loss": 2.1903,
      "step": 37320
    },
    {
      "epoch": 84.8409090909091,
      "grad_norm": 0.4181000590324402,
      "learning_rate": 0.00031339762898028917,
      "loss": 2.1949,
      "step": 37330
    },
    {
      "epoch": 84.86363636363636,
      "grad_norm": 0.18176034092903137,
      "learning_rate": 0.00031331042045221327,
      "loss": 2.1886,
      "step": 37340
    },
    {
      "epoch": 84.88636363636364,
      "grad_norm": 0.1693294644355774,
      "learning_rate": 0.00031322320369149586,
      "loss": 2.1897,
      "step": 37350
    },
    {
      "epoch": 84.9090909090909,
      "grad_norm": 0.44109559059143066,
      "learning_rate": 0.0003131359787094782,
      "loss": 2.1913,
      "step": 37360
    },
    {
      "epoch": 84.93181818181819,
      "grad_norm": 0.5251188278198242,
      "learning_rate": 0.0003130487455175028,
      "loss": 2.2006,
      "step": 37370
    },
    {
      "epoch": 84.95454545454545,
      "grad_norm": 0.1873175948858261,
      "learning_rate": 0.00031296150412691307,
      "loss": 2.1866,
      "step": 37380
    },
    {
      "epoch": 84.97727272727273,
      "grad_norm": 0.18554577231407166,
      "learning_rate": 0.0003128742545490535,
      "loss": 2.1826,
      "step": 37390
    },
    {
      "epoch": 85.0,
      "grad_norm": 0.29925766587257385,
      "learning_rate": 0.00031278699679526976,
      "loss": 2.2029,
      "step": 37400
    },
    {
      "epoch": 85.0,
      "eval_loss": 1.0990245342254639,
      "eval_runtime": 8.6986,
      "eval_samples_per_second": 3498.394,
      "eval_steps_per_second": 13.68,
      "step": 37400
    },
    {
      "epoch": 85.02272727272727,
      "grad_norm": 0.17999513447284698,
      "learning_rate": 0.0003126997308769086,
      "loss": 2.1923,
      "step": 37410
    },
    {
      "epoch": 85.04545454545455,
      "grad_norm": 0.230402410030365,
      "learning_rate": 0.0003126124568053175,
      "loss": 2.1934,
      "step": 37420
    },
    {
      "epoch": 85.06818181818181,
      "grad_norm": 0.23068638145923615,
      "learning_rate": 0.0003125251745918454,
      "loss": 2.1883,
      "step": 37430
    },
    {
      "epoch": 85.0909090909091,
      "grad_norm": 0.22586625814437866,
      "learning_rate": 0.0003124378842478421,
      "loss": 2.1911,
      "step": 37440
    },
    {
      "epoch": 85.11363636363636,
      "grad_norm": 0.12443150579929352,
      "learning_rate": 0.0003123505857846585,
      "loss": 2.1903,
      "step": 37450
    },
    {
      "epoch": 85.13636363636364,
      "grad_norm": 0.15479300916194916,
      "learning_rate": 0.0003122632792136466,
      "loss": 2.183,
      "step": 37460
    },
    {
      "epoch": 85.1590909090909,
      "grad_norm": 0.25977274775505066,
      "learning_rate": 0.00031217596454615935,
      "loss": 2.1808,
      "step": 37470
    },
    {
      "epoch": 85.18181818181819,
      "grad_norm": 0.23659232258796692,
      "learning_rate": 0.00031208864179355073,
      "loss": 2.1863,
      "step": 37480
    },
    {
      "epoch": 85.20454545454545,
      "grad_norm": 0.22490531206130981,
      "learning_rate": 0.00031200131096717606,
      "loss": 2.1944,
      "step": 37490
    },
    {
      "epoch": 85.22727272727273,
      "grad_norm": 0.2035495638847351,
      "learning_rate": 0.0003119139720783914,
      "loss": 2.1983,
      "step": 37500
    },
    {
      "epoch": 85.25,
      "grad_norm": 0.25018826127052307,
      "learning_rate": 0.00031182662513855384,
      "loss": 2.1916,
      "step": 37510
    },
    {
      "epoch": 85.27272727272727,
      "grad_norm": 0.18715251982212067,
      "learning_rate": 0.0003117392701590219,
      "loss": 2.1899,
      "step": 37520
    },
    {
      "epoch": 85.29545454545455,
      "grad_norm": 0.17896971106529236,
      "learning_rate": 0.0003116519071511547,
      "loss": 2.1879,
      "step": 37530
    },
    {
      "epoch": 85.31818181818181,
      "grad_norm": 0.2973746955394745,
      "learning_rate": 0.0003115645361263125,
      "loss": 2.1929,
      "step": 37540
    },
    {
      "epoch": 85.3409090909091,
      "grad_norm": 0.15477602183818817,
      "learning_rate": 0.0003114771570958569,
      "loss": 2.2008,
      "step": 37550
    },
    {
      "epoch": 85.36363636363636,
      "grad_norm": 0.14174994826316833,
      "learning_rate": 0.0003113897700711502,
      "loss": 2.2062,
      "step": 37560
    },
    {
      "epoch": 85.38636363636364,
      "grad_norm": 0.1931745558977127,
      "learning_rate": 0.00031130237506355596,
      "loss": 2.1912,
      "step": 37570
    },
    {
      "epoch": 85.4090909090909,
      "grad_norm": 0.18037022650241852,
      "learning_rate": 0.0003112149720844386,
      "loss": 2.2001,
      "step": 37580
    },
    {
      "epoch": 85.43181818181819,
      "grad_norm": 0.1587960571050644,
      "learning_rate": 0.0003111275611451637,
      "loss": 2.1882,
      "step": 37590
    },
    {
      "epoch": 85.45454545454545,
      "grad_norm": 0.14444144070148468,
      "learning_rate": 0.00031104014225709787,
      "loss": 2.1975,
      "step": 37600
    },
    {
      "epoch": 85.47727272727273,
      "grad_norm": 0.24299979209899902,
      "learning_rate": 0.0003109527154316086,
      "loss": 2.1954,
      "step": 37610
    },
    {
      "epoch": 85.5,
      "grad_norm": 0.16463904082775116,
      "learning_rate": 0.00031086528068006476,
      "loss": 2.1974,
      "step": 37620
    },
    {
      "epoch": 85.52272727272727,
      "grad_norm": 0.164509579539299,
      "learning_rate": 0.0003107778380138359,
      "loss": 2.1911,
      "step": 37630
    },
    {
      "epoch": 85.54545454545455,
      "grad_norm": 0.1390412598848343,
      "learning_rate": 0.0003106903874442926,
      "loss": 2.1886,
      "step": 37640
    },
    {
      "epoch": 85.56818181818181,
      "grad_norm": 0.18384459614753723,
      "learning_rate": 0.0003106029289828068,
      "loss": 2.1918,
      "step": 37650
    },
    {
      "epoch": 85.5909090909091,
      "grad_norm": 0.13855427503585815,
      "learning_rate": 0.0003105154626407511,
      "loss": 2.201,
      "step": 37660
    },
    {
      "epoch": 85.61363636363636,
      "grad_norm": 0.2063073217868805,
      "learning_rate": 0.0003104279884294994,
      "loss": 2.1872,
      "step": 37670
    },
    {
      "epoch": 85.63636363636364,
      "grad_norm": 0.5886663794517517,
      "learning_rate": 0.00031034050636042645,
      "loss": 2.1939,
      "step": 37680
    },
    {
      "epoch": 85.6590909090909,
      "grad_norm": 0.30456051230430603,
      "learning_rate": 0.00031025301644490807,
      "loss": 2.1908,
      "step": 37690
    },
    {
      "epoch": 85.68181818181819,
      "grad_norm": 0.2978059649467468,
      "learning_rate": 0.00031016551869432116,
      "loss": 2.1892,
      "step": 37700
    },
    {
      "epoch": 85.70454545454545,
      "grad_norm": 0.38235804438591003,
      "learning_rate": 0.0003100780131200435,
      "loss": 2.1899,
      "step": 37710
    },
    {
      "epoch": 85.72727272727273,
      "grad_norm": 0.19475920498371124,
      "learning_rate": 0.0003099904997334541,
      "loss": 2.1707,
      "step": 37720
    },
    {
      "epoch": 85.75,
      "grad_norm": 0.21284273266792297,
      "learning_rate": 0.0003099029785459328,
      "loss": 2.1873,
      "step": 37730
    },
    {
      "epoch": 85.77272727272727,
      "grad_norm": 0.215089350938797,
      "learning_rate": 0.0003098154495688605,
      "loss": 2.1943,
      "step": 37740
    },
    {
      "epoch": 85.79545454545455,
      "grad_norm": 0.27615174651145935,
      "learning_rate": 0.0003097279128136191,
      "loss": 2.1982,
      "step": 37750
    },
    {
      "epoch": 85.81818181818181,
      "grad_norm": 0.18432244658470154,
      "learning_rate": 0.00030964036829159164,
      "loss": 2.1856,
      "step": 37760
    },
    {
      "epoch": 85.8409090909091,
      "grad_norm": 0.1480497270822525,
      "learning_rate": 0.000309552816014162,
      "loss": 2.1914,
      "step": 37770
    },
    {
      "epoch": 85.86363636363636,
      "grad_norm": 0.24462832510471344,
      "learning_rate": 0.0003094652559927151,
      "loss": 2.1971,
      "step": 37780
    },
    {
      "epoch": 85.88636363636364,
      "grad_norm": 0.14492730796337128,
      "learning_rate": 0.000309377688238637,
      "loss": 2.1946,
      "step": 37790
    },
    {
      "epoch": 85.9090909090909,
      "grad_norm": 0.2463151216506958,
      "learning_rate": 0.00030929011276331457,
      "loss": 2.1934,
      "step": 37800
    },
    {
      "epoch": 85.93181818181819,
      "grad_norm": 0.18637438118457794,
      "learning_rate": 0.00030920252957813586,
      "loss": 2.1959,
      "step": 37810
    },
    {
      "epoch": 85.95454545454545,
      "grad_norm": 0.18162000179290771,
      "learning_rate": 0.00030911493869448985,
      "loss": 2.1917,
      "step": 37820
    },
    {
      "epoch": 85.97727272727273,
      "grad_norm": 0.16932469606399536,
      "learning_rate": 0.0003090273401237665,
      "loss": 2.2085,
      "step": 37830
    },
    {
      "epoch": 86.0,
      "grad_norm": 0.2507013976573944,
      "learning_rate": 0.0003089397338773569,
      "loss": 2.1891,
      "step": 37840
    },
    {
      "epoch": 86.0,
      "eval_loss": 1.0990965366363525,
      "eval_runtime": 8.7176,
      "eval_samples_per_second": 3490.761,
      "eval_steps_per_second": 13.651,
      "step": 37840
    },
    {
      "epoch": 86.02272727272727,
      "grad_norm": 0.3067508637905121,
      "learning_rate": 0.00030885211996665274,
      "loss": 2.1895,
      "step": 37850
    },
    {
      "epoch": 86.04545454545455,
      "grad_norm": 0.16729877889156342,
      "learning_rate": 0.00030876449840304713,
      "loss": 2.1844,
      "step": 37860
    },
    {
      "epoch": 86.06818181818181,
      "grad_norm": 0.16505229473114014,
      "learning_rate": 0.0003086768691979342,
      "loss": 2.1903,
      "step": 37870
    },
    {
      "epoch": 86.0909090909091,
      "grad_norm": 0.16426599025726318,
      "learning_rate": 0.00030858923236270873,
      "loss": 2.1882,
      "step": 37880
    },
    {
      "epoch": 86.11363636363636,
      "grad_norm": 0.635104238986969,
      "learning_rate": 0.0003085015879087668,
      "loss": 2.1774,
      "step": 37890
    },
    {
      "epoch": 86.13636363636364,
      "grad_norm": 0.16111107170581818,
      "learning_rate": 0.00030841393584750523,
      "loss": 2.1911,
      "step": 37900
    },
    {
      "epoch": 86.1590909090909,
      "grad_norm": 0.12538418173789978,
      "learning_rate": 0.000308326276190322,
      "loss": 2.1857,
      "step": 37910
    },
    {
      "epoch": 86.18181818181819,
      "grad_norm": 0.15712490677833557,
      "learning_rate": 0.00030823860894861605,
      "loss": 2.1828,
      "step": 37920
    },
    {
      "epoch": 86.20454545454545,
      "grad_norm": 0.16496707499027252,
      "learning_rate": 0.00030815093413378723,
      "loss": 2.1943,
      "step": 37930
    },
    {
      "epoch": 86.22727272727273,
      "grad_norm": 0.27579620480537415,
      "learning_rate": 0.00030806325175723655,
      "loss": 2.1752,
      "step": 37940
    },
    {
      "epoch": 86.25,
      "grad_norm": 0.21908093988895416,
      "learning_rate": 0.0003079755618303658,
      "loss": 2.1911,
      "step": 37950
    },
    {
      "epoch": 86.27272727272727,
      "grad_norm": 0.3859269917011261,
      "learning_rate": 0.0003078878643645778,
      "loss": 2.1952,
      "step": 37960
    },
    {
      "epoch": 86.29545454545455,
      "grad_norm": 0.3519856035709381,
      "learning_rate": 0.00030780015937127654,
      "loss": 2.2033,
      "step": 37970
    },
    {
      "epoch": 86.31818181818181,
      "grad_norm": 0.14910635352134705,
      "learning_rate": 0.0003077124468618667,
      "loss": 2.1992,
      "step": 37980
    },
    {
      "epoch": 86.3409090909091,
      "grad_norm": 0.257927805185318,
      "learning_rate": 0.0003076247268477541,
      "loss": 2.1889,
      "step": 37990
    },
    {
      "epoch": 86.36363636363636,
      "grad_norm": 0.1777152121067047,
      "learning_rate": 0.0003075369993403455,
      "loss": 2.1858,
      "step": 38000
    },
    {
      "epoch": 86.38636363636364,
      "grad_norm": 0.28473907709121704,
      "learning_rate": 0.00030744926435104876,
      "loss": 2.1918,
      "step": 38010
    },
    {
      "epoch": 86.4090909090909,
      "grad_norm": 0.258558064699173,
      "learning_rate": 0.0003073615218912724,
      "loss": 2.1838,
      "step": 38020
    },
    {
      "epoch": 86.43181818181819,
      "grad_norm": 0.23177100718021393,
      "learning_rate": 0.00030727377197242636,
      "loss": 2.1929,
      "step": 38030
    },
    {
      "epoch": 86.45454545454545,
      "grad_norm": 0.2882334589958191,
      "learning_rate": 0.000307186014605921,
      "loss": 2.1871,
      "step": 38040
    },
    {
      "epoch": 86.47727272727273,
      "grad_norm": 0.8046669960021973,
      "learning_rate": 0.0003070982498031682,
      "loss": 2.2,
      "step": 38050
    },
    {
      "epoch": 86.5,
      "grad_norm": 0.20189392566680908,
      "learning_rate": 0.0003070104775755804,
      "loss": 2.1962,
      "step": 38060
    },
    {
      "epoch": 86.52272727272727,
      "grad_norm": 0.18689386546611786,
      "learning_rate": 0.00030692269793457125,
      "loss": 2.1932,
      "step": 38070
    },
    {
      "epoch": 86.54545454545455,
      "grad_norm": 0.22517062723636627,
      "learning_rate": 0.0003068349108915553,
      "loss": 2.1907,
      "step": 38080
    },
    {
      "epoch": 86.56818181818181,
      "grad_norm": 0.16931821405887604,
      "learning_rate": 0.00030674711645794784,
      "loss": 2.1913,
      "step": 38090
    },
    {
      "epoch": 86.5909090909091,
      "grad_norm": 0.23669904470443726,
      "learning_rate": 0.00030665931464516553,
      "loss": 2.1993,
      "step": 38100
    },
    {
      "epoch": 86.61363636363636,
      "grad_norm": 0.16346625983715057,
      "learning_rate": 0.0003065715054646258,
      "loss": 2.1921,
      "step": 38110
    },
    {
      "epoch": 86.63636363636364,
      "grad_norm": 0.3262885510921478,
      "learning_rate": 0.00030648368892774675,
      "loss": 2.1969,
      "step": 38120
    },
    {
      "epoch": 86.6590909090909,
      "grad_norm": 0.2584627866744995,
      "learning_rate": 0.00030639586504594796,
      "loss": 2.193,
      "step": 38130
    },
    {
      "epoch": 86.68181818181819,
      "grad_norm": 0.1457672268152237,
      "learning_rate": 0.0003063080338306496,
      "loss": 2.2053,
      "step": 38140
    },
    {
      "epoch": 86.70454545454545,
      "grad_norm": 0.18484245240688324,
      "learning_rate": 0.00030622019529327293,
      "loss": 2.1928,
      "step": 38150
    },
    {
      "epoch": 86.72727272727273,
      "grad_norm": 0.14301708340644836,
      "learning_rate": 0.00030613234944524013,
      "loss": 2.2035,
      "step": 38160
    },
    {
      "epoch": 86.75,
      "grad_norm": 0.14427053928375244,
      "learning_rate": 0.0003060444962979743,
      "loss": 2.1891,
      "step": 38170
    },
    {
      "epoch": 86.77272727272727,
      "grad_norm": 0.6943256258964539,
      "learning_rate": 0.0003059566358628996,
      "loss": 2.2049,
      "step": 38180
    },
    {
      "epoch": 86.79545454545455,
      "grad_norm": 0.1466522216796875,
      "learning_rate": 0.0003058687681514409,
      "loss": 2.1876,
      "step": 38190
    },
    {
      "epoch": 86.81818181818181,
      "grad_norm": 0.19846543669700623,
      "learning_rate": 0.0003057808931750244,
      "loss": 2.1897,
      "step": 38200
    },
    {
      "epoch": 86.8409090909091,
      "grad_norm": 0.16523216664791107,
      "learning_rate": 0.0003056930109450768,
      "loss": 2.1945,
      "step": 38210
    },
    {
      "epoch": 86.86363636363636,
      "grad_norm": 0.14588716626167297,
      "learning_rate": 0.0003056051214730261,
      "loss": 2.1908,
      "step": 38220
    },
    {
      "epoch": 86.88636363636364,
      "grad_norm": 0.269185334444046,
      "learning_rate": 0.00030551722477030116,
      "loss": 2.1883,
      "step": 38230
    },
    {
      "epoch": 86.9090909090909,
      "grad_norm": 0.2049507051706314,
      "learning_rate": 0.00030542932084833154,
      "loss": 2.2017,
      "step": 38240
    },
    {
      "epoch": 86.93181818181819,
      "grad_norm": 0.1791694611310959,
      "learning_rate": 0.0003053414097185481,
      "loss": 2.1907,
      "step": 38250
    },
    {
      "epoch": 86.95454545454545,
      "grad_norm": 0.5770944952964783,
      "learning_rate": 0.0003052534913923823,
      "loss": 2.1904,
      "step": 38260
    },
    {
      "epoch": 86.97727272727273,
      "grad_norm": 0.18090081214904785,
      "learning_rate": 0.00030516556588126685,
      "loss": 2.1914,
      "step": 38270
    },
    {
      "epoch": 87.0,
      "grad_norm": 0.22781577706336975,
      "learning_rate": 0.00030507763319663516,
      "loss": 2.1999,
      "step": 38280
    },
    {
      "epoch": 87.0,
      "eval_loss": 1.0992175340652466,
      "eval_runtime": 8.7239,
      "eval_samples_per_second": 3488.216,
      "eval_steps_per_second": 13.641,
      "step": 38280
    },
    {
      "epoch": 87.02272727272727,
      "grad_norm": 0.13842520117759705,
      "learning_rate": 0.00030498969334992167,
      "loss": 2.1818,
      "step": 38290
    },
    {
      "epoch": 87.04545454545455,
      "grad_norm": 0.21123728156089783,
      "learning_rate": 0.0003049017463525618,
      "loss": 2.1936,
      "step": 38300
    },
    {
      "epoch": 87.06818181818181,
      "grad_norm": 0.28920406103134155,
      "learning_rate": 0.0003048137922159917,
      "loss": 2.1867,
      "step": 38310
    },
    {
      "epoch": 87.0909090909091,
      "grad_norm": 0.16843265295028687,
      "learning_rate": 0.00030472583095164874,
      "loss": 2.1809,
      "step": 38320
    },
    {
      "epoch": 87.11363636363636,
      "grad_norm": 0.17270517349243164,
      "learning_rate": 0.00030463786257097094,
      "loss": 2.2003,
      "step": 38330
    },
    {
      "epoch": 87.13636363636364,
      "grad_norm": 0.1665208339691162,
      "learning_rate": 0.00030454988708539737,
      "loss": 2.1869,
      "step": 38340
    },
    {
      "epoch": 87.1590909090909,
      "grad_norm": 0.2027197629213333,
      "learning_rate": 0.00030446190450636814,
      "loss": 2.1806,
      "step": 38350
    },
    {
      "epoch": 87.18181818181819,
      "grad_norm": 0.14082270860671997,
      "learning_rate": 0.00030437391484532404,
      "loss": 2.1887,
      "step": 38360
    },
    {
      "epoch": 87.20454545454545,
      "grad_norm": 0.19906456768512726,
      "learning_rate": 0.0003042859181137069,
      "loss": 2.1902,
      "step": 38370
    },
    {
      "epoch": 87.22727272727273,
      "grad_norm": 0.1647806614637375,
      "learning_rate": 0.0003041979143229596,
      "loss": 2.1885,
      "step": 38380
    },
    {
      "epoch": 87.25,
      "grad_norm": 0.32655444741249084,
      "learning_rate": 0.00030410990348452574,
      "loss": 2.1937,
      "step": 38390
    },
    {
      "epoch": 87.27272727272727,
      "grad_norm": 0.28574901819229126,
      "learning_rate": 0.0003040218856098499,
      "loss": 2.1964,
      "step": 38400
    },
    {
      "epoch": 87.29545454545455,
      "grad_norm": 0.3648378551006317,
      "learning_rate": 0.00030393386071037753,
      "loss": 2.1938,
      "step": 38410
    },
    {
      "epoch": 87.31818181818181,
      "grad_norm": 0.27413904666900635,
      "learning_rate": 0.0003038458287975551,
      "loss": 2.1858,
      "step": 38420
    },
    {
      "epoch": 87.3409090909091,
      "grad_norm": 0.14801302552223206,
      "learning_rate": 0.00030375778988283003,
      "loss": 2.1936,
      "step": 38430
    },
    {
      "epoch": 87.36363636363636,
      "grad_norm": 0.3323312997817993,
      "learning_rate": 0.0003036697439776504,
      "loss": 2.195,
      "step": 38440
    },
    {
      "epoch": 87.38636363636364,
      "grad_norm": 0.21529944241046906,
      "learning_rate": 0.0003035816910934655,
      "loss": 2.1916,
      "step": 38450
    },
    {
      "epoch": 87.4090909090909,
      "grad_norm": 0.2654544711112976,
      "learning_rate": 0.00030349363124172523,
      "loss": 2.1848,
      "step": 38460
    },
    {
      "epoch": 87.43181818181819,
      "grad_norm": 0.23565968871116638,
      "learning_rate": 0.00030340556443388063,
      "loss": 2.1996,
      "step": 38470
    },
    {
      "epoch": 87.45454545454545,
      "grad_norm": 0.2818453907966614,
      "learning_rate": 0.00030331749068138365,
      "loss": 2.1798,
      "step": 38480
    },
    {
      "epoch": 87.47727272727273,
      "grad_norm": 0.21543821692466736,
      "learning_rate": 0.0003032294099956869,
      "loss": 2.2064,
      "step": 38490
    },
    {
      "epoch": 87.5,
      "grad_norm": 0.2056894600391388,
      "learning_rate": 0.0003031413223882442,
      "loss": 2.193,
      "step": 38500
    },
    {
      "epoch": 87.52272727272727,
      "grad_norm": 0.22653096914291382,
      "learning_rate": 0.00030305322787051,
      "loss": 2.2037,
      "step": 38510
    },
    {
      "epoch": 87.54545454545455,
      "grad_norm": 0.20047464966773987,
      "learning_rate": 0.00030296512645393986,
      "loss": 2.1968,
      "step": 38520
    },
    {
      "epoch": 87.56818181818181,
      "grad_norm": 0.1845867931842804,
      "learning_rate": 0.0003028770181499901,
      "loss": 2.1875,
      "step": 38530
    },
    {
      "epoch": 87.5909090909091,
      "grad_norm": 0.21512390673160553,
      "learning_rate": 0.00030278890297011796,
      "loss": 2.195,
      "step": 38540
    },
    {
      "epoch": 87.61363636363636,
      "grad_norm": 0.27371907234191895,
      "learning_rate": 0.00030270078092578165,
      "loss": 2.1799,
      "step": 38550
    },
    {
      "epoch": 87.63636363636364,
      "grad_norm": 0.21121110022068024,
      "learning_rate": 0.0003026126520284402,
      "loss": 2.1953,
      "step": 38560
    },
    {
      "epoch": 87.6590909090909,
      "grad_norm": 0.1864166408777237,
      "learning_rate": 0.0003025245162895535,
      "loss": 2.1867,
      "step": 38570
    },
    {
      "epoch": 87.68181818181819,
      "grad_norm": 0.14328044652938843,
      "learning_rate": 0.0003024363737205825,
      "loss": 2.176,
      "step": 38580
    },
    {
      "epoch": 87.70454545454545,
      "grad_norm": 0.2661616802215576,
      "learning_rate": 0.0003023482243329888,
      "loss": 2.1832,
      "step": 38590
    },
    {
      "epoch": 87.72727272727273,
      "grad_norm": 0.16250842809677124,
      "learning_rate": 0.000302260068138235,
      "loss": 2.1922,
      "step": 38600
    },
    {
      "epoch": 87.75,
      "grad_norm": 0.23812542855739594,
      "learning_rate": 0.0003021719051477847,
      "loss": 2.1928,
      "step": 38610
    },
    {
      "epoch": 87.77272727272727,
      "grad_norm": 0.15278097987174988,
      "learning_rate": 0.0003020837353731022,
      "loss": 2.1828,
      "step": 38620
    },
    {
      "epoch": 87.79545454545455,
      "grad_norm": 0.15751925110816956,
      "learning_rate": 0.0003019955588256528,
      "loss": 2.1912,
      "step": 38630
    },
    {
      "epoch": 87.81818181818181,
      "grad_norm": 0.30923500657081604,
      "learning_rate": 0.00030190737551690253,
      "loss": 2.1955,
      "step": 38640
    },
    {
      "epoch": 87.8409090909091,
      "grad_norm": 0.25982558727264404,
      "learning_rate": 0.0003018191854583185,
      "loss": 2.1823,
      "step": 38650
    },
    {
      "epoch": 87.86363636363636,
      "grad_norm": 0.14432327449321747,
      "learning_rate": 0.0003017309886613686,
      "loss": 2.1922,
      "step": 38660
    },
    {
      "epoch": 87.88636363636364,
      "grad_norm": 0.3994983732700348,
      "learning_rate": 0.00030164278513752154,
      "loss": 2.2006,
      "step": 38670
    },
    {
      "epoch": 87.9090909090909,
      "grad_norm": 0.27327945828437805,
      "learning_rate": 0.00030155457489824706,
      "loss": 2.1957,
      "step": 38680
    },
    {
      "epoch": 87.93181818181819,
      "grad_norm": 0.6318566203117371,
      "learning_rate": 0.0003014663579550156,
      "loss": 2.1889,
      "step": 38690
    },
    {
      "epoch": 87.95454545454545,
      "grad_norm": 0.22022773325443268,
      "learning_rate": 0.00030137813431929856,
      "loss": 2.1969,
      "step": 38700
    },
    {
      "epoch": 87.97727272727273,
      "grad_norm": 0.35056304931640625,
      "learning_rate": 0.0003012899040025682,
      "loss": 2.1983,
      "step": 38710
    },
    {
      "epoch": 88.0,
      "grad_norm": 0.4413563311100006,
      "learning_rate": 0.0003012016670162977,
      "loss": 2.1966,
      "step": 38720
    },
    {
      "epoch": 88.0,
      "eval_loss": 1.0987824201583862,
      "eval_runtime": 8.7195,
      "eval_samples_per_second": 3489.991,
      "eval_steps_per_second": 13.648,
      "step": 38720
    },
    {
      "epoch": 88.02272727272727,
      "grad_norm": 0.2625429928302765,
      "learning_rate": 0.00030111342337196096,
      "loss": 2.1894,
      "step": 38730
    },
    {
      "epoch": 88.04545454545455,
      "grad_norm": 0.3887054920196533,
      "learning_rate": 0.0003010251730810329,
      "loss": 2.1893,
      "step": 38740
    },
    {
      "epoch": 88.06818181818181,
      "grad_norm": 0.5308600068092346,
      "learning_rate": 0.0003009369161549893,
      "loss": 2.1954,
      "step": 38750
    },
    {
      "epoch": 88.0909090909091,
      "grad_norm": 0.15762950479984283,
      "learning_rate": 0.0003008486526053067,
      "loss": 2.1982,
      "step": 38760
    },
    {
      "epoch": 88.11363636363636,
      "grad_norm": 0.20437300205230713,
      "learning_rate": 0.00030076038244346244,
      "loss": 2.1943,
      "step": 38770
    },
    {
      "epoch": 88.13636363636364,
      "grad_norm": 0.19897787272930145,
      "learning_rate": 0.00030067210568093496,
      "loss": 2.1783,
      "step": 38780
    },
    {
      "epoch": 88.1590909090909,
      "grad_norm": 0.5593851208686829,
      "learning_rate": 0.0003005838223292035,
      "loss": 2.1883,
      "step": 38790
    },
    {
      "epoch": 88.18181818181819,
      "grad_norm": 0.8672133684158325,
      "learning_rate": 0.0003004955323997478,
      "loss": 2.1886,
      "step": 38800
    },
    {
      "epoch": 88.20454545454545,
      "grad_norm": 0.1472383439540863,
      "learning_rate": 0.000300407235904049,
      "loss": 2.1872,
      "step": 38810
    },
    {
      "epoch": 88.22727272727273,
      "grad_norm": 0.24091912806034088,
      "learning_rate": 0.0003003189328535888,
      "loss": 2.1836,
      "step": 38820
    },
    {
      "epoch": 88.25,
      "grad_norm": 0.38159215450286865,
      "learning_rate": 0.00030023062325984967,
      "loss": 2.1869,
      "step": 38830
    },
    {
      "epoch": 88.27272727272727,
      "grad_norm": 0.20402364432811737,
      "learning_rate": 0.00030014230713431515,
      "loss": 2.2029,
      "step": 38840
    },
    {
      "epoch": 88.29545454545455,
      "grad_norm": 0.4585728347301483,
      "learning_rate": 0.0003000539844884695,
      "loss": 2.1875,
      "step": 38850
    },
    {
      "epoch": 88.31818181818181,
      "grad_norm": 0.1747809499502182,
      "learning_rate": 0.0002999656553337978,
      "loss": 2.1813,
      "step": 38860
    },
    {
      "epoch": 88.3409090909091,
      "grad_norm": 0.23655885457992554,
      "learning_rate": 0.0002998773196817862,
      "loss": 2.1941,
      "step": 38870
    },
    {
      "epoch": 88.36363636363636,
      "grad_norm": 0.3057190179824829,
      "learning_rate": 0.0002997889775439213,
      "loss": 2.1892,
      "step": 38880
    },
    {
      "epoch": 88.38636363636364,
      "grad_norm": 0.19640447199344635,
      "learning_rate": 0.00029970062893169093,
      "loss": 2.1924,
      "step": 38890
    },
    {
      "epoch": 88.4090909090909,
      "grad_norm": 0.27225056290626526,
      "learning_rate": 0.00029961227385658345,
      "loss": 2.1886,
      "step": 38900
    },
    {
      "epoch": 88.43181818181819,
      "grad_norm": 0.3058684766292572,
      "learning_rate": 0.00029952391233008843,
      "loss": 2.1862,
      "step": 38910
    },
    {
      "epoch": 88.45454545454545,
      "grad_norm": 0.2604376971721649,
      "learning_rate": 0.0002994355443636958,
      "loss": 2.1839,
      "step": 38920
    },
    {
      "epoch": 88.47727272727273,
      "grad_norm": 0.8871006369590759,
      "learning_rate": 0.00029934716996889675,
      "loss": 2.1873,
      "step": 38930
    },
    {
      "epoch": 88.5,
      "grad_norm": 0.42269477248191833,
      "learning_rate": 0.0002992587891571833,
      "loss": 2.1979,
      "step": 38940
    },
    {
      "epoch": 88.52272727272727,
      "grad_norm": 0.2496563047170639,
      "learning_rate": 0.00029917040194004773,
      "loss": 2.1952,
      "step": 38950
    },
    {
      "epoch": 88.54545454545455,
      "grad_norm": 0.2184167057275772,
      "learning_rate": 0.000299082008328984,
      "loss": 2.1827,
      "step": 38960
    },
    {
      "epoch": 88.56818181818181,
      "grad_norm": 0.26111409068107605,
      "learning_rate": 0.00029899360833548605,
      "loss": 2.1869,
      "step": 38970
    },
    {
      "epoch": 88.5909090909091,
      "grad_norm": 0.31017419695854187,
      "learning_rate": 0.0002989052019710495,
      "loss": 2.1915,
      "step": 38980
    },
    {
      "epoch": 88.61363636363636,
      "grad_norm": 0.21231350302696228,
      "learning_rate": 0.0002988167892471701,
      "loss": 2.199,
      "step": 38990
    },
    {
      "epoch": 88.63636363636364,
      "grad_norm": 0.25866496562957764,
      "learning_rate": 0.00029872837017534475,
      "loss": 2.1949,
      "step": 39000
    },
    {
      "epoch": 88.6590909090909,
      "grad_norm": 0.15716269612312317,
      "learning_rate": 0.0002986399447670712,
      "loss": 2.2088,
      "step": 39010
    },
    {
      "epoch": 88.68181818181819,
      "grad_norm": 0.2447838932275772,
      "learning_rate": 0.00029855151303384786,
      "loss": 2.1854,
      "step": 39020
    },
    {
      "epoch": 88.70454545454545,
      "grad_norm": 0.210261732339859,
      "learning_rate": 0.00029846307498717403,
      "loss": 2.1977,
      "step": 39030
    },
    {
      "epoch": 88.72727272727273,
      "grad_norm": 0.18093618750572205,
      "learning_rate": 0.0002983746306385499,
      "loss": 2.1956,
      "step": 39040
    },
    {
      "epoch": 88.75,
      "grad_norm": 0.19713883101940155,
      "learning_rate": 0.00029828617999947646,
      "loss": 2.1864,
      "step": 39050
    },
    {
      "epoch": 88.77272727272727,
      "grad_norm": 0.1560969054698944,
      "learning_rate": 0.00029819772308145554,
      "loss": 2.1966,
      "step": 39060
    },
    {
      "epoch": 88.79545454545455,
      "grad_norm": 0.3261978030204773,
      "learning_rate": 0.00029810925989598956,
      "loss": 2.186,
      "step": 39070
    },
    {
      "epoch": 88.81818181818181,
      "grad_norm": 0.5369260311126709,
      "learning_rate": 0.000298020790454582,
      "loss": 2.1842,
      "step": 39080
    },
    {
      "epoch": 88.8409090909091,
      "grad_norm": 0.18704327940940857,
      "learning_rate": 0.00029793231476873716,
      "loss": 2.1877,
      "step": 39090
    },
    {
      "epoch": 88.86363636363636,
      "grad_norm": 0.20277291536331177,
      "learning_rate": 0.00029784383284995995,
      "loss": 2.1917,
      "step": 39100
    },
    {
      "epoch": 88.88636363636364,
      "grad_norm": 0.14844660460948944,
      "learning_rate": 0.00029775534470975637,
      "loss": 2.1865,
      "step": 39110
    },
    {
      "epoch": 88.9090909090909,
      "grad_norm": 0.2997860610485077,
      "learning_rate": 0.00029766685035963295,
      "loss": 2.1987,
      "step": 39120
    },
    {
      "epoch": 88.93181818181819,
      "grad_norm": 0.2647968828678131,
      "learning_rate": 0.0002975783498110972,
      "loss": 2.2,
      "step": 39130
    },
    {
      "epoch": 88.95454545454545,
      "grad_norm": 0.24513617157936096,
      "learning_rate": 0.00029748984307565737,
      "loss": 2.1915,
      "step": 39140
    },
    {
      "epoch": 88.97727272727273,
      "grad_norm": 0.2388075888156891,
      "learning_rate": 0.0002974013301648225,
      "loss": 2.1869,
      "step": 39150
    },
    {
      "epoch": 89.0,
      "grad_norm": 2.037099838256836,
      "learning_rate": 0.00029731281109010257,
      "loss": 2.185,
      "step": 39160
    },
    {
      "epoch": 89.0,
      "eval_loss": 1.099024772644043,
      "eval_runtime": 8.6874,
      "eval_samples_per_second": 3502.902,
      "eval_steps_per_second": 13.698,
      "step": 39160
    },
    {
      "epoch": 89.02272727272727,
      "grad_norm": 0.18179944157600403,
      "learning_rate": 0.0002972242858630082,
      "loss": 2.1843,
      "step": 39170
    },
    {
      "epoch": 89.04545454545455,
      "grad_norm": 0.3308013081550598,
      "learning_rate": 0.0002971357544950508,
      "loss": 2.1968,
      "step": 39180
    },
    {
      "epoch": 89.06818181818181,
      "grad_norm": 0.28993919491767883,
      "learning_rate": 0.0002970472169977428,
      "loss": 2.1728,
      "step": 39190
    },
    {
      "epoch": 89.0909090909091,
      "grad_norm": 0.20615486800670624,
      "learning_rate": 0.0002969586733825971,
      "loss": 2.1863,
      "step": 39200
    },
    {
      "epoch": 89.11363636363636,
      "grad_norm": 0.4509391486644745,
      "learning_rate": 0.00029687012366112764,
      "loss": 2.1796,
      "step": 39210
    },
    {
      "epoch": 89.13636363636364,
      "grad_norm": 0.35690781474113464,
      "learning_rate": 0.0002967815678448492,
      "loss": 2.1851,
      "step": 39220
    },
    {
      "epoch": 89.1590909090909,
      "grad_norm": 0.7239984273910522,
      "learning_rate": 0.00029669300594527693,
      "loss": 2.1854,
      "step": 39230
    },
    {
      "epoch": 89.18181818181819,
      "grad_norm": 0.18642689287662506,
      "learning_rate": 0.0002966044379739274,
      "loss": 2.186,
      "step": 39240
    },
    {
      "epoch": 89.20454545454545,
      "grad_norm": 0.33193227648735046,
      "learning_rate": 0.00029651586394231755,
      "loss": 2.2004,
      "step": 39250
    },
    {
      "epoch": 89.22727272727273,
      "grad_norm": 0.33141791820526123,
      "learning_rate": 0.00029642728386196517,
      "loss": 2.1834,
      "step": 39260
    },
    {
      "epoch": 89.25,
      "grad_norm": 0.2148447334766388,
      "learning_rate": 0.00029633869774438885,
      "loss": 2.1953,
      "step": 39270
    },
    {
      "epoch": 89.27272727272727,
      "grad_norm": 0.18928338587284088,
      "learning_rate": 0.0002962501056011079,
      "loss": 2.1959,
      "step": 39280
    },
    {
      "epoch": 89.29545454545455,
      "grad_norm": 0.19729267060756683,
      "learning_rate": 0.0002961615074436427,
      "loss": 2.1828,
      "step": 39290
    },
    {
      "epoch": 89.31818181818181,
      "grad_norm": 0.1527950018644333,
      "learning_rate": 0.0002960729032835141,
      "loss": 2.1897,
      "step": 39300
    },
    {
      "epoch": 89.3409090909091,
      "grad_norm": 5.030553340911865,
      "learning_rate": 0.0002959842931322439,
      "loss": 2.2001,
      "step": 39310
    },
    {
      "epoch": 89.36363636363636,
      "grad_norm": 0.19267046451568604,
      "learning_rate": 0.00029589567700135457,
      "loss": 2.1843,
      "step": 39320
    },
    {
      "epoch": 89.38636363636364,
      "grad_norm": 0.2007024586200714,
      "learning_rate": 0.0002958070549023693,
      "loss": 2.189,
      "step": 39330
    },
    {
      "epoch": 89.4090909090909,
      "grad_norm": 0.3053048551082611,
      "learning_rate": 0.0002957184268468124,
      "loss": 2.1849,
      "step": 39340
    },
    {
      "epoch": 89.43181818181819,
      "grad_norm": 0.2605350613594055,
      "learning_rate": 0.0002956297928462085,
      "loss": 2.1906,
      "step": 39350
    },
    {
      "epoch": 89.45454545454545,
      "grad_norm": 0.21768060326576233,
      "learning_rate": 0.0002955411529120834,
      "loss": 2.1908,
      "step": 39360
    },
    {
      "epoch": 89.47727272727273,
      "grad_norm": 0.3169664442539215,
      "learning_rate": 0.00029545250705596344,
      "loss": 2.1948,
      "step": 39370
    },
    {
      "epoch": 89.5,
      "grad_norm": 0.2379901260137558,
      "learning_rate": 0.00029536385528937566,
      "loss": 2.1859,
      "step": 39380
    },
    {
      "epoch": 89.52272727272727,
      "grad_norm": 0.2441280633211136,
      "learning_rate": 0.0002952751976238481,
      "loss": 2.19,
      "step": 39390
    },
    {
      "epoch": 89.54545454545455,
      "grad_norm": 2.284285545349121,
      "learning_rate": 0.00029518653407090953,
      "loss": 2.1921,
      "step": 39400
    },
    {
      "epoch": 89.56818181818181,
      "grad_norm": 0.2996518909931183,
      "learning_rate": 0.0002950978646420893,
      "loss": 2.1973,
      "step": 39410
    },
    {
      "epoch": 89.5909090909091,
      "grad_norm": 0.1914905160665512,
      "learning_rate": 0.0002950091893489176,
      "loss": 2.1897,
      "step": 39420
    },
    {
      "epoch": 89.61363636363636,
      "grad_norm": 0.5139682292938232,
      "learning_rate": 0.0002949205082029256,
      "loss": 2.1877,
      "step": 39430
    },
    {
      "epoch": 89.63636363636364,
      "grad_norm": 0.15289823710918427,
      "learning_rate": 0.00029483182121564487,
      "loss": 2.1919,
      "step": 39440
    },
    {
      "epoch": 89.6590909090909,
      "grad_norm": 0.36036187410354614,
      "learning_rate": 0.00029474312839860797,
      "loss": 2.1842,
      "step": 39450
    },
    {
      "epoch": 89.68181818181819,
      "grad_norm": 0.28643545508384705,
      "learning_rate": 0.0002946544297633482,
      "loss": 2.1896,
      "step": 39460
    },
    {
      "epoch": 89.70454545454545,
      "grad_norm": 0.21198661625385284,
      "learning_rate": 0.00029456572532139967,
      "loss": 2.1976,
      "step": 39470
    },
    {
      "epoch": 89.72727272727273,
      "grad_norm": 0.18772512674331665,
      "learning_rate": 0.00029447701508429695,
      "loss": 2.184,
      "step": 39480
    },
    {
      "epoch": 89.75,
      "grad_norm": 0.2963707447052002,
      "learning_rate": 0.0002943882990635759,
      "loss": 2.1951,
      "step": 39490
    },
    {
      "epoch": 89.77272727272727,
      "grad_norm": 0.32898563146591187,
      "learning_rate": 0.00029429957727077245,
      "loss": 2.1871,
      "step": 39500
    },
    {
      "epoch": 89.79545454545455,
      "grad_norm": 0.22118841111660004,
      "learning_rate": 0.0002942108497174238,
      "loss": 2.1962,
      "step": 39510
    },
    {
      "epoch": 89.81818181818181,
      "grad_norm": 0.15081873536109924,
      "learning_rate": 0.00029412211641506776,
      "loss": 2.1897,
      "step": 39520
    },
    {
      "epoch": 89.8409090909091,
      "grad_norm": 1.2805851697921753,
      "learning_rate": 0.0002940333773752428,
      "loss": 2.1964,
      "step": 39530
    },
    {
      "epoch": 89.86363636363636,
      "grad_norm": 0.4100637435913086,
      "learning_rate": 0.0002939446326094882,
      "loss": 2.1871,
      "step": 39540
    },
    {
      "epoch": 89.88636363636364,
      "grad_norm": 0.16448833048343658,
      "learning_rate": 0.0002938558821293441,
      "loss": 2.1882,
      "step": 39550
    },
    {
      "epoch": 89.9090909090909,
      "grad_norm": 0.16601760685443878,
      "learning_rate": 0.0002937671259463512,
      "loss": 2.1913,
      "step": 39560
    },
    {
      "epoch": 89.93181818181819,
      "grad_norm": 0.3229120075702667,
      "learning_rate": 0.00029367836407205095,
      "loss": 2.2024,
      "step": 39570
    },
    {
      "epoch": 89.95454545454545,
      "grad_norm": 0.23280180990695953,
      "learning_rate": 0.00029358959651798564,
      "loss": 2.1922,
      "step": 39580
    },
    {
      "epoch": 89.97727272727273,
      "grad_norm": 0.19919045269489288,
      "learning_rate": 0.00029350082329569825,
      "loss": 2.1984,
      "step": 39590
    },
    {
      "epoch": 90.0,
      "grad_norm": 0.20740637183189392,
      "learning_rate": 0.00029341204441673266,
      "loss": 2.1811,
      "step": 39600
    },
    {
      "epoch": 90.0,
      "eval_loss": 1.0989117622375488,
      "eval_runtime": 8.767,
      "eval_samples_per_second": 3471.093,
      "eval_steps_per_second": 13.574,
      "step": 39600
    },
    {
      "epoch": 90.02272727272727,
      "grad_norm": 0.7964478135108948,
      "learning_rate": 0.0002933232598926331,
      "loss": 2.1831,
      "step": 39610
    },
    {
      "epoch": 90.04545454545455,
      "grad_norm": 0.31290629506111145,
      "learning_rate": 0.0002932344697349448,
      "loss": 2.2011,
      "step": 39620
    },
    {
      "epoch": 90.06818181818181,
      "grad_norm": 0.24238181114196777,
      "learning_rate": 0.0002931456739552138,
      "loss": 2.1823,
      "step": 39630
    },
    {
      "epoch": 90.0909090909091,
      "grad_norm": 0.14030131697654724,
      "learning_rate": 0.0002930568725649867,
      "loss": 2.197,
      "step": 39640
    },
    {
      "epoch": 90.11363636363636,
      "grad_norm": 0.3211600184440613,
      "learning_rate": 0.0002929680655758109,
      "loss": 2.1939,
      "step": 39650
    },
    {
      "epoch": 90.13636363636364,
      "grad_norm": 0.2295127511024475,
      "learning_rate": 0.0002928792529992345,
      "loss": 2.1905,
      "step": 39660
    },
    {
      "epoch": 90.1590909090909,
      "grad_norm": 0.13831520080566406,
      "learning_rate": 0.0002927904348468063,
      "loss": 2.1938,
      "step": 39670
    },
    {
      "epoch": 90.18181818181819,
      "grad_norm": 0.44816577434539795,
      "learning_rate": 0.00029270161113007604,
      "loss": 2.1879,
      "step": 39680
    },
    {
      "epoch": 90.20454545454545,
      "grad_norm": 0.19650432467460632,
      "learning_rate": 0.00029261278186059383,
      "loss": 2.1841,
      "step": 39690
    },
    {
      "epoch": 90.22727272727273,
      "grad_norm": 0.4835743308067322,
      "learning_rate": 0.00029252394704991076,
      "loss": 2.1887,
      "step": 39700
    },
    {
      "epoch": 90.25,
      "grad_norm": 0.2644595503807068,
      "learning_rate": 0.0002924351067095785,
      "loss": 2.1784,
      "step": 39710
    },
    {
      "epoch": 90.27272727272727,
      "grad_norm": 0.5169901847839355,
      "learning_rate": 0.00029234626085114957,
      "loss": 2.1906,
      "step": 39720
    },
    {
      "epoch": 90.29545454545455,
      "grad_norm": 0.22041547298431396,
      "learning_rate": 0.0002922574094861772,
      "loss": 2.1979,
      "step": 39730
    },
    {
      "epoch": 90.31818181818181,
      "grad_norm": 0.1520996391773224,
      "learning_rate": 0.0002921685526262151,
      "loss": 2.1886,
      "step": 39740
    },
    {
      "epoch": 90.3409090909091,
      "grad_norm": 0.1828918159008026,
      "learning_rate": 0.00029207969028281804,
      "loss": 2.1883,
      "step": 39750
    },
    {
      "epoch": 90.36363636363636,
      "grad_norm": 0.1816302090883255,
      "learning_rate": 0.00029199082246754123,
      "loss": 2.196,
      "step": 39760
    },
    {
      "epoch": 90.38636363636364,
      "grad_norm": 0.16438478231430054,
      "learning_rate": 0.00029190194919194077,
      "loss": 2.1889,
      "step": 39770
    },
    {
      "epoch": 90.4090909090909,
      "grad_norm": 0.2044946402311325,
      "learning_rate": 0.00029181307046757327,
      "loss": 2.1919,
      "step": 39780
    },
    {
      "epoch": 90.43181818181819,
      "grad_norm": 0.20398014783859253,
      "learning_rate": 0.00029172418630599633,
      "loss": 2.1943,
      "step": 39790
    },
    {
      "epoch": 90.45454545454545,
      "grad_norm": 0.14957457780838013,
      "learning_rate": 0.0002916352967187681,
      "loss": 2.19,
      "step": 39800
    },
    {
      "epoch": 90.47727272727273,
      "grad_norm": 0.33071205019950867,
      "learning_rate": 0.0002915464017174473,
      "loss": 2.1804,
      "step": 39810
    },
    {
      "epoch": 90.5,
      "grad_norm": 0.1709715873003006,
      "learning_rate": 0.00029145750131359357,
      "loss": 2.1921,
      "step": 39820
    },
    {
      "epoch": 90.52272727272727,
      "grad_norm": 0.4990697503089905,
      "learning_rate": 0.00029136859551876713,
      "loss": 2.1886,
      "step": 39830
    },
    {
      "epoch": 90.54545454545455,
      "grad_norm": 0.24566751718521118,
      "learning_rate": 0.000291279684344529,
      "loss": 2.198,
      "step": 39840
    },
    {
      "epoch": 90.56818181818181,
      "grad_norm": 0.45109978318214417,
      "learning_rate": 0.0002911907678024409,
      "loss": 2.1876,
      "step": 39850
    },
    {
      "epoch": 90.5909090909091,
      "grad_norm": 0.2597430944442749,
      "learning_rate": 0.0002911018459040651,
      "loss": 2.1935,
      "step": 39860
    },
    {
      "epoch": 90.61363636363636,
      "grad_norm": 0.1886993795633316,
      "learning_rate": 0.0002910129186609646,
      "loss": 2.1972,
      "step": 39870
    },
    {
      "epoch": 90.63636363636364,
      "grad_norm": 0.21449322998523712,
      "learning_rate": 0.00029092398608470326,
      "loss": 2.1872,
      "step": 39880
    },
    {
      "epoch": 90.6590909090909,
      "grad_norm": 0.17035382986068726,
      "learning_rate": 0.00029083504818684555,
      "loss": 2.1845,
      "step": 39890
    },
    {
      "epoch": 90.68181818181819,
      "grad_norm": 0.22367741167545319,
      "learning_rate": 0.0002907461049789565,
      "loss": 2.1819,
      "step": 39900
    },
    {
      "epoch": 90.70454545454545,
      "grad_norm": 1.2432584762573242,
      "learning_rate": 0.000290657156472602,
      "loss": 2.1927,
      "step": 39910
    },
    {
      "epoch": 90.72727272727273,
      "grad_norm": 0.21516689658164978,
      "learning_rate": 0.0002905682026793486,
      "loss": 2.1984,
      "step": 39920
    },
    {
      "epoch": 90.75,
      "grad_norm": 0.31375887989997864,
      "learning_rate": 0.00029047924361076346,
      "loss": 2.1813,
      "step": 39930
    },
    {
      "epoch": 90.77272727272727,
      "grad_norm": 0.19221314787864685,
      "learning_rate": 0.0002903902792784145,
      "loss": 2.1992,
      "step": 39940
    },
    {
      "epoch": 90.79545454545455,
      "grad_norm": 0.3837853670120239,
      "learning_rate": 0.00029030130969387024,
      "loss": 2.1969,
      "step": 39950
    },
    {
      "epoch": 90.81818181818181,
      "grad_norm": 0.21859435737133026,
      "learning_rate": 0.00029021233486869997,
      "loss": 2.1917,
      "step": 39960
    },
    {
      "epoch": 90.8409090909091,
      "grad_norm": 0.34886372089385986,
      "learning_rate": 0.00029012335481447374,
      "loss": 2.1892,
      "step": 39970
    },
    {
      "epoch": 90.86363636363636,
      "grad_norm": 0.37969306111335754,
      "learning_rate": 0.00029003436954276195,
      "loss": 2.1878,
      "step": 39980
    },
    {
      "epoch": 90.88636363636364,
      "grad_norm": 0.199601948261261,
      "learning_rate": 0.0002899453790651361,
      "loss": 2.1818,
      "step": 39990
    },
    {
      "epoch": 90.9090909090909,
      "grad_norm": 0.4242936074733734,
      "learning_rate": 0.00028985638339316814,
      "loss": 2.2009,
      "step": 40000
    },
    {
      "epoch": 90.93181818181819,
      "grad_norm": 0.6679455637931824,
      "learning_rate": 0.0002897673825384305,
      "loss": 2.1857,
      "step": 40010
    },
    {
      "epoch": 90.95454545454545,
      "grad_norm": 0.4317348301410675,
      "learning_rate": 0.00028967837651249677,
      "loss": 2.192,
      "step": 40020
    },
    {
      "epoch": 90.97727272727273,
      "grad_norm": 0.17908358573913574,
      "learning_rate": 0.00028958936532694093,
      "loss": 2.1924,
      "step": 40030
    },
    {
      "epoch": 91.0,
      "grad_norm": 0.4024072289466858,
      "learning_rate": 0.0002895003489933375,
      "loss": 2.1853,
      "step": 40040
    },
    {
      "epoch": 91.0,
      "eval_loss": 1.0990430116653442,
      "eval_runtime": 8.7122,
      "eval_samples_per_second": 3492.905,
      "eval_steps_per_second": 13.659,
      "step": 40040
    },
    {
      "epoch": 91.02272727272727,
      "grad_norm": 0.2839105725288391,
      "learning_rate": 0.00028941132752326193,
      "loss": 2.1889,
      "step": 40050
    },
    {
      "epoch": 91.04545454545455,
      "grad_norm": 0.1808542162179947,
      "learning_rate": 0.00028932230092829016,
      "loss": 2.1773,
      "step": 40060
    },
    {
      "epoch": 91.06818181818181,
      "grad_norm": 0.16743440926074982,
      "learning_rate": 0.00028923326921999885,
      "loss": 2.1878,
      "step": 40070
    },
    {
      "epoch": 91.0909090909091,
      "grad_norm": 0.45531386137008667,
      "learning_rate": 0.0002891442324099655,
      "loss": 2.1842,
      "step": 40080
    },
    {
      "epoch": 91.11363636363636,
      "grad_norm": 0.18522462248802185,
      "learning_rate": 0.0002890551905097679,
      "loss": 2.1856,
      "step": 40090
    },
    {
      "epoch": 91.13636363636364,
      "grad_norm": 0.23778624832630157,
      "learning_rate": 0.0002889661435309848,
      "loss": 2.1879,
      "step": 40100
    },
    {
      "epoch": 91.1590909090909,
      "grad_norm": 0.34559527039527893,
      "learning_rate": 0.0002888770914851956,
      "loss": 2.1783,
      "step": 40110
    },
    {
      "epoch": 91.18181818181819,
      "grad_norm": 0.2861935496330261,
      "learning_rate": 0.00028878803438398015,
      "loss": 2.1898,
      "step": 40120
    },
    {
      "epoch": 91.20454545454545,
      "grad_norm": 0.2666845917701721,
      "learning_rate": 0.00028869897223891925,
      "loss": 2.1923,
      "step": 40130
    },
    {
      "epoch": 91.22727272727273,
      "grad_norm": 0.4178837537765503,
      "learning_rate": 0.000288609905061594,
      "loss": 2.1933,
      "step": 40140
    },
    {
      "epoch": 91.25,
      "grad_norm": 0.37324631214141846,
      "learning_rate": 0.00028852083286358643,
      "loss": 2.1852,
      "step": 40150
    },
    {
      "epoch": 91.27272727272727,
      "grad_norm": 0.2735806703567505,
      "learning_rate": 0.00028843175565647925,
      "loss": 2.194,
      "step": 40160
    },
    {
      "epoch": 91.29545454545455,
      "grad_norm": 0.3342908024787903,
      "learning_rate": 0.00028834267345185556,
      "loss": 2.1905,
      "step": 40170
    },
    {
      "epoch": 91.31818181818181,
      "grad_norm": 0.2575356960296631,
      "learning_rate": 0.00028825358626129936,
      "loss": 2.1896,
      "step": 40180
    },
    {
      "epoch": 91.3409090909091,
      "grad_norm": 0.2166849970817566,
      "learning_rate": 0.0002881644940963952,
      "loss": 2.1934,
      "step": 40190
    },
    {
      "epoch": 91.36363636363636,
      "grad_norm": 0.1767917275428772,
      "learning_rate": 0.0002880753969687282,
      "loss": 2.1847,
      "step": 40200
    },
    {
      "epoch": 91.38636363636364,
      "grad_norm": 0.18645159900188446,
      "learning_rate": 0.0002879862948898842,
      "loss": 2.1859,
      "step": 40210
    },
    {
      "epoch": 91.4090909090909,
      "grad_norm": 0.22358037531375885,
      "learning_rate": 0.00028789718787144985,
      "loss": 2.1924,
      "step": 40220
    },
    {
      "epoch": 91.43181818181819,
      "grad_norm": 0.408046156167984,
      "learning_rate": 0.0002878080759250122,
      "loss": 2.1841,
      "step": 40230
    },
    {
      "epoch": 91.45454545454545,
      "grad_norm": 0.3977797031402588,
      "learning_rate": 0.00028771895906215897,
      "loss": 2.1932,
      "step": 40240
    },
    {
      "epoch": 91.47727272727273,
      "grad_norm": 0.32012009620666504,
      "learning_rate": 0.00028762983729447856,
      "loss": 2.1915,
      "step": 40250
    },
    {
      "epoch": 91.5,
      "grad_norm": 0.2500368356704712,
      "learning_rate": 0.00028754071063356006,
      "loss": 2.194,
      "step": 40260
    },
    {
      "epoch": 91.52272727272727,
      "grad_norm": 0.8795655965805054,
      "learning_rate": 0.00028745157909099324,
      "loss": 2.1885,
      "step": 40270
    },
    {
      "epoch": 91.54545454545455,
      "grad_norm": 0.3108106255531311,
      "learning_rate": 0.00028736244267836824,
      "loss": 2.1906,
      "step": 40280
    },
    {
      "epoch": 91.56818181818181,
      "grad_norm": 0.17904189229011536,
      "learning_rate": 0.0002872733014072762,
      "loss": 2.1791,
      "step": 40290
    },
    {
      "epoch": 91.5909090909091,
      "grad_norm": 1.1175413131713867,
      "learning_rate": 0.0002871841552893086,
      "loss": 2.1853,
      "step": 40300
    },
    {
      "epoch": 91.61363636363636,
      "grad_norm": 0.20424291491508484,
      "learning_rate": 0.00028709500433605757,
      "loss": 2.1915,
      "step": 40310
    },
    {
      "epoch": 91.63636363636364,
      "grad_norm": 0.23571698367595673,
      "learning_rate": 0.0002870058485591161,
      "loss": 2.1954,
      "step": 40320
    },
    {
      "epoch": 91.6590909090909,
      "grad_norm": 0.2320100963115692,
      "learning_rate": 0.0002869166879700776,
      "loss": 2.1847,
      "step": 40330
    },
    {
      "epoch": 91.68181818181819,
      "grad_norm": 0.5598874092102051,
      "learning_rate": 0.0002868275225805362,
      "loss": 2.1961,
      "step": 40340
    },
    {
      "epoch": 91.70454545454545,
      "grad_norm": 0.17755554616451263,
      "learning_rate": 0.00028673835240208666,
      "loss": 2.1765,
      "step": 40350
    },
    {
      "epoch": 91.72727272727273,
      "grad_norm": 0.6728946566581726,
      "learning_rate": 0.0002866491774463242,
      "loss": 2.2088,
      "step": 40360
    },
    {
      "epoch": 91.75,
      "grad_norm": 0.29758915305137634,
      "learning_rate": 0.00028655999772484494,
      "loss": 2.1925,
      "step": 40370
    },
    {
      "epoch": 91.77272727272727,
      "grad_norm": 0.28208813071250916,
      "learning_rate": 0.00028647081324924533,
      "loss": 2.1916,
      "step": 40380
    },
    {
      "epoch": 91.79545454545455,
      "grad_norm": 1.7173633575439453,
      "learning_rate": 0.00028638162403112256,
      "loss": 2.1959,
      "step": 40390
    },
    {
      "epoch": 91.81818181818181,
      "grad_norm": 0.561173677444458,
      "learning_rate": 0.00028629243008207456,
      "loss": 2.1904,
      "step": 40400
    },
    {
      "epoch": 91.8409090909091,
      "grad_norm": 0.17162315547466278,
      "learning_rate": 0.00028620323141369974,
      "loss": 2.1861,
      "step": 40410
    },
    {
      "epoch": 91.86363636363636,
      "grad_norm": 0.3117404580116272,
      "learning_rate": 0.0002861140280375971,
      "loss": 2.1924,
      "step": 40420
    },
    {
      "epoch": 91.88636363636364,
      "grad_norm": 0.21629364788532257,
      "learning_rate": 0.0002860248199653664,
      "loss": 2.1928,
      "step": 40430
    },
    {
      "epoch": 91.9090909090909,
      "grad_norm": 0.1574898511171341,
      "learning_rate": 0.0002859356072086077,
      "loss": 2.1868,
      "step": 40440
    },
    {
      "epoch": 91.93181818181819,
      "grad_norm": 0.23483619093894958,
      "learning_rate": 0.00028584638977892206,
      "loss": 2.1929,
      "step": 40450
    },
    {
      "epoch": 91.95454545454545,
      "grad_norm": 0.22207634150981903,
      "learning_rate": 0.000285757167687911,
      "loss": 2.1876,
      "step": 40460
    },
    {
      "epoch": 91.97727272727273,
      "grad_norm": 0.23920080065727234,
      "learning_rate": 0.00028566794094717654,
      "loss": 2.1967,
      "step": 40470
    },
    {
      "epoch": 92.0,
      "grad_norm": 0.35828927159309387,
      "learning_rate": 0.00028557870956832135,
      "loss": 2.1913,
      "step": 40480
    },
    {
      "epoch": 92.0,
      "eval_loss": 1.099138617515564,
      "eval_runtime": 8.693,
      "eval_samples_per_second": 3500.613,
      "eval_steps_per_second": 13.689,
      "step": 40480
    },
    {
      "epoch": 92.02272727272727,
      "grad_norm": 0.21502134203910828,
      "learning_rate": 0.00028548947356294867,
      "loss": 2.1792,
      "step": 40490
    },
    {
      "epoch": 92.04545454545455,
      "grad_norm": 0.21336039900779724,
      "learning_rate": 0.00028540023294266266,
      "loss": 2.1905,
      "step": 40500
    },
    {
      "epoch": 92.06818181818181,
      "grad_norm": 0.1793747991323471,
      "learning_rate": 0.00028531098771906755,
      "loss": 2.1942,
      "step": 40510
    },
    {
      "epoch": 92.0909090909091,
      "grad_norm": 0.31441256403923035,
      "learning_rate": 0.0002852217379037686,
      "loss": 2.1938,
      "step": 40520
    },
    {
      "epoch": 92.11363636363636,
      "grad_norm": 0.28886231780052185,
      "learning_rate": 0.0002851324835083715,
      "loss": 2.1884,
      "step": 40530
    },
    {
      "epoch": 92.13636363636364,
      "grad_norm": 0.26176518201828003,
      "learning_rate": 0.0002850432245444824,
      "loss": 2.1892,
      "step": 40540
    },
    {
      "epoch": 92.1590909090909,
      "grad_norm": 0.4154289662837982,
      "learning_rate": 0.00028495396102370843,
      "loss": 2.18,
      "step": 40550
    },
    {
      "epoch": 92.18181818181819,
      "grad_norm": 0.12255947291851044,
      "learning_rate": 0.0002848646929576569,
      "loss": 2.1758,
      "step": 40560
    },
    {
      "epoch": 92.20454545454545,
      "grad_norm": 0.2825928330421448,
      "learning_rate": 0.0002847754203579358,
      "loss": 2.1835,
      "step": 40570
    },
    {
      "epoch": 92.22727272727273,
      "grad_norm": 0.17945386469364166,
      "learning_rate": 0.00028468614323615404,
      "loss": 2.1846,
      "step": 40580
    },
    {
      "epoch": 92.25,
      "grad_norm": 0.3765428364276886,
      "learning_rate": 0.0002845968616039207,
      "loss": 2.1929,
      "step": 40590
    },
    {
      "epoch": 92.27272727272727,
      "grad_norm": 0.16336537897586823,
      "learning_rate": 0.0002845075754728458,
      "loss": 2.191,
      "step": 40600
    },
    {
      "epoch": 92.29545454545455,
      "grad_norm": 0.1811947524547577,
      "learning_rate": 0.00028441828485453947,
      "loss": 2.1887,
      "step": 40610
    },
    {
      "epoch": 92.31818181818181,
      "grad_norm": 0.2087341547012329,
      "learning_rate": 0.0002843289897606129,
      "loss": 2.1912,
      "step": 40620
    },
    {
      "epoch": 92.3409090909091,
      "grad_norm": 0.23622524738311768,
      "learning_rate": 0.0002842396902026777,
      "loss": 2.1849,
      "step": 40630
    },
    {
      "epoch": 92.36363636363636,
      "grad_norm": 0.4944581985473633,
      "learning_rate": 0.0002841503861923459,
      "loss": 2.1876,
      "step": 40640
    },
    {
      "epoch": 92.38636363636364,
      "grad_norm": 0.2595110237598419,
      "learning_rate": 0.0002840610777412304,
      "loss": 2.1847,
      "step": 40650
    },
    {
      "epoch": 92.4090909090909,
      "grad_norm": 0.3076513409614563,
      "learning_rate": 0.0002839717648609445,
      "loss": 2.1881,
      "step": 40660
    },
    {
      "epoch": 92.43181818181819,
      "grad_norm": 0.23191441595554352,
      "learning_rate": 0.00028388244756310203,
      "loss": 2.1984,
      "step": 40670
    },
    {
      "epoch": 92.45454545454545,
      "grad_norm": 0.39232102036476135,
      "learning_rate": 0.0002837931258593175,
      "loss": 2.1909,
      "step": 40680
    },
    {
      "epoch": 92.47727272727273,
      "grad_norm": 0.24350042641162872,
      "learning_rate": 0.00028370379976120594,
      "loss": 2.1884,
      "step": 40690
    },
    {
      "epoch": 92.5,
      "grad_norm": 0.345826119184494,
      "learning_rate": 0.00028361446928038296,
      "loss": 2.1939,
      "step": 40700
    },
    {
      "epoch": 92.52272727272727,
      "grad_norm": 0.1742434799671173,
      "learning_rate": 0.00028352513442846485,
      "loss": 2.1849,
      "step": 40710
    },
    {
      "epoch": 92.54545454545455,
      "grad_norm": 0.20424914360046387,
      "learning_rate": 0.00028343579521706826,
      "loss": 2.1961,
      "step": 40720
    },
    {
      "epoch": 92.56818181818181,
      "grad_norm": 0.3856958746910095,
      "learning_rate": 0.00028334645165781046,
      "loss": 2.189,
      "step": 40730
    },
    {
      "epoch": 92.5909090909091,
      "grad_norm": 0.9294229745864868,
      "learning_rate": 0.0002832571037623095,
      "loss": 2.1967,
      "step": 40740
    },
    {
      "epoch": 92.61363636363636,
      "grad_norm": 0.18590538203716278,
      "learning_rate": 0.00028316775154218376,
      "loss": 2.1838,
      "step": 40750
    },
    {
      "epoch": 92.63636363636364,
      "grad_norm": 0.7738738059997559,
      "learning_rate": 0.00028307839500905224,
      "loss": 2.1875,
      "step": 40760
    },
    {
      "epoch": 92.6590909090909,
      "grad_norm": 0.36731788516044617,
      "learning_rate": 0.0002829890341745345,
      "loss": 2.1824,
      "step": 40770
    },
    {
      "epoch": 92.68181818181819,
      "grad_norm": 0.2482551783323288,
      "learning_rate": 0.0002828996690502508,
      "loss": 2.1799,
      "step": 40780
    },
    {
      "epoch": 92.70454545454545,
      "grad_norm": 0.36127427220344543,
      "learning_rate": 0.0002828102996478217,
      "loss": 2.1789,
      "step": 40790
    },
    {
      "epoch": 92.72727272727273,
      "grad_norm": 0.17940449714660645,
      "learning_rate": 0.00028272092597886843,
      "loss": 2.1875,
      "step": 40800
    },
    {
      "epoch": 92.75,
      "grad_norm": 0.921026885509491,
      "learning_rate": 0.000282631548055013,
      "loss": 2.1955,
      "step": 40810
    },
    {
      "epoch": 92.77272727272727,
      "grad_norm": 0.19589117169380188,
      "learning_rate": 0.0002825421658878774,
      "loss": 2.1944,
      "step": 40820
    },
    {
      "epoch": 92.79545454545455,
      "grad_norm": 0.36574921011924744,
      "learning_rate": 0.0002824527794890849,
      "loss": 2.1899,
      "step": 40830
    },
    {
      "epoch": 92.81818181818181,
      "grad_norm": 0.6832772493362427,
      "learning_rate": 0.00028236338887025885,
      "loss": 2.1926,
      "step": 40840
    },
    {
      "epoch": 92.8409090909091,
      "grad_norm": 0.4153144061565399,
      "learning_rate": 0.00028227399404302323,
      "loss": 2.1886,
      "step": 40850
    },
    {
      "epoch": 92.86363636363636,
      "grad_norm": 0.29227229952812195,
      "learning_rate": 0.0002821845950190026,
      "loss": 2.1905,
      "step": 40860
    },
    {
      "epoch": 92.88636363636364,
      "grad_norm": 0.24606040120124817,
      "learning_rate": 0.00028209519180982204,
      "loss": 2.1937,
      "step": 40870
    },
    {
      "epoch": 92.9090909090909,
      "grad_norm": 0.5036948323249817,
      "learning_rate": 0.00028200578442710725,
      "loss": 2.1907,
      "step": 40880
    },
    {
      "epoch": 92.93181818181819,
      "grad_norm": 0.3662905693054199,
      "learning_rate": 0.0002819163728824844,
      "loss": 2.1988,
      "step": 40890
    },
    {
      "epoch": 92.95454545454545,
      "grad_norm": 0.2350214272737503,
      "learning_rate": 0.0002818269571875801,
      "loss": 2.1834,
      "step": 40900
    },
    {
      "epoch": 92.97727272727273,
      "grad_norm": 0.181977316737175,
      "learning_rate": 0.0002817375373540219,
      "loss": 2.1917,
      "step": 40910
    },
    {
      "epoch": 93.0,
      "grad_norm": 0.257193922996521,
      "learning_rate": 0.00028164811339343734,
      "loss": 2.1929,
      "step": 40920
    },
    {
      "epoch": 93.0,
      "eval_loss": 1.099226713180542,
      "eval_runtime": 8.7122,
      "eval_samples_per_second": 3492.926,
      "eval_steps_per_second": 13.659,
      "step": 40920
    },
    {
      "epoch": 93.02272727272727,
      "grad_norm": 0.29374462366104126,
      "learning_rate": 0.00028155868531745486,
      "loss": 2.1844,
      "step": 40930
    },
    {
      "epoch": 93.04545454545455,
      "grad_norm": 0.8637718558311462,
      "learning_rate": 0.00028146925313770346,
      "loss": 2.1932,
      "step": 40940
    },
    {
      "epoch": 93.06818181818181,
      "grad_norm": 0.26683560013771057,
      "learning_rate": 0.00028137981686581225,
      "loss": 2.1677,
      "step": 40950
    },
    {
      "epoch": 93.0909090909091,
      "grad_norm": 0.33075952529907227,
      "learning_rate": 0.0002812903765134115,
      "loss": 2.1869,
      "step": 40960
    },
    {
      "epoch": 93.11363636363636,
      "grad_norm": 0.20908565819263458,
      "learning_rate": 0.00028120093209213156,
      "loss": 2.1961,
      "step": 40970
    },
    {
      "epoch": 93.13636363636364,
      "grad_norm": 1.3695329427719116,
      "learning_rate": 0.0002811114836136034,
      "loss": 2.1902,
      "step": 40980
    },
    {
      "epoch": 93.1590909090909,
      "grad_norm": 0.39804086089134216,
      "learning_rate": 0.00028102203108945856,
      "loss": 2.198,
      "step": 40990
    },
    {
      "epoch": 93.18181818181819,
      "grad_norm": 0.223800927400589,
      "learning_rate": 0.0002809325745313291,
      "loss": 2.1804,
      "step": 41000
    },
    {
      "epoch": 93.20454545454545,
      "grad_norm": 2.5730819702148438,
      "learning_rate": 0.0002808431139508476,
      "loss": 2.1831,
      "step": 41010
    },
    {
      "epoch": 93.22727272727273,
      "grad_norm": 0.24975456297397614,
      "learning_rate": 0.00028075364935964723,
      "loss": 2.1754,
      "step": 41020
    },
    {
      "epoch": 93.25,
      "grad_norm": 0.36183488368988037,
      "learning_rate": 0.0002806641807693617,
      "loss": 2.1977,
      "step": 41030
    },
    {
      "epoch": 93.27272727272727,
      "grad_norm": 0.24571362137794495,
      "learning_rate": 0.0002805747081916249,
      "loss": 2.1819,
      "step": 41040
    },
    {
      "epoch": 93.29545454545455,
      "grad_norm": 0.2937125861644745,
      "learning_rate": 0.0002804852316380716,
      "loss": 2.1819,
      "step": 41050
    },
    {
      "epoch": 93.31818181818181,
      "grad_norm": 0.45926615595817566,
      "learning_rate": 0.0002803957511203371,
      "loss": 2.184,
      "step": 41060
    },
    {
      "epoch": 93.3409090909091,
      "grad_norm": 0.3569449186325073,
      "learning_rate": 0.000280306266650057,
      "loss": 2.1839,
      "step": 41070
    },
    {
      "epoch": 93.36363636363636,
      "grad_norm": 0.3828505277633667,
      "learning_rate": 0.0002802167782388675,
      "loss": 2.1855,
      "step": 41080
    },
    {
      "epoch": 93.38636363636364,
      "grad_norm": 0.22939693927764893,
      "learning_rate": 0.0002801272858984054,
      "loss": 2.1863,
      "step": 41090
    },
    {
      "epoch": 93.4090909090909,
      "grad_norm": 0.18842752277851105,
      "learning_rate": 0.0002800377896403079,
      "loss": 2.1869,
      "step": 41100
    },
    {
      "epoch": 93.43181818181819,
      "grad_norm": 0.36401841044425964,
      "learning_rate": 0.00027994828947621277,
      "loss": 2.1902,
      "step": 41110
    },
    {
      "epoch": 93.45454545454545,
      "grad_norm": 0.5062578916549683,
      "learning_rate": 0.00027985878541775814,
      "loss": 2.178,
      "step": 41120
    },
    {
      "epoch": 93.47727272727273,
      "grad_norm": 0.33011704683303833,
      "learning_rate": 0.0002797692774765829,
      "loss": 2.1802,
      "step": 41130
    },
    {
      "epoch": 93.5,
      "grad_norm": 0.237963005900383,
      "learning_rate": 0.0002796797656643263,
      "loss": 2.1901,
      "step": 41140
    },
    {
      "epoch": 93.52272727272727,
      "grad_norm": 0.5250728726387024,
      "learning_rate": 0.00027959024999262813,
      "loss": 2.1865,
      "step": 41150
    },
    {
      "epoch": 93.54545454545455,
      "grad_norm": 0.26391011476516724,
      "learning_rate": 0.00027950073047312856,
      "loss": 2.199,
      "step": 41160
    },
    {
      "epoch": 93.56818181818181,
      "grad_norm": 0.23736339807510376,
      "learning_rate": 0.00027941120711746844,
      "loss": 2.2016,
      "step": 41170
    },
    {
      "epoch": 93.5909090909091,
      "grad_norm": 0.2703178822994232,
      "learning_rate": 0.000279321679937289,
      "loss": 2.2047,
      "step": 41180
    },
    {
      "epoch": 93.61363636363636,
      "grad_norm": 0.2936272919178009,
      "learning_rate": 0.00027923214894423203,
      "loss": 2.1931,
      "step": 41190
    },
    {
      "epoch": 93.63636363636364,
      "grad_norm": 0.2693263292312622,
      "learning_rate": 0.0002791426141499398,
      "loss": 2.1842,
      "step": 41200
    },
    {
      "epoch": 93.6590909090909,
      "grad_norm": 0.2355116605758667,
      "learning_rate": 0.00027905307556605507,
      "loss": 2.1988,
      "step": 41210
    },
    {
      "epoch": 93.68181818181819,
      "grad_norm": 0.36918365955352783,
      "learning_rate": 0.0002789635332042211,
      "loss": 2.184,
      "step": 41220
    },
    {
      "epoch": 93.70454545454545,
      "grad_norm": 0.654657781124115,
      "learning_rate": 0.0002788739870760816,
      "loss": 2.1885,
      "step": 41230
    },
    {
      "epoch": 93.72727272727273,
      "grad_norm": 0.26506224274635315,
      "learning_rate": 0.0002787844371932808,
      "loss": 2.1914,
      "step": 41240
    },
    {
      "epoch": 93.75,
      "grad_norm": 0.2312757819890976,
      "learning_rate": 0.0002786948835674634,
      "loss": 2.1847,
      "step": 41250
    },
    {
      "epoch": 93.77272727272727,
      "grad_norm": 0.2527734935283661,
      "learning_rate": 0.0002786053262102747,
      "loss": 2.1956,
      "step": 41260
    },
    {
      "epoch": 93.79545454545455,
      "grad_norm": 0.14067167043685913,
      "learning_rate": 0.0002785157651333603,
      "loss": 2.1752,
      "step": 41270
    },
    {
      "epoch": 93.81818181818181,
      "grad_norm": 0.22421970963478088,
      "learning_rate": 0.0002784262003483664,
      "loss": 2.1954,
      "step": 41280
    },
    {
      "epoch": 93.8409090909091,
      "grad_norm": 0.456947386264801,
      "learning_rate": 0.0002783366318669397,
      "loss": 2.1967,
      "step": 41290
    },
    {
      "epoch": 93.86363636363636,
      "grad_norm": 0.21436098217964172,
      "learning_rate": 0.0002782470597007272,
      "loss": 2.1849,
      "step": 41300
    },
    {
      "epoch": 93.88636363636364,
      "grad_norm": 0.2456367462873459,
      "learning_rate": 0.0002781574838613767,
      "loss": 2.1873,
      "step": 41310
    },
    {
      "epoch": 93.9090909090909,
      "grad_norm": 0.20633465051651,
      "learning_rate": 0.0002780679043605361,
      "loss": 2.1911,
      "step": 41320
    },
    {
      "epoch": 93.93181818181819,
      "grad_norm": 0.31725767254829407,
      "learning_rate": 0.00027797832120985425,
      "loss": 2.194,
      "step": 41330
    },
    {
      "epoch": 93.95454545454545,
      "grad_norm": 0.22326035797595978,
      "learning_rate": 0.00027788873442098,
      "loss": 2.1917,
      "step": 41340
    },
    {
      "epoch": 93.97727272727273,
      "grad_norm": 0.29834362864494324,
      "learning_rate": 0.00027779914400556275,
      "loss": 2.1868,
      "step": 41350
    },
    {
      "epoch": 94.0,
      "grad_norm": 0.4038129150867462,
      "learning_rate": 0.0002777095499752528,
      "loss": 2.1981,
      "step": 41360
    },
    {
      "epoch": 94.0,
      "eval_loss": 1.0992785692214966,
      "eval_runtime": 8.8199,
      "eval_samples_per_second": 3450.261,
      "eval_steps_per_second": 13.492,
      "step": 41360
    },
    {
      "epoch": 94.02272727272727,
      "grad_norm": 0.1866881549358368,
      "learning_rate": 0.00027761995234170035,
      "loss": 2.1856,
      "step": 41370
    },
    {
      "epoch": 94.04545454545455,
      "grad_norm": 0.17096945643424988,
      "learning_rate": 0.00027753035111655646,
      "loss": 2.1814,
      "step": 41380
    },
    {
      "epoch": 94.06818181818181,
      "grad_norm": 0.24844224750995636,
      "learning_rate": 0.0002774407463114725,
      "loss": 2.2,
      "step": 41390
    },
    {
      "epoch": 94.0909090909091,
      "grad_norm": 2.465754270553589,
      "learning_rate": 0.0002773511379381003,
      "loss": 2.1844,
      "step": 41400
    },
    {
      "epoch": 94.11363636363636,
      "grad_norm": 0.17752788960933685,
      "learning_rate": 0.00027726152600809217,
      "loss": 2.1798,
      "step": 41410
    },
    {
      "epoch": 94.13636363636364,
      "grad_norm": 0.19494323432445526,
      "learning_rate": 0.00027717191053310106,
      "loss": 2.1974,
      "step": 41420
    },
    {
      "epoch": 94.1590909090909,
      "grad_norm": 0.24818624556064606,
      "learning_rate": 0.0002770822915247799,
      "loss": 2.1955,
      "step": 41430
    },
    {
      "epoch": 94.18181818181819,
      "grad_norm": 0.4576416611671448,
      "learning_rate": 0.00027699266899478274,
      "loss": 2.1821,
      "step": 41440
    },
    {
      "epoch": 94.20454545454545,
      "grad_norm": 0.9554863572120667,
      "learning_rate": 0.00027690304295476355,
      "loss": 2.1845,
      "step": 41450
    },
    {
      "epoch": 94.22727272727273,
      "grad_norm": 0.22366704046726227,
      "learning_rate": 0.0002768134134163771,
      "loss": 2.1812,
      "step": 41460
    },
    {
      "epoch": 94.25,
      "grad_norm": 0.29205355048179626,
      "learning_rate": 0.0002767237803912783,
      "loss": 2.1859,
      "step": 41470
    },
    {
      "epoch": 94.27272727272727,
      "grad_norm": 0.29327526688575745,
      "learning_rate": 0.00027663414389112264,
      "loss": 2.2019,
      "step": 41480
    },
    {
      "epoch": 94.29545454545455,
      "grad_norm": 0.21312685310840607,
      "learning_rate": 0.00027654450392756633,
      "loss": 2.1904,
      "step": 41490
    },
    {
      "epoch": 94.31818181818181,
      "grad_norm": 0.7278209328651428,
      "learning_rate": 0.0002764548605122656,
      "loss": 2.1912,
      "step": 41500
    },
    {
      "epoch": 94.3409090909091,
      "grad_norm": 0.9835716485977173,
      "learning_rate": 0.0002763652136568775,
      "loss": 2.1846,
      "step": 41510
    },
    {
      "epoch": 94.36363636363636,
      "grad_norm": 0.4202701151371002,
      "learning_rate": 0.0002762755633730593,
      "loss": 2.1911,
      "step": 41520
    },
    {
      "epoch": 94.38636363636364,
      "grad_norm": 0.6307283639907837,
      "learning_rate": 0.0002761859096724687,
      "loss": 2.1942,
      "step": 41530
    },
    {
      "epoch": 94.4090909090909,
      "grad_norm": 0.3457052707672119,
      "learning_rate": 0.000276096252566764,
      "loss": 2.1918,
      "step": 41540
    },
    {
      "epoch": 94.43181818181819,
      "grad_norm": 0.27941176295280457,
      "learning_rate": 0.00027600659206760374,
      "loss": 2.1959,
      "step": 41550
    },
    {
      "epoch": 94.45454545454545,
      "grad_norm": 0.1562282145023346,
      "learning_rate": 0.0002759169281866472,
      "loss": 2.1842,
      "step": 41560
    },
    {
      "epoch": 94.47727272727273,
      "grad_norm": 0.34838080406188965,
      "learning_rate": 0.00027582726093555384,
      "loss": 2.1962,
      "step": 41570
    },
    {
      "epoch": 94.5,
      "grad_norm": 0.16446180641651154,
      "learning_rate": 0.00027573759032598365,
      "loss": 2.1694,
      "step": 41580
    },
    {
      "epoch": 94.52272727272727,
      "grad_norm": 0.22020262479782104,
      "learning_rate": 0.000275647916369597,
      "loss": 2.1875,
      "step": 41590
    },
    {
      "epoch": 94.54545454545455,
      "grad_norm": 0.5354503393173218,
      "learning_rate": 0.0002755582390780548,
      "loss": 2.1802,
      "step": 41600
    },
    {
      "epoch": 94.56818181818181,
      "grad_norm": 0.3627038598060608,
      "learning_rate": 0.0002754685584630183,
      "loss": 2.1807,
      "step": 41610
    },
    {
      "epoch": 94.5909090909091,
      "grad_norm": 0.2416548728942871,
      "learning_rate": 0.0002753788745361493,
      "loss": 2.1933,
      "step": 41620
    },
    {
      "epoch": 94.61363636363636,
      "grad_norm": 0.26820239424705505,
      "learning_rate": 0.0002752891873091098,
      "loss": 2.1915,
      "step": 41630
    },
    {
      "epoch": 94.63636363636364,
      "grad_norm": 0.21899621188640594,
      "learning_rate": 0.00027519949679356257,
      "loss": 2.1783,
      "step": 41640
    },
    {
      "epoch": 94.6590909090909,
      "grad_norm": 0.4112355709075928,
      "learning_rate": 0.0002751098030011705,
      "loss": 2.1974,
      "step": 41650
    },
    {
      "epoch": 94.68181818181819,
      "grad_norm": 0.25724321603775024,
      "learning_rate": 0.000275020105943597,
      "loss": 2.1888,
      "step": 41660
    },
    {
      "epoch": 94.70454545454545,
      "grad_norm": 0.30264028906822205,
      "learning_rate": 0.000274930405632506,
      "loss": 2.1859,
      "step": 41670
    },
    {
      "epoch": 94.72727272727273,
      "grad_norm": 0.6365244388580322,
      "learning_rate": 0.0002748407020795617,
      "loss": 2.2024,
      "step": 41680
    },
    {
      "epoch": 94.75,
      "grad_norm": 0.9800862669944763,
      "learning_rate": 0.00027475099529642886,
      "loss": 2.1718,
      "step": 41690
    },
    {
      "epoch": 94.77272727272727,
      "grad_norm": 0.23434144258499146,
      "learning_rate": 0.0002746612852947726,
      "loss": 2.1886,
      "step": 41700
    },
    {
      "epoch": 94.79545454545455,
      "grad_norm": 0.4752289056777954,
      "learning_rate": 0.0002745715720862585,
      "loss": 2.1875,
      "step": 41710
    },
    {
      "epoch": 94.81818181818181,
      "grad_norm": 0.2804548740386963,
      "learning_rate": 0.00027448185568255246,
      "loss": 2.1784,
      "step": 41720
    },
    {
      "epoch": 94.8409090909091,
      "grad_norm": 0.20041842758655548,
      "learning_rate": 0.00027439213609532076,
      "loss": 2.2022,
      "step": 41730
    },
    {
      "epoch": 94.86363636363636,
      "grad_norm": 0.23010092973709106,
      "learning_rate": 0.0002743024133362304,
      "loss": 2.1847,
      "step": 41740
    },
    {
      "epoch": 94.88636363636364,
      "grad_norm": 0.34532785415649414,
      "learning_rate": 0.00027421268741694837,
      "loss": 2.1918,
      "step": 41750
    },
    {
      "epoch": 94.9090909090909,
      "grad_norm": 0.2887188196182251,
      "learning_rate": 0.00027412295834914246,
      "loss": 2.1845,
      "step": 41760
    },
    {
      "epoch": 94.93181818181819,
      "grad_norm": 0.24379272758960724,
      "learning_rate": 0.00027403322614448066,
      "loss": 2.1986,
      "step": 41770
    },
    {
      "epoch": 94.95454545454545,
      "grad_norm": 0.19711989164352417,
      "learning_rate": 0.0002739434908146312,
      "loss": 2.1954,
      "step": 41780
    },
    {
      "epoch": 94.97727272727273,
      "grad_norm": 0.2647674083709717,
      "learning_rate": 0.00027385375237126317,
      "loss": 2.1831,
      "step": 41790
    },
    {
      "epoch": 95.0,
      "grad_norm": 3.171880006790161,
      "learning_rate": 0.00027376401082604564,
      "loss": 2.1882,
      "step": 41800
    },
    {
      "epoch": 95.0,
      "eval_loss": 1.0991698503494263,
      "eval_runtime": 8.7047,
      "eval_samples_per_second": 3495.916,
      "eval_steps_per_second": 13.671,
      "step": 41800
    },
    {
      "epoch": 95.02272727272727,
      "grad_norm": 0.23440895974636078,
      "learning_rate": 0.0002736742661906484,
      "loss": 2.1796,
      "step": 41810
    },
    {
      "epoch": 95.04545454545455,
      "grad_norm": 0.27775880694389343,
      "learning_rate": 0.00027358451847674136,
      "loss": 2.173,
      "step": 41820
    },
    {
      "epoch": 95.06818181818181,
      "grad_norm": 0.213583305478096,
      "learning_rate": 0.00027349476769599504,
      "loss": 2.1816,
      "step": 41830
    },
    {
      "epoch": 95.0909090909091,
      "grad_norm": 0.3144945800304413,
      "learning_rate": 0.0002734050138600802,
      "loss": 2.1883,
      "step": 41840
    },
    {
      "epoch": 95.11363636363636,
      "grad_norm": 0.2047562152147293,
      "learning_rate": 0.0002733152569806682,
      "loss": 2.188,
      "step": 41850
    },
    {
      "epoch": 95.13636363636364,
      "grad_norm": 0.5554564595222473,
      "learning_rate": 0.0002732254970694305,
      "loss": 2.1935,
      "step": 41860
    },
    {
      "epoch": 95.1590909090909,
      "grad_norm": 0.48469170928001404,
      "learning_rate": 0.00027313573413803935,
      "loss": 2.1905,
      "step": 41870
    },
    {
      "epoch": 95.18181818181819,
      "grad_norm": 0.301260769367218,
      "learning_rate": 0.000273045968198167,
      "loss": 2.1951,
      "step": 41880
    },
    {
      "epoch": 95.20454545454545,
      "grad_norm": 0.30614033341407776,
      "learning_rate": 0.0002729561992614864,
      "loss": 2.1813,
      "step": 41890
    },
    {
      "epoch": 95.22727272727273,
      "grad_norm": 0.260023295879364,
      "learning_rate": 0.00027286642733967073,
      "loss": 2.187,
      "step": 41900
    },
    {
      "epoch": 95.25,
      "grad_norm": 0.28415241837501526,
      "learning_rate": 0.00027277665244439334,
      "loss": 2.1863,
      "step": 41910
    },
    {
      "epoch": 95.27272727272727,
      "grad_norm": 0.330718994140625,
      "learning_rate": 0.0002726868745873286,
      "loss": 2.1848,
      "step": 41920
    },
    {
      "epoch": 95.29545454545455,
      "grad_norm": 0.2735978066921234,
      "learning_rate": 0.00027259709378015055,
      "loss": 2.1847,
      "step": 41930
    },
    {
      "epoch": 95.31818181818181,
      "grad_norm": 0.4627830982208252,
      "learning_rate": 0.00027250731003453414,
      "loss": 2.1744,
      "step": 41940
    },
    {
      "epoch": 95.3409090909091,
      "grad_norm": 0.4134526550769806,
      "learning_rate": 0.00027241752336215445,
      "loss": 2.1936,
      "step": 41950
    },
    {
      "epoch": 95.36363636363636,
      "grad_norm": 0.25313541293144226,
      "learning_rate": 0.000272327733774687,
      "loss": 2.1931,
      "step": 41960
    },
    {
      "epoch": 95.38636363636364,
      "grad_norm": 0.19099807739257812,
      "learning_rate": 0.00027223794128380756,
      "loss": 2.1798,
      "step": 41970
    },
    {
      "epoch": 95.4090909090909,
      "grad_norm": 0.24212466180324554,
      "learning_rate": 0.00027214814590119255,
      "loss": 2.1925,
      "step": 41980
    },
    {
      "epoch": 95.43181818181819,
      "grad_norm": 0.2819402813911438,
      "learning_rate": 0.0002720583476385185,
      "loss": 2.1907,
      "step": 41990
    },
    {
      "epoch": 95.45454545454545,
      "grad_norm": 0.5644586086273193,
      "learning_rate": 0.00027196854650746253,
      "loss": 2.1983,
      "step": 42000
    },
    {
      "epoch": 95.47727272727273,
      "grad_norm": 0.6077503561973572,
      "learning_rate": 0.00027187874251970195,
      "loss": 2.2025,
      "step": 42010
    },
    {
      "epoch": 95.5,
      "grad_norm": 0.2060813307762146,
      "learning_rate": 0.0002717889356869146,
      "loss": 2.1909,
      "step": 42020
    },
    {
      "epoch": 95.52272727272727,
      "grad_norm": 0.6198284029960632,
      "learning_rate": 0.0002716991260207785,
      "loss": 2.1876,
      "step": 42030
    },
    {
      "epoch": 95.54545454545455,
      "grad_norm": 0.5242209434509277,
      "learning_rate": 0.00027160931353297223,
      "loss": 2.177,
      "step": 42040
    },
    {
      "epoch": 95.56818181818181,
      "grad_norm": 0.29892340302467346,
      "learning_rate": 0.0002715194982351747,
      "loss": 2.1826,
      "step": 42050
    },
    {
      "epoch": 95.5909090909091,
      "grad_norm": 0.19755056500434875,
      "learning_rate": 0.0002714296801390649,
      "loss": 2.1858,
      "step": 42060
    },
    {
      "epoch": 95.61363636363636,
      "grad_norm": 0.4526880383491516,
      "learning_rate": 0.00027133985925632283,
      "loss": 2.195,
      "step": 42070
    },
    {
      "epoch": 95.63636363636364,
      "grad_norm": 0.29289141297340393,
      "learning_rate": 0.00027125003559862816,
      "loss": 2.1871,
      "step": 42080
    },
    {
      "epoch": 95.6590909090909,
      "grad_norm": 0.33319327235221863,
      "learning_rate": 0.00027116020917766125,
      "loss": 2.1933,
      "step": 42090
    },
    {
      "epoch": 95.68181818181819,
      "grad_norm": 0.26539862155914307,
      "learning_rate": 0.00027107038000510284,
      "loss": 2.182,
      "step": 42100
    },
    {
      "epoch": 95.70454545454545,
      "grad_norm": 0.22665274143218994,
      "learning_rate": 0.0002709805480926339,
      "loss": 2.1762,
      "step": 42110
    },
    {
      "epoch": 95.72727272727273,
      "grad_norm": 0.7311712503433228,
      "learning_rate": 0.00027089071345193596,
      "loss": 2.1894,
      "step": 42120
    },
    {
      "epoch": 95.75,
      "grad_norm": 0.2952178120613098,
      "learning_rate": 0.00027080087609469063,
      "loss": 2.1953,
      "step": 42130
    },
    {
      "epoch": 95.77272727272727,
      "grad_norm": 2.1000301837921143,
      "learning_rate": 0.00027071103603258,
      "loss": 2.1954,
      "step": 42140
    },
    {
      "epoch": 95.79545454545455,
      "grad_norm": 0.2617282271385193,
      "learning_rate": 0.0002706211932772866,
      "loss": 2.1888,
      "step": 42150
    },
    {
      "epoch": 95.81818181818181,
      "grad_norm": 0.2232731282711029,
      "learning_rate": 0.00027053134784049313,
      "loss": 2.1807,
      "step": 42160
    },
    {
      "epoch": 95.8409090909091,
      "grad_norm": 0.34348613023757935,
      "learning_rate": 0.00027044149973388297,
      "loss": 2.1963,
      "step": 42170
    },
    {
      "epoch": 95.86363636363636,
      "grad_norm": 0.3813655376434326,
      "learning_rate": 0.00027035164896913933,
      "loss": 2.1922,
      "step": 42180
    },
    {
      "epoch": 95.88636363636364,
      "grad_norm": 0.26518601179122925,
      "learning_rate": 0.0002702617955579463,
      "loss": 2.1876,
      "step": 42190
    },
    {
      "epoch": 95.9090909090909,
      "grad_norm": 0.4161124527454376,
      "learning_rate": 0.000270171939511988,
      "loss": 2.1907,
      "step": 42200
    },
    {
      "epoch": 95.93181818181819,
      "grad_norm": 0.2295798510313034,
      "learning_rate": 0.0002700820808429488,
      "loss": 2.187,
      "step": 42210
    },
    {
      "epoch": 95.95454545454545,
      "grad_norm": 0.18293915688991547,
      "learning_rate": 0.0002699922195625137,
      "loss": 2.1913,
      "step": 42220
    },
    {
      "epoch": 95.97727272727273,
      "grad_norm": 0.3771669566631317,
      "learning_rate": 0.000269902355682368,
      "loss": 2.1789,
      "step": 42230
    },
    {
      "epoch": 96.0,
      "grad_norm": 1.0667362213134766,
      "learning_rate": 0.0002698124892141971,
      "loss": 2.1969,
      "step": 42240
    },
    {
      "epoch": 96.0,
      "eval_loss": 1.098802924156189,
      "eval_runtime": 8.71,
      "eval_samples_per_second": 3493.79,
      "eval_steps_per_second": 13.662,
      "step": 42240
    },
    {
      "epoch": 96.02272727272727,
      "grad_norm": 0.3043670356273651,
      "learning_rate": 0.000269722620169687,
      "loss": 2.1785,
      "step": 42250
    },
    {
      "epoch": 96.04545454545455,
      "grad_norm": 0.3116871416568756,
      "learning_rate": 0.00026963274856052385,
      "loss": 2.1802,
      "step": 42260
    },
    {
      "epoch": 96.06818181818181,
      "grad_norm": 0.2756338119506836,
      "learning_rate": 0.0002695428743983942,
      "loss": 2.1841,
      "step": 42270
    },
    {
      "epoch": 96.0909090909091,
      "grad_norm": 0.19262178242206573,
      "learning_rate": 0.00026945299769498494,
      "loss": 2.1727,
      "step": 42280
    },
    {
      "epoch": 96.11363636363636,
      "grad_norm": 0.6832932233810425,
      "learning_rate": 0.00026936311846198334,
      "loss": 2.1921,
      "step": 42290
    },
    {
      "epoch": 96.13636363636364,
      "grad_norm": 0.274973064661026,
      "learning_rate": 0.00026927323671107695,
      "loss": 2.2018,
      "step": 42300
    },
    {
      "epoch": 96.1590909090909,
      "grad_norm": 0.28180891275405884,
      "learning_rate": 0.0002691833524539536,
      "loss": 2.192,
      "step": 42310
    },
    {
      "epoch": 96.18181818181819,
      "grad_norm": 0.3984047472476959,
      "learning_rate": 0.0002690934657023015,
      "loss": 2.1912,
      "step": 42320
    },
    {
      "epoch": 96.20454545454545,
      "grad_norm": 0.2501627802848816,
      "learning_rate": 0.0002690035764678092,
      "loss": 2.1914,
      "step": 42330
    },
    {
      "epoch": 96.22727272727273,
      "grad_norm": 0.2705574035644531,
      "learning_rate": 0.00026891368476216545,
      "loss": 2.1917,
      "step": 42340
    },
    {
      "epoch": 96.25,
      "grad_norm": 0.28981900215148926,
      "learning_rate": 0.0002688237905970595,
      "loss": 2.183,
      "step": 42350
    },
    {
      "epoch": 96.27272727272727,
      "grad_norm": 0.23408827185630798,
      "learning_rate": 0.00026873389398418085,
      "loss": 2.1915,
      "step": 42360
    },
    {
      "epoch": 96.29545454545455,
      "grad_norm": 0.27026689052581787,
      "learning_rate": 0.00026864399493521933,
      "loss": 2.1817,
      "step": 42370
    },
    {
      "epoch": 96.31818181818181,
      "grad_norm": 0.30606064200401306,
      "learning_rate": 0.000268554093461865,
      "loss": 2.1809,
      "step": 42380
    },
    {
      "epoch": 96.3409090909091,
      "grad_norm": 0.28623634576797485,
      "learning_rate": 0.00026846418957580834,
      "loss": 2.1899,
      "step": 42390
    },
    {
      "epoch": 96.36363636363636,
      "grad_norm": 0.3592647314071655,
      "learning_rate": 0.0002683742832887401,
      "loss": 2.1883,
      "step": 42400
    },
    {
      "epoch": 96.38636363636364,
      "grad_norm": 0.6460571885108948,
      "learning_rate": 0.0002682843746123512,
      "loss": 2.1947,
      "step": 42410
    },
    {
      "epoch": 96.4090909090909,
      "grad_norm": 0.4067792296409607,
      "learning_rate": 0.0002681944635583333,
      "loss": 2.1829,
      "step": 42420
    },
    {
      "epoch": 96.43181818181819,
      "grad_norm": 0.21020027995109558,
      "learning_rate": 0.0002681045501383778,
      "loss": 2.179,
      "step": 42430
    },
    {
      "epoch": 96.45454545454545,
      "grad_norm": 0.2337597906589508,
      "learning_rate": 0.00026801463436417687,
      "loss": 2.1868,
      "step": 42440
    },
    {
      "epoch": 96.47727272727273,
      "grad_norm": 0.4460461437702179,
      "learning_rate": 0.0002679247162474228,
      "loss": 2.1762,
      "step": 42450
    },
    {
      "epoch": 96.5,
      "grad_norm": 0.2630139887332916,
      "learning_rate": 0.00026783479579980805,
      "loss": 2.186,
      "step": 42460
    },
    {
      "epoch": 96.52272727272727,
      "grad_norm": 0.4158001244068146,
      "learning_rate": 0.00026774487303302574,
      "loss": 2.1905,
      "step": 42470
    },
    {
      "epoch": 96.54545454545455,
      "grad_norm": 0.3667517900466919,
      "learning_rate": 0.0002676549479587689,
      "loss": 2.1892,
      "step": 42480
    },
    {
      "epoch": 96.56818181818181,
      "grad_norm": 0.4052455425262451,
      "learning_rate": 0.0002675650205887311,
      "loss": 2.1853,
      "step": 42490
    },
    {
      "epoch": 96.5909090909091,
      "grad_norm": 0.3794988989830017,
      "learning_rate": 0.00026747509093460624,
      "loss": 2.1858,
      "step": 42500
    },
    {
      "epoch": 96.61363636363636,
      "grad_norm": 0.8346432447433472,
      "learning_rate": 0.0002673851590080883,
      "loss": 2.1887,
      "step": 42510
    },
    {
      "epoch": 96.63636363636364,
      "grad_norm": 0.2890165150165558,
      "learning_rate": 0.00026729522482087163,
      "loss": 2.1836,
      "step": 42520
    },
    {
      "epoch": 96.6590909090909,
      "grad_norm": 0.5289216637611389,
      "learning_rate": 0.00026720528838465113,
      "loss": 2.1846,
      "step": 42530
    },
    {
      "epoch": 96.68181818181819,
      "grad_norm": 0.5017997026443481,
      "learning_rate": 0.00026711534971112156,
      "loss": 2.1971,
      "step": 42540
    },
    {
      "epoch": 96.70454545454545,
      "grad_norm": 0.3048551678657532,
      "learning_rate": 0.00026702540881197835,
      "loss": 2.1827,
      "step": 42550
    },
    {
      "epoch": 96.72727272727273,
      "grad_norm": 0.5275706052780151,
      "learning_rate": 0.000266935465698917,
      "loss": 2.1954,
      "step": 42560
    },
    {
      "epoch": 96.75,
      "grad_norm": 0.4244459867477417,
      "learning_rate": 0.00026684552038363336,
      "loss": 2.1954,
      "step": 42570
    },
    {
      "epoch": 96.77272727272727,
      "grad_norm": 0.5503268837928772,
      "learning_rate": 0.00026675557287782366,
      "loss": 2.1975,
      "step": 42580
    },
    {
      "epoch": 96.79545454545455,
      "grad_norm": 0.25238466262817383,
      "learning_rate": 0.0002666656231931841,
      "loss": 2.1947,
      "step": 42590
    },
    {
      "epoch": 96.81818181818181,
      "grad_norm": 0.2860260605812073,
      "learning_rate": 0.0002665756713414116,
      "loss": 2.1826,
      "step": 42600
    },
    {
      "epoch": 96.8409090909091,
      "grad_norm": 0.28424063324928284,
      "learning_rate": 0.0002664857173342031,
      "loss": 2.1973,
      "step": 42610
    },
    {
      "epoch": 96.86363636363636,
      "grad_norm": 0.28085818886756897,
      "learning_rate": 0.0002663957611832559,
      "loss": 2.1923,
      "step": 42620
    },
    {
      "epoch": 96.88636363636364,
      "grad_norm": 0.22390493750572205,
      "learning_rate": 0.0002663058029002676,
      "loss": 2.1882,
      "step": 42630
    },
    {
      "epoch": 96.9090909090909,
      "grad_norm": 0.6211274862289429,
      "learning_rate": 0.00026621584249693575,
      "loss": 2.1833,
      "step": 42640
    },
    {
      "epoch": 96.93181818181819,
      "grad_norm": 0.2304055094718933,
      "learning_rate": 0.00026612587998495874,
      "loss": 2.1968,
      "step": 42650
    },
    {
      "epoch": 96.95454545454545,
      "grad_norm": 0.19693833589553833,
      "learning_rate": 0.00026603591537603476,
      "loss": 2.1975,
      "step": 42660
    },
    {
      "epoch": 96.97727272727273,
      "grad_norm": 0.2664271891117096,
      "learning_rate": 0.00026594594868186263,
      "loss": 2.187,
      "step": 42670
    },
    {
      "epoch": 97.0,
      "grad_norm": 0.4248635172843933,
      "learning_rate": 0.0002658559799141411,
      "loss": 2.1916,
      "step": 42680
    },
    {
      "epoch": 97.0,
      "eval_loss": 1.099517583847046,
      "eval_runtime": 8.7179,
      "eval_samples_per_second": 3490.623,
      "eval_steps_per_second": 13.65,
      "step": 42680
    },
    {
      "epoch": 97.02272727272727,
      "grad_norm": 0.5463442802429199,
      "learning_rate": 0.00026576600908456954,
      "loss": 2.1728,
      "step": 42690
    },
    {
      "epoch": 97.04545454545455,
      "grad_norm": 0.28090566396713257,
      "learning_rate": 0.00026567603620484723,
      "loss": 2.1863,
      "step": 42700
    },
    {
      "epoch": 97.06818181818181,
      "grad_norm": 0.3656338155269623,
      "learning_rate": 0.00026558606128667403,
      "loss": 2.1895,
      "step": 42710
    },
    {
      "epoch": 97.0909090909091,
      "grad_norm": 0.3417101502418518,
      "learning_rate": 0.00026549608434174983,
      "loss": 2.1937,
      "step": 42720
    },
    {
      "epoch": 97.11363636363636,
      "grad_norm": 0.2956574261188507,
      "learning_rate": 0.00026540610538177496,
      "loss": 2.2004,
      "step": 42730
    },
    {
      "epoch": 97.13636363636364,
      "grad_norm": 0.22674959897994995,
      "learning_rate": 0.0002653161244184498,
      "loss": 2.2005,
      "step": 42740
    },
    {
      "epoch": 97.1590909090909,
      "grad_norm": 0.2683499753475189,
      "learning_rate": 0.0002652261414634754,
      "loss": 2.1884,
      "step": 42750
    },
    {
      "epoch": 97.18181818181819,
      "grad_norm": 0.5097533464431763,
      "learning_rate": 0.0002651361565285525,
      "loss": 2.1819,
      "step": 42760
    },
    {
      "epoch": 97.20454545454545,
      "grad_norm": 0.4562419652938843,
      "learning_rate": 0.00026504616962538254,
      "loss": 2.1803,
      "step": 42770
    },
    {
      "epoch": 97.22727272727273,
      "grad_norm": 0.3827490508556366,
      "learning_rate": 0.0002649561807656671,
      "loss": 2.1848,
      "step": 42780
    },
    {
      "epoch": 97.25,
      "grad_norm": 0.30242690443992615,
      "learning_rate": 0.00026486618996110777,
      "loss": 2.188,
      "step": 42790
    },
    {
      "epoch": 97.27272727272727,
      "grad_norm": 0.46605879068374634,
      "learning_rate": 0.00026477619722340686,
      "loss": 2.1817,
      "step": 42800
    },
    {
      "epoch": 97.29545454545455,
      "grad_norm": 1.0059784650802612,
      "learning_rate": 0.00026468620256426656,
      "loss": 2.1932,
      "step": 42810
    },
    {
      "epoch": 97.31818181818181,
      "grad_norm": 0.3975282609462738,
      "learning_rate": 0.00026459620599538945,
      "loss": 2.1823,
      "step": 42820
    },
    {
      "epoch": 97.3409090909091,
      "grad_norm": 2.467214822769165,
      "learning_rate": 0.00026450620752847836,
      "loss": 2.1878,
      "step": 42830
    },
    {
      "epoch": 97.36363636363636,
      "grad_norm": 0.38204848766326904,
      "learning_rate": 0.00026441620717523623,
      "loss": 2.1874,
      "step": 42840
    },
    {
      "epoch": 97.38636363636364,
      "grad_norm": 0.22655370831489563,
      "learning_rate": 0.0002643262049473665,
      "loss": 2.1822,
      "step": 42850
    },
    {
      "epoch": 97.4090909090909,
      "grad_norm": 0.405272901058197,
      "learning_rate": 0.00026423620085657263,
      "loss": 2.1861,
      "step": 42860
    },
    {
      "epoch": 97.43181818181819,
      "grad_norm": 0.28569296002388,
      "learning_rate": 0.0002641461949145584,
      "loss": 2.1922,
      "step": 42870
    },
    {
      "epoch": 97.45454545454545,
      "grad_norm": 0.29369038343429565,
      "learning_rate": 0.00026405618713302786,
      "loss": 2.206,
      "step": 42880
    },
    {
      "epoch": 97.47727272727273,
      "grad_norm": 0.35002559423446655,
      "learning_rate": 0.00026396617752368523,
      "loss": 2.1879,
      "step": 42890
    },
    {
      "epoch": 97.5,
      "grad_norm": 2.6377813816070557,
      "learning_rate": 0.00026387616609823507,
      "loss": 2.1979,
      "step": 42900
    },
    {
      "epoch": 97.52272727272727,
      "grad_norm": 0.1954311728477478,
      "learning_rate": 0.00026378615286838214,
      "loss": 2.1838,
      "step": 42910
    },
    {
      "epoch": 97.54545454545455,
      "grad_norm": 0.2297327220439911,
      "learning_rate": 0.0002636961378458312,
      "loss": 2.1763,
      "step": 42920
    },
    {
      "epoch": 97.56818181818181,
      "grad_norm": 0.46574532985687256,
      "learning_rate": 0.00026360612104228784,
      "loss": 2.1838,
      "step": 42930
    },
    {
      "epoch": 97.5909090909091,
      "grad_norm": 0.41437461972236633,
      "learning_rate": 0.0002635161024694571,
      "loss": 2.186,
      "step": 42940
    },
    {
      "epoch": 97.61363636363636,
      "grad_norm": 0.37861543893814087,
      "learning_rate": 0.00026342608213904495,
      "loss": 2.186,
      "step": 42950
    },
    {
      "epoch": 97.63636363636364,
      "grad_norm": 0.40885308384895325,
      "learning_rate": 0.00026333606006275704,
      "loss": 2.1799,
      "step": 42960
    },
    {
      "epoch": 97.6590909090909,
      "grad_norm": 0.23266668617725372,
      "learning_rate": 0.00026324603625229963,
      "loss": 2.1877,
      "step": 42970
    },
    {
      "epoch": 97.68181818181819,
      "grad_norm": 0.4168427288532257,
      "learning_rate": 0.0002631560107193791,
      "loss": 2.1847,
      "step": 42980
    },
    {
      "epoch": 97.70454545454545,
      "grad_norm": 0.2285764366388321,
      "learning_rate": 0.0002630659834757019,
      "loss": 2.1828,
      "step": 42990
    },
    {
      "epoch": 97.72727272727273,
      "grad_norm": 2.0520224571228027,
      "learning_rate": 0.00026297595453297494,
      "loss": 2.1858,
      "step": 43000
    },
    {
      "epoch": 97.75,
      "grad_norm": 0.23870083689689636,
      "learning_rate": 0.0002628859239029052,
      "loss": 2.1817,
      "step": 43010
    },
    {
      "epoch": 97.77272727272727,
      "grad_norm": 0.36217764019966125,
      "learning_rate": 0.0002627958915971998,
      "loss": 2.1893,
      "step": 43020
    },
    {
      "epoch": 97.79545454545455,
      "grad_norm": 1.597022533416748,
      "learning_rate": 0.00026270585762756636,
      "loss": 2.1816,
      "step": 43030
    },
    {
      "epoch": 97.81818181818181,
      "grad_norm": 0.24489130079746246,
      "learning_rate": 0.0002626158220057124,
      "loss": 2.2002,
      "step": 43040
    },
    {
      "epoch": 97.8409090909091,
      "grad_norm": 1.034080982208252,
      "learning_rate": 0.000262525784743346,
      "loss": 2.1898,
      "step": 43050
    },
    {
      "epoch": 97.86363636363636,
      "grad_norm": 0.29722699522972107,
      "learning_rate": 0.00026243574585217504,
      "loss": 2.1825,
      "step": 43060
    },
    {
      "epoch": 97.88636363636364,
      "grad_norm": 0.19242267310619354,
      "learning_rate": 0.000262345705343908,
      "loss": 2.1885,
      "step": 43070
    },
    {
      "epoch": 97.9090909090909,
      "grad_norm": 0.3096306025981903,
      "learning_rate": 0.00026225566323025324,
      "loss": 2.1773,
      "step": 43080
    },
    {
      "epoch": 97.93181818181819,
      "grad_norm": 0.31320294737815857,
      "learning_rate": 0.0002621656195229196,
      "loss": 2.1858,
      "step": 43090
    },
    {
      "epoch": 97.95454545454545,
      "grad_norm": 0.2615235447883606,
      "learning_rate": 0.000262075574233616,
      "loss": 2.185,
      "step": 43100
    },
    {
      "epoch": 97.97727272727273,
      "grad_norm": 0.3829810619354248,
      "learning_rate": 0.0002619855273740515,
      "loss": 2.1901,
      "step": 43110
    },
    {
      "epoch": 98.0,
      "grad_norm": 0.719585657119751,
      "learning_rate": 0.0002618954789559356,
      "loss": 2.1813,
      "step": 43120
    },
    {
      "epoch": 98.0,
      "eval_loss": 1.099293828010559,
      "eval_runtime": 8.8646,
      "eval_samples_per_second": 3432.856,
      "eval_steps_per_second": 13.424,
      "step": 43120
    },
    {
      "epoch": 98.02272727272727,
      "grad_norm": 0.21579861640930176,
      "learning_rate": 0.00026180542899097777,
      "loss": 2.1819,
      "step": 43130
    },
    {
      "epoch": 98.04545454545455,
      "grad_norm": 0.4099206328392029,
      "learning_rate": 0.0002617153774908876,
      "loss": 2.1824,
      "step": 43140
    },
    {
      "epoch": 98.06818181818181,
      "grad_norm": 1.5229324102401733,
      "learning_rate": 0.0002616253244673753,
      "loss": 2.185,
      "step": 43150
    },
    {
      "epoch": 98.0909090909091,
      "grad_norm": 0.3607669770717621,
      "learning_rate": 0.00026153526993215087,
      "loss": 2.19,
      "step": 43160
    },
    {
      "epoch": 98.11363636363636,
      "grad_norm": 1.3429070711135864,
      "learning_rate": 0.0002614452138969246,
      "loss": 2.1838,
      "step": 43170
    },
    {
      "epoch": 98.13636363636364,
      "grad_norm": 0.20328199863433838,
      "learning_rate": 0.0002613551563734072,
      "loss": 2.1889,
      "step": 43180
    },
    {
      "epoch": 98.1590909090909,
      "grad_norm": 0.7361583709716797,
      "learning_rate": 0.0002612650973733093,
      "loss": 2.1861,
      "step": 43190
    },
    {
      "epoch": 98.18181818181819,
      "grad_norm": 0.2918400466442108,
      "learning_rate": 0.0002611750369083418,
      "loss": 2.1844,
      "step": 43200
    },
    {
      "epoch": 98.20454545454545,
      "grad_norm": 0.3828526437282562,
      "learning_rate": 0.00026108497499021586,
      "loss": 2.1846,
      "step": 43210
    },
    {
      "epoch": 98.22727272727273,
      "grad_norm": 0.9906165599822998,
      "learning_rate": 0.00026099491163064274,
      "loss": 2.194,
      "step": 43220
    },
    {
      "epoch": 98.25,
      "grad_norm": 0.3927241861820221,
      "learning_rate": 0.00026090484684133404,
      "loss": 2.1878,
      "step": 43230
    },
    {
      "epoch": 98.27272727272727,
      "grad_norm": 0.4968928098678589,
      "learning_rate": 0.0002608147806340013,
      "loss": 2.1853,
      "step": 43240
    },
    {
      "epoch": 98.29545454545455,
      "grad_norm": 0.7369754314422607,
      "learning_rate": 0.00026072471302035645,
      "loss": 2.1727,
      "step": 43250
    },
    {
      "epoch": 98.31818181818181,
      "grad_norm": 0.3783842921257019,
      "learning_rate": 0.0002606346440121115,
      "loss": 2.1856,
      "step": 43260
    },
    {
      "epoch": 98.3409090909091,
      "grad_norm": 0.35132256150245667,
      "learning_rate": 0.0002605445736209786,
      "loss": 2.1898,
      "step": 43270
    },
    {
      "epoch": 98.36363636363636,
      "grad_norm": 0.2856864631175995,
      "learning_rate": 0.00026045450185867037,
      "loss": 2.1916,
      "step": 43280
    },
    {
      "epoch": 98.38636363636364,
      "grad_norm": 0.38469552993774414,
      "learning_rate": 0.00026036442873689924,
      "loss": 2.1897,
      "step": 43290
    },
    {
      "epoch": 98.4090909090909,
      "grad_norm": 0.3060102164745331,
      "learning_rate": 0.0002602743542673779,
      "loss": 2.1857,
      "step": 43300
    },
    {
      "epoch": 98.43181818181819,
      "grad_norm": 0.33170104026794434,
      "learning_rate": 0.0002601842784618196,
      "loss": 2.1932,
      "step": 43310
    },
    {
      "epoch": 98.45454545454545,
      "grad_norm": 0.25623083114624023,
      "learning_rate": 0.000260094201331937,
      "loss": 2.1821,
      "step": 43320
    },
    {
      "epoch": 98.47727272727273,
      "grad_norm": 0.6025518178939819,
      "learning_rate": 0.0002600041228894437,
      "loss": 2.2019,
      "step": 43330
    },
    {
      "epoch": 98.5,
      "grad_norm": 1.3375961780548096,
      "learning_rate": 0.0002599140431460531,
      "loss": 2.1875,
      "step": 43340
    },
    {
      "epoch": 98.52272727272727,
      "grad_norm": 0.15731051564216614,
      "learning_rate": 0.0002598239621134787,
      "loss": 2.1833,
      "step": 43350
    },
    {
      "epoch": 98.54545454545455,
      "grad_norm": 0.49771296977996826,
      "learning_rate": 0.0002597338798034344,
      "loss": 2.192,
      "step": 43360
    },
    {
      "epoch": 98.56818181818181,
      "grad_norm": 0.3695463240146637,
      "learning_rate": 0.00025964379622763417,
      "loss": 2.1919,
      "step": 43370
    },
    {
      "epoch": 98.5909090909091,
      "grad_norm": 0.45310521125793457,
      "learning_rate": 0.0002595537113977921,
      "loss": 2.189,
      "step": 43380
    },
    {
      "epoch": 98.61363636363636,
      "grad_norm": 0.2597917914390564,
      "learning_rate": 0.0002594636253256224,
      "loss": 2.1861,
      "step": 43390
    },
    {
      "epoch": 98.63636363636364,
      "grad_norm": 0.3275367021560669,
      "learning_rate": 0.0002593735380228396,
      "loss": 2.1873,
      "step": 43400
    },
    {
      "epoch": 98.6590909090909,
      "grad_norm": 0.20828014612197876,
      "learning_rate": 0.0002592834495011582,
      "loss": 2.1837,
      "step": 43410
    },
    {
      "epoch": 98.68181818181819,
      "grad_norm": 0.4941541850566864,
      "learning_rate": 0.00025919335977229313,
      "loss": 2.1881,
      "step": 43420
    },
    {
      "epoch": 98.70454545454545,
      "grad_norm": 0.2903137803077698,
      "learning_rate": 0.00025910326884795914,
      "loss": 2.1772,
      "step": 43430
    },
    {
      "epoch": 98.72727272727273,
      "grad_norm": 0.23733121156692505,
      "learning_rate": 0.0002590131767398715,
      "loss": 2.1681,
      "step": 43440
    },
    {
      "epoch": 98.75,
      "grad_norm": 0.4387655258178711,
      "learning_rate": 0.0002589230834597451,
      "loss": 2.1928,
      "step": 43450
    },
    {
      "epoch": 98.77272727272727,
      "grad_norm": 0.2329002171754837,
      "learning_rate": 0.0002588329890192957,
      "loss": 2.1947,
      "step": 43460
    },
    {
      "epoch": 98.79545454545455,
      "grad_norm": 0.4109143018722534,
      "learning_rate": 0.0002587428934302386,
      "loss": 2.194,
      "step": 43470
    },
    {
      "epoch": 98.81818181818181,
      "grad_norm": 0.5009207725524902,
      "learning_rate": 0.0002586527967042896,
      "loss": 2.1833,
      "step": 43480
    },
    {
      "epoch": 98.8409090909091,
      "grad_norm": 0.4843675196170807,
      "learning_rate": 0.0002585626988531644,
      "loss": 2.1786,
      "step": 43490
    },
    {
      "epoch": 98.86363636363636,
      "grad_norm": 0.371197372674942,
      "learning_rate": 0.0002584725998885791,
      "loss": 2.1816,
      "step": 43500
    },
    {
      "epoch": 98.88636363636364,
      "grad_norm": 0.8782576322555542,
      "learning_rate": 0.0002583824998222498,
      "loss": 2.1863,
      "step": 43510
    },
    {
      "epoch": 98.9090909090909,
      "grad_norm": 0.27957361936569214,
      "learning_rate": 0.0002582923986658927,
      "loss": 2.2051,
      "step": 43520
    },
    {
      "epoch": 98.93181818181819,
      "grad_norm": 0.3004644513130188,
      "learning_rate": 0.00025820229643122426,
      "loss": 2.1866,
      "step": 43530
    },
    {
      "epoch": 98.95454545454545,
      "grad_norm": 0.25695449113845825,
      "learning_rate": 0.00025811219312996104,
      "loss": 2.1831,
      "step": 43540
    },
    {
      "epoch": 98.97727272727273,
      "grad_norm": 0.27397990226745605,
      "learning_rate": 0.0002580220887738197,
      "loss": 2.1839,
      "step": 43550
    },
    {
      "epoch": 99.0,
      "grad_norm": 0.38919803500175476,
      "learning_rate": 0.00025793198337451694,
      "loss": 2.1781,
      "step": 43560
    },
    {
      "epoch": 99.0,
      "eval_loss": 1.099241018295288,
      "eval_runtime": 8.8401,
      "eval_samples_per_second": 3442.376,
      "eval_steps_per_second": 13.461,
      "step": 43560
    },
    {
      "epoch": 99.02272727272727,
      "grad_norm": 0.2596272826194763,
      "learning_rate": 0.00025784187694376985,
      "loss": 2.18,
      "step": 43570
    },
    {
      "epoch": 99.04545454545455,
      "grad_norm": 0.288409024477005,
      "learning_rate": 0.00025775176949329555,
      "loss": 2.186,
      "step": 43580
    },
    {
      "epoch": 99.06818181818181,
      "grad_norm": 0.8097062706947327,
      "learning_rate": 0.0002576616610348112,
      "loss": 2.1818,
      "step": 43590
    },
    {
      "epoch": 99.0909090909091,
      "grad_norm": 0.6905837655067444,
      "learning_rate": 0.00025757155158003415,
      "loss": 2.1914,
      "step": 43600
    },
    {
      "epoch": 99.11363636363636,
      "grad_norm": 0.2485468089580536,
      "learning_rate": 0.00025748144114068187,
      "loss": 2.1842,
      "step": 43610
    },
    {
      "epoch": 99.13636363636364,
      "grad_norm": 0.27254626154899597,
      "learning_rate": 0.0002573913297284721,
      "loss": 2.1872,
      "step": 43620
    },
    {
      "epoch": 99.1590909090909,
      "grad_norm": 0.30968645215034485,
      "learning_rate": 0.00025730121735512243,
      "loss": 2.1836,
      "step": 43630
    },
    {
      "epoch": 99.18181818181819,
      "grad_norm": 0.32919567823410034,
      "learning_rate": 0.00025721110403235074,
      "loss": 2.1884,
      "step": 43640
    },
    {
      "epoch": 99.20454545454545,
      "grad_norm": 0.7019334435462952,
      "learning_rate": 0.0002571209897718751,
      "loss": 2.1855,
      "step": 43650
    },
    {
      "epoch": 99.22727272727273,
      "grad_norm": 0.21616993844509125,
      "learning_rate": 0.00025703087458541353,
      "loss": 2.1821,
      "step": 43660
    },
    {
      "epoch": 99.25,
      "grad_norm": 0.31590941548347473,
      "learning_rate": 0.00025694075848468434,
      "loss": 2.1749,
      "step": 43670
    },
    {
      "epoch": 99.27272727272727,
      "grad_norm": 0.26220712065696716,
      "learning_rate": 0.0002568506414814058,
      "loss": 2.1842,
      "step": 43680
    },
    {
      "epoch": 99.29545454545455,
      "grad_norm": 0.3069988489151001,
      "learning_rate": 0.00025676052358729643,
      "loss": 2.1932,
      "step": 43690
    },
    {
      "epoch": 99.31818181818181,
      "grad_norm": 0.24393241107463837,
      "learning_rate": 0.00025667040481407475,
      "loss": 2.1938,
      "step": 43700
    },
    {
      "epoch": 99.3409090909091,
      "grad_norm": 0.22872619330883026,
      "learning_rate": 0.0002565802851734596,
      "loss": 2.1833,
      "step": 43710
    },
    {
      "epoch": 99.36363636363636,
      "grad_norm": 0.3117680847644806,
      "learning_rate": 0.0002564901646771696,
      "loss": 2.1835,
      "step": 43720
    },
    {
      "epoch": 99.38636363636364,
      "grad_norm": 0.28403931856155396,
      "learning_rate": 0.0002564000433369238,
      "loss": 2.1963,
      "step": 43730
    },
    {
      "epoch": 99.4090909090909,
      "grad_norm": 0.3482005298137665,
      "learning_rate": 0.0002563099211644412,
      "loss": 2.1892,
      "step": 43740
    },
    {
      "epoch": 99.43181818181819,
      "grad_norm": 0.31267908215522766,
      "learning_rate": 0.00025621979817144085,
      "loss": 2.1916,
      "step": 43750
    },
    {
      "epoch": 99.45454545454545,
      "grad_norm": 0.32366567850112915,
      "learning_rate": 0.0002561296743696421,
      "loss": 2.1886,
      "step": 43760
    },
    {
      "epoch": 99.47727272727273,
      "grad_norm": 0.3579467833042145,
      "learning_rate": 0.0002560395497707644,
      "loss": 2.1833,
      "step": 43770
    },
    {
      "epoch": 99.5,
      "grad_norm": 0.27459582686424255,
      "learning_rate": 0.00025594942438652684,
      "loss": 2.1858,
      "step": 43780
    },
    {
      "epoch": 99.52272727272727,
      "grad_norm": 0.3222234547138214,
      "learning_rate": 0.0002558592982286494,
      "loss": 2.175,
      "step": 43790
    },
    {
      "epoch": 99.54545454545455,
      "grad_norm": 0.2983311414718628,
      "learning_rate": 0.0002557691713088515,
      "loss": 2.1846,
      "step": 43800
    },
    {
      "epoch": 99.56818181818181,
      "grad_norm": 0.2108256220817566,
      "learning_rate": 0.0002556790436388529,
      "loss": 2.1864,
      "step": 43810
    },
    {
      "epoch": 99.5909090909091,
      "grad_norm": 0.2974948287010193,
      "learning_rate": 0.00025558891523037354,
      "loss": 2.1832,
      "step": 43820
    },
    {
      "epoch": 99.61363636363636,
      "grad_norm": 0.7916935682296753,
      "learning_rate": 0.0002554987860951332,
      "loss": 2.1955,
      "step": 43830
    },
    {
      "epoch": 99.63636363636364,
      "grad_norm": 0.5578203201293945,
      "learning_rate": 0.0002554086562448522,
      "loss": 2.1881,
      "step": 43840
    },
    {
      "epoch": 99.6590909090909,
      "grad_norm": 0.6827365159988403,
      "learning_rate": 0.00025531852569125033,
      "loss": 2.1869,
      "step": 43850
    },
    {
      "epoch": 99.68181818181819,
      "grad_norm": 0.5515667796134949,
      "learning_rate": 0.00025522839444604823,
      "loss": 2.1916,
      "step": 43860
    },
    {
      "epoch": 99.70454545454545,
      "grad_norm": 0.6015418767929077,
      "learning_rate": 0.0002551382625209659,
      "loss": 2.1789,
      "step": 43870
    },
    {
      "epoch": 99.72727272727273,
      "grad_norm": 0.23284295201301575,
      "learning_rate": 0.0002550481299277238,
      "loss": 2.1818,
      "step": 43880
    },
    {
      "epoch": 99.75,
      "grad_norm": 0.37856683135032654,
      "learning_rate": 0.00025495799667804255,
      "loss": 2.1946,
      "step": 43890
    },
    {
      "epoch": 99.77272727272727,
      "grad_norm": 0.20441816747188568,
      "learning_rate": 0.00025486786278364253,
      "loss": 2.1872,
      "step": 43900
    },
    {
      "epoch": 99.79545454545455,
      "grad_norm": 0.29385286569595337,
      "learning_rate": 0.0002547777282562446,
      "loss": 2.1767,
      "step": 43910
    },
    {
      "epoch": 99.81818181818181,
      "grad_norm": 0.5310907363891602,
      "learning_rate": 0.0002546875931075694,
      "loss": 2.1864,
      "step": 43920
    },
    {
      "epoch": 99.8409090909091,
      "grad_norm": 0.3632424771785736,
      "learning_rate": 0.00025459745734933783,
      "loss": 2.1895,
      "step": 43930
    },
    {
      "epoch": 99.86363636363636,
      "grad_norm": 0.42506805062294006,
      "learning_rate": 0.00025450732099327065,
      "loss": 2.1876,
      "step": 43940
    },
    {
      "epoch": 99.88636363636364,
      "grad_norm": 0.2941652238368988,
      "learning_rate": 0.00025441718405108893,
      "loss": 2.1896,
      "step": 43950
    },
    {
      "epoch": 99.9090909090909,
      "grad_norm": 0.14715491235256195,
      "learning_rate": 0.00025432704653451376,
      "loss": 2.1859,
      "step": 43960
    },
    {
      "epoch": 99.93181818181819,
      "grad_norm": 0.2154303640127182,
      "learning_rate": 0.0002542369084552663,
      "loss": 2.1904,
      "step": 43970
    },
    {
      "epoch": 99.95454545454545,
      "grad_norm": 0.32602107524871826,
      "learning_rate": 0.0002541467698250676,
      "loss": 2.1751,
      "step": 43980
    },
    {
      "epoch": 99.97727272727273,
      "grad_norm": 0.23089878261089325,
      "learning_rate": 0.00025405663065563905,
      "loss": 2.1977,
      "step": 43990
    },
    {
      "epoch": 100.0,
      "grad_norm": 0.29222196340560913,
      "learning_rate": 0.000253966490958702,
      "loss": 2.173,
      "step": 44000
    },
    {
      "epoch": 100.0,
      "eval_loss": 1.0993164777755737,
      "eval_runtime": 8.7507,
      "eval_samples_per_second": 3477.542,
      "eval_steps_per_second": 13.599,
      "step": 44000
    },
    {
      "epoch": 100.02272727272727,
      "grad_norm": 0.3022078275680542,
      "learning_rate": 0.0002538763507459779,
      "loss": 2.1933,
      "step": 44010
    },
    {
      "epoch": 100.04545454545455,
      "grad_norm": 0.28517013788223267,
      "learning_rate": 0.0002537862100291881,
      "loss": 2.1737,
      "step": 44020
    },
    {
      "epoch": 100.06818181818181,
      "grad_norm": 0.3952747881412506,
      "learning_rate": 0.00025369606882005423,
      "loss": 2.1875,
      "step": 44030
    },
    {
      "epoch": 100.0909090909091,
      "grad_norm": 0.30932578444480896,
      "learning_rate": 0.00025360592713029794,
      "loss": 2.1789,
      "step": 44040
    },
    {
      "epoch": 100.11363636363636,
      "grad_norm": 0.2783048450946808,
      "learning_rate": 0.0002535157849716409,
      "loss": 2.1881,
      "step": 44050
    },
    {
      "epoch": 100.13636363636364,
      "grad_norm": 0.4633084535598755,
      "learning_rate": 0.0002534256423558048,
      "loss": 2.1911,
      "step": 44060
    },
    {
      "epoch": 100.1590909090909,
      "grad_norm": 0.29288962483406067,
      "learning_rate": 0.0002533354992945115,
      "loss": 2.1757,
      "step": 44070
    },
    {
      "epoch": 100.18181818181819,
      "grad_norm": 0.28988176584243774,
      "learning_rate": 0.00025324535579948274,
      "loss": 2.177,
      "step": 44080
    },
    {
      "epoch": 100.20454545454545,
      "grad_norm": 0.22960561513900757,
      "learning_rate": 0.0002531552118824405,
      "loss": 2.1794,
      "step": 44090
    },
    {
      "epoch": 100.22727272727273,
      "grad_norm": 0.4220755100250244,
      "learning_rate": 0.00025306506755510676,
      "loss": 2.1918,
      "step": 44100
    },
    {
      "epoch": 100.25,
      "grad_norm": 0.24207893013954163,
      "learning_rate": 0.00025297492282920356,
      "loss": 2.1758,
      "step": 44110
    },
    {
      "epoch": 100.27272727272727,
      "grad_norm": 0.3397548496723175,
      "learning_rate": 0.00025288477771645296,
      "loss": 2.1902,
      "step": 44120
    },
    {
      "epoch": 100.29545454545455,
      "grad_norm": 0.21076904237270355,
      "learning_rate": 0.000252794632228577,
      "loss": 2.1742,
      "step": 44130
    },
    {
      "epoch": 100.31818181818181,
      "grad_norm": 0.27669593691825867,
      "learning_rate": 0.0002527044863772979,
      "loss": 2.1744,
      "step": 44140
    },
    {
      "epoch": 100.3409090909091,
      "grad_norm": 0.27256545424461365,
      "learning_rate": 0.0002526143401743378,
      "loss": 2.1875,
      "step": 44150
    },
    {
      "epoch": 100.36363636363636,
      "grad_norm": 0.31921619176864624,
      "learning_rate": 0.0002525241936314192,
      "loss": 2.1844,
      "step": 44160
    },
    {
      "epoch": 100.38636363636364,
      "grad_norm": 0.1702672392129898,
      "learning_rate": 0.0002524340467602642,
      "loss": 2.1943,
      "step": 44170
    },
    {
      "epoch": 100.4090909090909,
      "grad_norm": 0.29688453674316406,
      "learning_rate": 0.0002523438995725951,
      "loss": 2.1781,
      "step": 44180
    },
    {
      "epoch": 100.43181818181819,
      "grad_norm": 0.3758556842803955,
      "learning_rate": 0.0002522537520801345,
      "loss": 2.1755,
      "step": 44190
    },
    {
      "epoch": 100.45454545454545,
      "grad_norm": 0.8719410300254822,
      "learning_rate": 0.00025216360429460466,
      "loss": 2.1878,
      "step": 44200
    },
    {
      "epoch": 100.47727272727273,
      "grad_norm": 0.23513206839561462,
      "learning_rate": 0.00025207345622772813,
      "loss": 2.1905,
      "step": 44210
    },
    {
      "epoch": 100.5,
      "grad_norm": 0.23050455749034882,
      "learning_rate": 0.00025198330789122744,
      "loss": 2.1885,
      "step": 44220
    },
    {
      "epoch": 100.52272727272727,
      "grad_norm": 0.32419559359550476,
      "learning_rate": 0.000251893159296825,
      "loss": 2.1771,
      "step": 44230
    },
    {
      "epoch": 100.54545454545455,
      "grad_norm": 5.260026454925537,
      "learning_rate": 0.0002518030104562436,
      "loss": 2.1971,
      "step": 44240
    },
    {
      "epoch": 100.56818181818181,
      "grad_norm": 0.3327212929725647,
      "learning_rate": 0.00025171286138120564,
      "loss": 2.1918,
      "step": 44250
    },
    {
      "epoch": 100.5909090909091,
      "grad_norm": 0.21297746896743774,
      "learning_rate": 0.0002516227120834338,
      "loss": 2.1858,
      "step": 44260
    },
    {
      "epoch": 100.61363636363636,
      "grad_norm": 0.2764509618282318,
      "learning_rate": 0.00025153256257465077,
      "loss": 2.1903,
      "step": 44270
    },
    {
      "epoch": 100.63636363636364,
      "grad_norm": 0.22965145111083984,
      "learning_rate": 0.0002514424128665792,
      "loss": 2.1873,
      "step": 44280
    },
    {
      "epoch": 100.6590909090909,
      "grad_norm": 0.3251589834690094,
      "learning_rate": 0.00025135226297094205,
      "loss": 2.1834,
      "step": 44290
    },
    {
      "epoch": 100.68181818181819,
      "grad_norm": 0.2149524688720703,
      "learning_rate": 0.00025126211289946173,
      "loss": 2.1957,
      "step": 44300
    },
    {
      "epoch": 100.70454545454545,
      "grad_norm": 0.2868632376194,
      "learning_rate": 0.0002511719626638611,
      "loss": 2.1915,
      "step": 44310
    },
    {
      "epoch": 100.72727272727273,
      "grad_norm": 0.211747407913208,
      "learning_rate": 0.00025108181227586306,
      "loss": 2.2029,
      "step": 44320
    },
    {
      "epoch": 100.75,
      "grad_norm": 0.30277904868125916,
      "learning_rate": 0.0002509916617471903,
      "loss": 2.191,
      "step": 44330
    },
    {
      "epoch": 100.77272727272727,
      "grad_norm": 0.2113700956106186,
      "learning_rate": 0.0002509015110895658,
      "loss": 2.1849,
      "step": 44340
    },
    {
      "epoch": 100.79545454545455,
      "grad_norm": 0.41076821088790894,
      "learning_rate": 0.0002508113603147122,
      "loss": 2.1846,
      "step": 44350
    },
    {
      "epoch": 100.81818181818181,
      "grad_norm": 0.31730249524116516,
      "learning_rate": 0.0002507212094343525,
      "loss": 2.1859,
      "step": 44360
    },
    {
      "epoch": 100.8409090909091,
      "grad_norm": 0.3773610293865204,
      "learning_rate": 0.0002506310584602095,
      "loss": 2.1836,
      "step": 44370
    },
    {
      "epoch": 100.86363636363636,
      "grad_norm": 0.2292354702949524,
      "learning_rate": 0.00025054090740400604,
      "loss": 2.1878,
      "step": 44380
    },
    {
      "epoch": 100.88636363636364,
      "grad_norm": 0.3892103433609009,
      "learning_rate": 0.0002504507562774652,
      "loss": 2.1811,
      "step": 44390
    },
    {
      "epoch": 100.9090909090909,
      "grad_norm": 0.3180358111858368,
      "learning_rate": 0.0002503606050923097,
      "loss": 2.1869,
      "step": 44400
    },
    {
      "epoch": 100.93181818181819,
      "grad_norm": 0.24273957312107086,
      "learning_rate": 0.00025027045386026255,
      "loss": 2.1815,
      "step": 44410
    },
    {
      "epoch": 100.95454545454545,
      "grad_norm": 0.30452755093574524,
      "learning_rate": 0.00025018030259304656,
      "loss": 2.1989,
      "step": 44420
    },
    {
      "epoch": 100.97727272727273,
      "grad_norm": 0.17067459225654602,
      "learning_rate": 0.0002500901513023847,
      "loss": 2.1799,
      "step": 44430
    },
    {
      "epoch": 101.0,
      "grad_norm": 0.2589721977710724,
      "learning_rate": 0.00025,
      "loss": 2.1844,
      "step": 44440
    },
    {
      "epoch": 101.0,
      "eval_loss": 1.0991308689117432,
      "eval_runtime": 8.7028,
      "eval_samples_per_second": 3496.685,
      "eval_steps_per_second": 13.674,
      "step": 44440
    },
    {
      "epoch": 101.02272727272727,
      "grad_norm": 0.21413639187812805,
      "learning_rate": 0.00024990984869761526,
      "loss": 2.1859,
      "step": 44450
    },
    {
      "epoch": 101.04545454545455,
      "grad_norm": 0.2632911205291748,
      "learning_rate": 0.0002498196974069535,
      "loss": 2.1872,
      "step": 44460
    },
    {
      "epoch": 101.06818181818181,
      "grad_norm": 0.3186028003692627,
      "learning_rate": 0.0002497295461397375,
      "loss": 2.1829,
      "step": 44470
    },
    {
      "epoch": 101.0909090909091,
      "grad_norm": 1.3696917295455933,
      "learning_rate": 0.0002496393949076904,
      "loss": 2.1903,
      "step": 44480
    },
    {
      "epoch": 101.11363636363636,
      "grad_norm": 0.25687259435653687,
      "learning_rate": 0.00024954924372253494,
      "loss": 2.1868,
      "step": 44490
    },
    {
      "epoch": 101.13636363636364,
      "grad_norm": 0.47175315022468567,
      "learning_rate": 0.00024945909259599397,
      "loss": 2.1897,
      "step": 44500
    },
    {
      "epoch": 101.1590909090909,
      "grad_norm": 0.49410584568977356,
      "learning_rate": 0.00024936894153979055,
      "loss": 2.1879,
      "step": 44510
    },
    {
      "epoch": 101.18181818181819,
      "grad_norm": 0.7255082130432129,
      "learning_rate": 0.0002492787905656475,
      "loss": 2.1805,
      "step": 44520
    },
    {
      "epoch": 101.20454545454545,
      "grad_norm": 0.16650845110416412,
      "learning_rate": 0.0002491886396852878,
      "loss": 2.1721,
      "step": 44530
    },
    {
      "epoch": 101.22727272727273,
      "grad_norm": 0.21509473025798798,
      "learning_rate": 0.0002490984889104343,
      "loss": 2.186,
      "step": 44540
    },
    {
      "epoch": 101.25,
      "grad_norm": 0.28160977363586426,
      "learning_rate": 0.00024900833825280966,
      "loss": 2.1784,
      "step": 44550
    },
    {
      "epoch": 101.27272727272727,
      "grad_norm": 0.5984378457069397,
      "learning_rate": 0.000248918187724137,
      "loss": 2.1817,
      "step": 44560
    },
    {
      "epoch": 101.29545454545455,
      "grad_norm": 0.3735748827457428,
      "learning_rate": 0.00024882803733613894,
      "loss": 2.1902,
      "step": 44570
    },
    {
      "epoch": 101.31818181818181,
      "grad_norm": 0.37135306000709534,
      "learning_rate": 0.0002487378871005383,
      "loss": 2.1838,
      "step": 44580
    },
    {
      "epoch": 101.3409090909091,
      "grad_norm": 0.21286746859550476,
      "learning_rate": 0.00024864773702905807,
      "loss": 2.1942,
      "step": 44590
    },
    {
      "epoch": 101.36363636363636,
      "grad_norm": 0.2119608223438263,
      "learning_rate": 0.0002485575871334208,
      "loss": 2.1964,
      "step": 44600
    },
    {
      "epoch": 101.38636363636364,
      "grad_norm": 0.30926260352134705,
      "learning_rate": 0.0002484674374253493,
      "loss": 2.189,
      "step": 44610
    },
    {
      "epoch": 101.4090909090909,
      "grad_norm": 0.3123956322669983,
      "learning_rate": 0.0002483772879165662,
      "loss": 2.1837,
      "step": 44620
    },
    {
      "epoch": 101.43181818181819,
      "grad_norm": 0.38397446274757385,
      "learning_rate": 0.0002482871386187945,
      "loss": 2.186,
      "step": 44630
    },
    {
      "epoch": 101.45454545454545,
      "grad_norm": 0.25038760900497437,
      "learning_rate": 0.00024819698954375646,
      "loss": 2.1865,
      "step": 44640
    },
    {
      "epoch": 101.47727272727273,
      "grad_norm": 0.4058806896209717,
      "learning_rate": 0.000248106840703175,
      "loss": 2.1825,
      "step": 44650
    },
    {
      "epoch": 101.5,
      "grad_norm": 0.587226152420044,
      "learning_rate": 0.00024801669210877263,
      "loss": 2.1818,
      "step": 44660
    },
    {
      "epoch": 101.52272727272727,
      "grad_norm": 0.27727800607681274,
      "learning_rate": 0.0002479265437722719,
      "loss": 2.1812,
      "step": 44670
    },
    {
      "epoch": 101.54545454545455,
      "grad_norm": 0.3428781032562256,
      "learning_rate": 0.00024783639570539535,
      "loss": 2.1909,
      "step": 44680
    },
    {
      "epoch": 101.56818181818181,
      "grad_norm": 0.46665820479393005,
      "learning_rate": 0.0002477462479198656,
      "loss": 2.1951,
      "step": 44690
    },
    {
      "epoch": 101.5909090909091,
      "grad_norm": 0.17948712408542633,
      "learning_rate": 0.0002476561004274049,
      "loss": 2.185,
      "step": 44700
    },
    {
      "epoch": 101.61363636363636,
      "grad_norm": 0.25922921299934387,
      "learning_rate": 0.00024756595323973584,
      "loss": 2.1924,
      "step": 44710
    },
    {
      "epoch": 101.63636363636364,
      "grad_norm": 0.3422915041446686,
      "learning_rate": 0.00024747580636858083,
      "loss": 2.1734,
      "step": 44720
    },
    {
      "epoch": 101.6590909090909,
      "grad_norm": 0.40125805139541626,
      "learning_rate": 0.00024738565982566224,
      "loss": 2.1723,
      "step": 44730
    },
    {
      "epoch": 101.68181818181819,
      "grad_norm": 0.3363686800003052,
      "learning_rate": 0.0002472955136227022,
      "loss": 2.1903,
      "step": 44740
    },
    {
      "epoch": 101.70454545454545,
      "grad_norm": 0.8391842246055603,
      "learning_rate": 0.00024720536777142303,
      "loss": 2.1766,
      "step": 44750
    },
    {
      "epoch": 101.72727272727273,
      "grad_norm": 1.256824016571045,
      "learning_rate": 0.00024711522228354715,
      "loss": 2.1893,
      "step": 44760
    },
    {
      "epoch": 101.75,
      "grad_norm": 0.5667986273765564,
      "learning_rate": 0.00024702507717079645,
      "loss": 2.1855,
      "step": 44770
    },
    {
      "epoch": 101.77272727272727,
      "grad_norm": 0.26169317960739136,
      "learning_rate": 0.00024693493244489325,
      "loss": 2.177,
      "step": 44780
    },
    {
      "epoch": 101.79545454545455,
      "grad_norm": 0.2977888584136963,
      "learning_rate": 0.0002468447881175596,
      "loss": 2.1996,
      "step": 44790
    },
    {
      "epoch": 101.81818181818181,
      "grad_norm": 0.29887133836746216,
      "learning_rate": 0.00024675464420051733,
      "loss": 2.184,
      "step": 44800
    },
    {
      "epoch": 101.8409090909091,
      "grad_norm": 0.2526460587978363,
      "learning_rate": 0.0002466645007054885,
      "loss": 2.1917,
      "step": 44810
    },
    {
      "epoch": 101.86363636363636,
      "grad_norm": 0.3222557306289673,
      "learning_rate": 0.00024657435764419513,
      "loss": 2.1893,
      "step": 44820
    },
    {
      "epoch": 101.88636363636364,
      "grad_norm": 0.2886577844619751,
      "learning_rate": 0.0002464842150283591,
      "loss": 2.1878,
      "step": 44830
    },
    {
      "epoch": 101.9090909090909,
      "grad_norm": 0.4258772134780884,
      "learning_rate": 0.00024639407286970207,
      "loss": 2.181,
      "step": 44840
    },
    {
      "epoch": 101.93181818181819,
      "grad_norm": 0.5250590443611145,
      "learning_rate": 0.0002463039311799457,
      "loss": 2.1694,
      "step": 44850
    },
    {
      "epoch": 101.95454545454545,
      "grad_norm": 0.27790331840515137,
      "learning_rate": 0.000246213789970812,
      "loss": 2.1871,
      "step": 44860
    },
    {
      "epoch": 101.97727272727273,
      "grad_norm": 0.33351802825927734,
      "learning_rate": 0.0002461236492540222,
      "loss": 2.1825,
      "step": 44870
    },
    {
      "epoch": 102.0,
      "grad_norm": 0.3787679076194763,
      "learning_rate": 0.000246033509041298,
      "loss": 2.2014,
      "step": 44880
    },
    {
      "epoch": 102.0,
      "eval_loss": 1.0992501974105835,
      "eval_runtime": 8.6747,
      "eval_samples_per_second": 3508.009,
      "eval_steps_per_second": 13.718,
      "step": 44880
    },
    {
      "epoch": 102.02272727272727,
      "grad_norm": 0.20164471864700317,
      "learning_rate": 0.000245943369344361,
      "loss": 2.1924,
      "step": 44890
    },
    {
      "epoch": 102.04545454545455,
      "grad_norm": 0.2482667714357376,
      "learning_rate": 0.0002458532301749324,
      "loss": 2.1752,
      "step": 44900
    },
    {
      "epoch": 102.06818181818181,
      "grad_norm": 0.7901971936225891,
      "learning_rate": 0.0002457630915447338,
      "loss": 2.1788,
      "step": 44910
    },
    {
      "epoch": 102.0909090909091,
      "grad_norm": 0.2670816481113434,
      "learning_rate": 0.00024567295346548636,
      "loss": 2.1822,
      "step": 44920
    },
    {
      "epoch": 102.11363636363636,
      "grad_norm": 0.42629000544548035,
      "learning_rate": 0.00024558281594891113,
      "loss": 2.1808,
      "step": 44930
    },
    {
      "epoch": 102.13636363636364,
      "grad_norm": 0.2619529366493225,
      "learning_rate": 0.0002454926790067294,
      "loss": 2.1771,
      "step": 44940
    },
    {
      "epoch": 102.1590909090909,
      "grad_norm": 0.3636339008808136,
      "learning_rate": 0.0002454025426506622,
      "loss": 2.1885,
      "step": 44950
    },
    {
      "epoch": 102.18181818181819,
      "grad_norm": 0.2530630826950073,
      "learning_rate": 0.00024531240689243064,
      "loss": 2.1882,
      "step": 44960
    },
    {
      "epoch": 102.20454545454545,
      "grad_norm": 0.27920565009117126,
      "learning_rate": 0.00024522227174375545,
      "loss": 2.1785,
      "step": 44970
    },
    {
      "epoch": 102.22727272727273,
      "grad_norm": 0.2264024168252945,
      "learning_rate": 0.00024513213721635743,
      "loss": 2.1903,
      "step": 44980
    },
    {
      "epoch": 102.25,
      "grad_norm": 0.28816044330596924,
      "learning_rate": 0.00024504200332195757,
      "loss": 2.1952,
      "step": 44990
    },
    {
      "epoch": 102.27272727272727,
      "grad_norm": 0.3813283443450928,
      "learning_rate": 0.0002449518700722762,
      "loss": 2.1832,
      "step": 45000
    },
    {
      "epoch": 102.29545454545455,
      "grad_norm": 0.994256854057312,
      "learning_rate": 0.00024486173747903414,
      "loss": 2.1819,
      "step": 45010
    },
    {
      "epoch": 102.31818181818181,
      "grad_norm": 0.21576176583766937,
      "learning_rate": 0.00024477160555395184,
      "loss": 2.181,
      "step": 45020
    },
    {
      "epoch": 102.3409090909091,
      "grad_norm": 0.1993779093027115,
      "learning_rate": 0.0002446814743087497,
      "loss": 2.1815,
      "step": 45030
    },
    {
      "epoch": 102.36363636363636,
      "grad_norm": 0.24927709996700287,
      "learning_rate": 0.0002445913437551479,
      "loss": 2.1911,
      "step": 45040
    },
    {
      "epoch": 102.38636363636364,
      "grad_norm": 0.23851357400417328,
      "learning_rate": 0.00024450121390486674,
      "loss": 2.1841,
      "step": 45050
    },
    {
      "epoch": 102.4090909090909,
      "grad_norm": 0.7255639433860779,
      "learning_rate": 0.0002444110847696266,
      "loss": 2.1942,
      "step": 45060
    },
    {
      "epoch": 102.43181818181819,
      "grad_norm": 0.2193390429019928,
      "learning_rate": 0.00024432095636114714,
      "loss": 2.1793,
      "step": 45070
    },
    {
      "epoch": 102.45454545454545,
      "grad_norm": 0.24355483055114746,
      "learning_rate": 0.0002442308286911486,
      "loss": 2.1818,
      "step": 45080
    },
    {
      "epoch": 102.47727272727273,
      "grad_norm": 0.208314448595047,
      "learning_rate": 0.00024414070177135067,
      "loss": 2.176,
      "step": 45090
    },
    {
      "epoch": 102.5,
      "grad_norm": 0.38656386733055115,
      "learning_rate": 0.00024405057561347317,
      "loss": 2.189,
      "step": 45100
    },
    {
      "epoch": 102.52272727272727,
      "grad_norm": 0.4361273944377899,
      "learning_rate": 0.0002439604502292357,
      "loss": 2.1768,
      "step": 45110
    },
    {
      "epoch": 102.54545454545455,
      "grad_norm": 0.20379962027072906,
      "learning_rate": 0.00024387032563035795,
      "loss": 2.183,
      "step": 45120
    },
    {
      "epoch": 102.56818181818181,
      "grad_norm": 0.24652834236621857,
      "learning_rate": 0.00024378020182855918,
      "loss": 2.1923,
      "step": 45130
    },
    {
      "epoch": 102.5909090909091,
      "grad_norm": 0.2584649920463562,
      "learning_rate": 0.00024369007883555881,
      "loss": 2.1871,
      "step": 45140
    },
    {
      "epoch": 102.61363636363636,
      "grad_norm": 0.20147138833999634,
      "learning_rate": 0.00024359995666307623,
      "loss": 2.1857,
      "step": 45150
    },
    {
      "epoch": 102.63636363636364,
      "grad_norm": 0.217158704996109,
      "learning_rate": 0.00024350983532283048,
      "loss": 2.1862,
      "step": 45160
    },
    {
      "epoch": 102.6590909090909,
      "grad_norm": 0.30878692865371704,
      "learning_rate": 0.00024341971482654047,
      "loss": 2.1795,
      "step": 45170
    },
    {
      "epoch": 102.68181818181819,
      "grad_norm": 0.3389282524585724,
      "learning_rate": 0.0002433295951859252,
      "loss": 2.1849,
      "step": 45180
    },
    {
      "epoch": 102.70454545454545,
      "grad_norm": 0.2272067815065384,
      "learning_rate": 0.00024323947641270363,
      "loss": 2.1948,
      "step": 45190
    },
    {
      "epoch": 102.72727272727273,
      "grad_norm": 0.3129637837409973,
      "learning_rate": 0.00024314935851859422,
      "loss": 2.1795,
      "step": 45200
    },
    {
      "epoch": 102.75,
      "grad_norm": 0.2771448791027069,
      "learning_rate": 0.0002430592415153157,
      "loss": 2.179,
      "step": 45210
    },
    {
      "epoch": 102.77272727272727,
      "grad_norm": 0.5481222867965698,
      "learning_rate": 0.0002429691254145865,
      "loss": 2.1903,
      "step": 45220
    },
    {
      "epoch": 102.79545454545455,
      "grad_norm": 0.2544766962528229,
      "learning_rate": 0.00024287901022812498,
      "loss": 2.1868,
      "step": 45230
    },
    {
      "epoch": 102.81818181818181,
      "grad_norm": 0.21538767218589783,
      "learning_rate": 0.00024278889596764924,
      "loss": 2.1862,
      "step": 45240
    },
    {
      "epoch": 102.8409090909091,
      "grad_norm": 0.2503573000431061,
      "learning_rate": 0.00024269878264487755,
      "loss": 2.186,
      "step": 45250
    },
    {
      "epoch": 102.86363636363636,
      "grad_norm": 0.24035844206809998,
      "learning_rate": 0.00024260867027152793,
      "loss": 2.18,
      "step": 45260
    },
    {
      "epoch": 102.88636363636364,
      "grad_norm": 0.42833125591278076,
      "learning_rate": 0.00024251855885931811,
      "loss": 2.1845,
      "step": 45270
    },
    {
      "epoch": 102.9090909090909,
      "grad_norm": 0.35695934295654297,
      "learning_rate": 0.00024242844841996584,
      "loss": 2.1862,
      "step": 45280
    },
    {
      "epoch": 102.93181818181819,
      "grad_norm": 0.18719275295734406,
      "learning_rate": 0.00024233833896518885,
      "loss": 2.1956,
      "step": 45290
    },
    {
      "epoch": 102.95454545454545,
      "grad_norm": 0.3623400628566742,
      "learning_rate": 0.00024224823050670447,
      "loss": 2.1805,
      "step": 45300
    },
    {
      "epoch": 102.97727272727273,
      "grad_norm": 0.27670374512672424,
      "learning_rate": 0.0002421581230562301,
      "loss": 2.1795,
      "step": 45310
    },
    {
      "epoch": 103.0,
      "grad_norm": 0.5245306491851807,
      "learning_rate": 0.00024206801662548315,
      "loss": 2.1854,
      "step": 45320
    },
    {
      "epoch": 103.0,
      "eval_loss": 1.099071979522705,
      "eval_runtime": 8.7112,
      "eval_samples_per_second": 3493.304,
      "eval_steps_per_second": 13.661,
      "step": 45320
    },
    {
      "epoch": 103.02272727272727,
      "grad_norm": 0.1764243245124817,
      "learning_rate": 0.0002419779112261804,
      "loss": 2.1839,
      "step": 45330
    },
    {
      "epoch": 103.04545454545455,
      "grad_norm": 0.2194923311471939,
      "learning_rate": 0.000241887806870039,
      "loss": 2.1832,
      "step": 45340
    },
    {
      "epoch": 103.06818181818181,
      "grad_norm": 0.4181760549545288,
      "learning_rate": 0.0002417977035687757,
      "loss": 2.1748,
      "step": 45350
    },
    {
      "epoch": 103.0909090909091,
      "grad_norm": 0.21159186959266663,
      "learning_rate": 0.00024170760133410731,
      "loss": 2.1821,
      "step": 45360
    },
    {
      "epoch": 103.11363636363636,
      "grad_norm": 0.6173216104507446,
      "learning_rate": 0.0002416175001777502,
      "loss": 2.1759,
      "step": 45370
    },
    {
      "epoch": 103.13636363636364,
      "grad_norm": 0.24524593353271484,
      "learning_rate": 0.00024152740011142084,
      "loss": 2.182,
      "step": 45380
    },
    {
      "epoch": 103.1590909090909,
      "grad_norm": 0.22968333959579468,
      "learning_rate": 0.0002414373011468356,
      "loss": 2.1858,
      "step": 45390
    },
    {
      "epoch": 103.18181818181819,
      "grad_norm": 0.333451509475708,
      "learning_rate": 0.00024134720329571047,
      "loss": 2.184,
      "step": 45400
    },
    {
      "epoch": 103.20454545454545,
      "grad_norm": 0.25549590587615967,
      "learning_rate": 0.0002412571065697614,
      "loss": 2.1821,
      "step": 45410
    },
    {
      "epoch": 103.22727272727273,
      "grad_norm": 0.43197962641716003,
      "learning_rate": 0.00024116701098070439,
      "loss": 2.1771,
      "step": 45420
    },
    {
      "epoch": 103.25,
      "grad_norm": 0.48053067922592163,
      "learning_rate": 0.0002410769165402549,
      "loss": 2.1837,
      "step": 45430
    },
    {
      "epoch": 103.27272727272727,
      "grad_norm": 0.35095903277397156,
      "learning_rate": 0.00024098682326012858,
      "loss": 2.1737,
      "step": 45440
    },
    {
      "epoch": 103.29545454545455,
      "grad_norm": 0.2102651298046112,
      "learning_rate": 0.00024089673115204092,
      "loss": 2.1909,
      "step": 45450
    },
    {
      "epoch": 103.31818181818181,
      "grad_norm": 0.17416523396968842,
      "learning_rate": 0.00024080664022770693,
      "loss": 2.1805,
      "step": 45460
    },
    {
      "epoch": 103.3409090909091,
      "grad_norm": 0.15894517302513123,
      "learning_rate": 0.0002407165504988418,
      "loss": 2.1931,
      "step": 45470
    },
    {
      "epoch": 103.36363636363636,
      "grad_norm": 0.2719005346298218,
      "learning_rate": 0.00024062646197716043,
      "loss": 2.1807,
      "step": 45480
    },
    {
      "epoch": 103.38636363636364,
      "grad_norm": 0.1942935436964035,
      "learning_rate": 0.00024053637467437767,
      "loss": 2.2024,
      "step": 45490
    },
    {
      "epoch": 103.4090909090909,
      "grad_norm": 0.19709660112857819,
      "learning_rate": 0.00024044628860220792,
      "loss": 2.1834,
      "step": 45500
    },
    {
      "epoch": 103.43181818181819,
      "grad_norm": 0.2892434895038605,
      "learning_rate": 0.00024035620377236584,
      "loss": 2.1903,
      "step": 45510
    },
    {
      "epoch": 103.45454545454545,
      "grad_norm": 0.25108101963996887,
      "learning_rate": 0.0002402661201965656,
      "loss": 2.1839,
      "step": 45520
    },
    {
      "epoch": 103.47727272727273,
      "grad_norm": 0.21404318511486053,
      "learning_rate": 0.00024017603788652135,
      "loss": 2.1829,
      "step": 45530
    },
    {
      "epoch": 103.5,
      "grad_norm": 0.35310447216033936,
      "learning_rate": 0.00024008595685394692,
      "loss": 2.1751,
      "step": 45540
    },
    {
      "epoch": 103.52272727272727,
      "grad_norm": 0.3229522705078125,
      "learning_rate": 0.00023999587711055637,
      "loss": 2.1879,
      "step": 45550
    },
    {
      "epoch": 103.54545454545455,
      "grad_norm": 0.17094239592552185,
      "learning_rate": 0.00023990579866806303,
      "loss": 2.187,
      "step": 45560
    },
    {
      "epoch": 103.56818181818181,
      "grad_norm": 0.394325852394104,
      "learning_rate": 0.0002398157215381805,
      "loss": 2.1813,
      "step": 45570
    },
    {
      "epoch": 103.5909090909091,
      "grad_norm": 0.3186485767364502,
      "learning_rate": 0.00023972564573262206,
      "loss": 2.1856,
      "step": 45580
    },
    {
      "epoch": 103.61363636363636,
      "grad_norm": 0.2497284859418869,
      "learning_rate": 0.00023963557126310088,
      "loss": 2.1903,
      "step": 45590
    },
    {
      "epoch": 103.63636363636364,
      "grad_norm": 0.2802177369594574,
      "learning_rate": 0.0002395454981413297,
      "loss": 2.1914,
      "step": 45600
    },
    {
      "epoch": 103.6590909090909,
      "grad_norm": 0.29955893754959106,
      "learning_rate": 0.00023945542637902138,
      "loss": 2.1869,
      "step": 45610
    },
    {
      "epoch": 103.68181818181819,
      "grad_norm": 0.22548694908618927,
      "learning_rate": 0.00023936535598788862,
      "loss": 2.1872,
      "step": 45620
    },
    {
      "epoch": 103.70454545454545,
      "grad_norm": 0.2506689131259918,
      "learning_rate": 0.00023927528697964364,
      "loss": 2.1718,
      "step": 45630
    },
    {
      "epoch": 103.72727272727273,
      "grad_norm": 0.25067710876464844,
      "learning_rate": 0.00023918521936599878,
      "loss": 2.1916,
      "step": 45640
    },
    {
      "epoch": 103.75,
      "grad_norm": 0.17276731133460999,
      "learning_rate": 0.00023909515315866605,
      "loss": 2.187,
      "step": 45650
    },
    {
      "epoch": 103.77272727272727,
      "grad_norm": 0.19500252604484558,
      "learning_rate": 0.00023900508836935728,
      "loss": 2.1949,
      "step": 45660
    },
    {
      "epoch": 103.79545454545455,
      "grad_norm": 0.2789522409439087,
      "learning_rate": 0.00023891502500978415,
      "loss": 2.1946,
      "step": 45670
    },
    {
      "epoch": 103.81818181818181,
      "grad_norm": 0.20812305808067322,
      "learning_rate": 0.00023882496309165816,
      "loss": 2.1842,
      "step": 45680
    },
    {
      "epoch": 103.8409090909091,
      "grad_norm": 0.24859239161014557,
      "learning_rate": 0.00023873490262669075,
      "loss": 2.1855,
      "step": 45690
    },
    {
      "epoch": 103.86363636363636,
      "grad_norm": 0.20722132921218872,
      "learning_rate": 0.00023864484362659283,
      "loss": 2.1787,
      "step": 45700
    },
    {
      "epoch": 103.88636363636364,
      "grad_norm": 0.46936845779418945,
      "learning_rate": 0.00023855478610307538,
      "loss": 2.1862,
      "step": 45710
    },
    {
      "epoch": 103.9090909090909,
      "grad_norm": 0.28688400983810425,
      "learning_rate": 0.00023846473006784925,
      "loss": 2.1693,
      "step": 45720
    },
    {
      "epoch": 103.93181818181819,
      "grad_norm": 0.19353392720222473,
      "learning_rate": 0.00023837467553262476,
      "loss": 2.1856,
      "step": 45730
    },
    {
      "epoch": 103.95454545454545,
      "grad_norm": 0.3498837947845459,
      "learning_rate": 0.00023828462250911238,
      "loss": 2.1907,
      "step": 45740
    },
    {
      "epoch": 103.97727272727273,
      "grad_norm": 0.21599066257476807,
      "learning_rate": 0.00023819457100902238,
      "loss": 2.1854,
      "step": 45750
    },
    {
      "epoch": 104.0,
      "grad_norm": 0.3627144992351532,
      "learning_rate": 0.00023810452104406444,
      "loss": 2.1854,
      "step": 45760
    },
    {
      "epoch": 104.0,
      "eval_loss": 1.0993878841400146,
      "eval_runtime": 8.7577,
      "eval_samples_per_second": 3474.775,
      "eval_steps_per_second": 13.588,
      "step": 45760
    },
    {
      "epoch": 104.02272727272727,
      "grad_norm": 0.5133004784584045,
      "learning_rate": 0.00023801447262594848,
      "loss": 2.182,
      "step": 45770
    },
    {
      "epoch": 104.04545454545455,
      "grad_norm": 0.45308083295822144,
      "learning_rate": 0.00023792442576638402,
      "loss": 2.1794,
      "step": 45780
    },
    {
      "epoch": 104.06818181818181,
      "grad_norm": 0.2376398891210556,
      "learning_rate": 0.00023783438047708045,
      "loss": 2.1813,
      "step": 45790
    },
    {
      "epoch": 104.0909090909091,
      "grad_norm": 0.3045622706413269,
      "learning_rate": 0.00023774433676974677,
      "loss": 2.1917,
      "step": 45800
    },
    {
      "epoch": 104.11363636363636,
      "grad_norm": 0.23886893689632416,
      "learning_rate": 0.00023765429465609202,
      "loss": 2.1806,
      "step": 45810
    },
    {
      "epoch": 104.13636363636364,
      "grad_norm": 0.16759784519672394,
      "learning_rate": 0.000237564254147825,
      "loss": 2.1768,
      "step": 45820
    },
    {
      "epoch": 104.1590909090909,
      "grad_norm": 0.301518052816391,
      "learning_rate": 0.00023747421525665408,
      "loss": 2.1774,
      "step": 45830
    },
    {
      "epoch": 104.18181818181819,
      "grad_norm": 0.22784067690372467,
      "learning_rate": 0.00023738417799428757,
      "loss": 2.1853,
      "step": 45840
    },
    {
      "epoch": 104.20454545454545,
      "grad_norm": 0.41947853565216064,
      "learning_rate": 0.0002372941423724337,
      "loss": 2.1726,
      "step": 45850
    },
    {
      "epoch": 104.22727272727273,
      "grad_norm": 0.23901572823524475,
      "learning_rate": 0.00023720410840280022,
      "loss": 2.1822,
      "step": 45860
    },
    {
      "epoch": 104.25,
      "grad_norm": 0.15075622498989105,
      "learning_rate": 0.00023711407609709485,
      "loss": 2.1868,
      "step": 45870
    },
    {
      "epoch": 104.27272727272727,
      "grad_norm": 0.3363219201564789,
      "learning_rate": 0.0002370240454670251,
      "loss": 2.1663,
      "step": 45880
    },
    {
      "epoch": 104.29545454545455,
      "grad_norm": 0.21301832795143127,
      "learning_rate": 0.00023693401652429808,
      "loss": 2.1764,
      "step": 45890
    },
    {
      "epoch": 104.31818181818181,
      "grad_norm": 0.37158721685409546,
      "learning_rate": 0.00023684398928062092,
      "loss": 2.1884,
      "step": 45900
    },
    {
      "epoch": 104.3409090909091,
      "grad_norm": 0.23128573596477509,
      "learning_rate": 0.00023675396374770033,
      "loss": 2.1931,
      "step": 45910
    },
    {
      "epoch": 104.36363636363636,
      "grad_norm": 0.20432397723197937,
      "learning_rate": 0.000236663939937243,
      "loss": 2.171,
      "step": 45920
    },
    {
      "epoch": 104.38636363636364,
      "grad_norm": 0.2958172559738159,
      "learning_rate": 0.00023657391786095511,
      "loss": 2.1941,
      "step": 45930
    },
    {
      "epoch": 104.4090909090909,
      "grad_norm": 0.2922566533088684,
      "learning_rate": 0.0002364838975305429,
      "loss": 2.1881,
      "step": 45940
    },
    {
      "epoch": 104.43181818181819,
      "grad_norm": 0.2587171792984009,
      "learning_rate": 0.00023639387895771225,
      "loss": 2.1854,
      "step": 45950
    },
    {
      "epoch": 104.45454545454545,
      "grad_norm": 0.2776532471179962,
      "learning_rate": 0.0002363038621541688,
      "loss": 2.1819,
      "step": 45960
    },
    {
      "epoch": 104.47727272727273,
      "grad_norm": 0.43359020352363586,
      "learning_rate": 0.00023621384713161793,
      "loss": 2.1903,
      "step": 45970
    },
    {
      "epoch": 104.5,
      "grad_norm": 0.3057427704334259,
      "learning_rate": 0.000236123833901765,
      "loss": 2.184,
      "step": 45980
    },
    {
      "epoch": 104.52272727272727,
      "grad_norm": 0.47807401418685913,
      "learning_rate": 0.00023603382247631483,
      "loss": 2.1949,
      "step": 45990
    },
    {
      "epoch": 104.54545454545455,
      "grad_norm": 0.1660042405128479,
      "learning_rate": 0.00023594381286697218,
      "loss": 2.1843,
      "step": 46000
    },
    {
      "epoch": 104.56818181818181,
      "grad_norm": 0.30802568793296814,
      "learning_rate": 0.00023585380508544162,
      "loss": 2.1859,
      "step": 46010
    },
    {
      "epoch": 104.5909090909091,
      "grad_norm": 0.17561399936676025,
      "learning_rate": 0.00023576379914342748,
      "loss": 2.194,
      "step": 46020
    },
    {
      "epoch": 104.61363636363636,
      "grad_norm": 0.4136126935482025,
      "learning_rate": 0.00023567379505263356,
      "loss": 2.1859,
      "step": 46030
    },
    {
      "epoch": 104.63636363636364,
      "grad_norm": 0.3356800973415375,
      "learning_rate": 0.00023558379282476375,
      "loss": 2.1906,
      "step": 46040
    },
    {
      "epoch": 104.6590909090909,
      "grad_norm": 0.4650121331214905,
      "learning_rate": 0.00023549379247152173,
      "loss": 2.1786,
      "step": 46050
    },
    {
      "epoch": 104.68181818181819,
      "grad_norm": 0.2798353433609009,
      "learning_rate": 0.00023540379400461056,
      "loss": 2.1817,
      "step": 46060
    },
    {
      "epoch": 104.70454545454545,
      "grad_norm": 0.31205320358276367,
      "learning_rate": 0.00023531379743573346,
      "loss": 2.1744,
      "step": 46070
    },
    {
      "epoch": 104.72727272727273,
      "grad_norm": 1.8856221437454224,
      "learning_rate": 0.00023522380277659317,
      "loss": 2.1828,
      "step": 46080
    },
    {
      "epoch": 104.75,
      "grad_norm": 0.36880216002464294,
      "learning_rate": 0.00023513381003889227,
      "loss": 2.185,
      "step": 46090
    },
    {
      "epoch": 104.77272727272727,
      "grad_norm": 0.2416590303182602,
      "learning_rate": 0.00023504381923433298,
      "loss": 2.1737,
      "step": 46100
    },
    {
      "epoch": 104.79545454545455,
      "grad_norm": 0.4148370325565338,
      "learning_rate": 0.00023495383037461747,
      "loss": 2.1719,
      "step": 46110
    },
    {
      "epoch": 104.81818181818181,
      "grad_norm": 0.15989038348197937,
      "learning_rate": 0.00023486384347144753,
      "loss": 2.1763,
      "step": 46120
    },
    {
      "epoch": 104.8409090909091,
      "grad_norm": 0.44524896144866943,
      "learning_rate": 0.00023477385853652464,
      "loss": 2.1856,
      "step": 46130
    },
    {
      "epoch": 104.86363636363636,
      "grad_norm": 0.31832602620124817,
      "learning_rate": 0.00023468387558155015,
      "loss": 2.1824,
      "step": 46140
    },
    {
      "epoch": 104.88636363636364,
      "grad_norm": 0.18707984685897827,
      "learning_rate": 0.00023459389461822513,
      "loss": 2.1796,
      "step": 46150
    },
    {
      "epoch": 104.9090909090909,
      "grad_norm": 0.17676608264446259,
      "learning_rate": 0.0002345039156582502,
      "loss": 2.1947,
      "step": 46160
    },
    {
      "epoch": 104.93181818181819,
      "grad_norm": 0.24303175508975983,
      "learning_rate": 0.00023441393871332598,
      "loss": 2.1866,
      "step": 46170
    },
    {
      "epoch": 104.95454545454545,
      "grad_norm": 0.16271747648715973,
      "learning_rate": 0.0002343239637951528,
      "loss": 2.1868,
      "step": 46180
    },
    {
      "epoch": 104.97727272727273,
      "grad_norm": 0.276767373085022,
      "learning_rate": 0.0002342339909154305,
      "loss": 2.1916,
      "step": 46190
    },
    {
      "epoch": 105.0,
      "grad_norm": 2.821603298187256,
      "learning_rate": 0.00023414402008585888,
      "loss": 2.1906,
      "step": 46200
    },
    {
      "epoch": 105.0,
      "eval_loss": 1.0993545055389404,
      "eval_runtime": 8.9121,
      "eval_samples_per_second": 3414.562,
      "eval_steps_per_second": 13.353,
      "step": 46200
    },
    {
      "epoch": 105.02272727272727,
      "grad_norm": 0.22626261413097382,
      "learning_rate": 0.00023405405131813738,
      "loss": 2.1951,
      "step": 46210
    },
    {
      "epoch": 105.04545454545455,
      "grad_norm": 0.21442073583602905,
      "learning_rate": 0.00023396408462396525,
      "loss": 2.1824,
      "step": 46220
    },
    {
      "epoch": 105.06818181818181,
      "grad_norm": 0.40453970432281494,
      "learning_rate": 0.0002338741200150413,
      "loss": 2.1915,
      "step": 46230
    },
    {
      "epoch": 105.0909090909091,
      "grad_norm": 1.0944756269454956,
      "learning_rate": 0.00023378415750306423,
      "loss": 2.1764,
      "step": 46240
    },
    {
      "epoch": 105.11363636363636,
      "grad_norm": 0.20710761845111847,
      "learning_rate": 0.00023369419709973253,
      "loss": 2.1811,
      "step": 46250
    },
    {
      "epoch": 105.13636363636364,
      "grad_norm": 0.2714232802391052,
      "learning_rate": 0.00023360423881674413,
      "loss": 2.1805,
      "step": 46260
    },
    {
      "epoch": 105.1590909090909,
      "grad_norm": 0.25502336025238037,
      "learning_rate": 0.00023351428266579684,
      "loss": 2.1735,
      "step": 46270
    },
    {
      "epoch": 105.18181818181819,
      "grad_norm": 0.19410251080989838,
      "learning_rate": 0.00023342432865858842,
      "loss": 2.184,
      "step": 46280
    },
    {
      "epoch": 105.20454545454545,
      "grad_norm": 3.6394991874694824,
      "learning_rate": 0.00023333437680681595,
      "loss": 2.1785,
      "step": 46290
    },
    {
      "epoch": 105.22727272727273,
      "grad_norm": 0.3550449311733246,
      "learning_rate": 0.0002332444271221764,
      "loss": 2.1895,
      "step": 46300
    },
    {
      "epoch": 105.25,
      "grad_norm": 0.25445160269737244,
      "learning_rate": 0.00023315447961636665,
      "loss": 2.178,
      "step": 46310
    },
    {
      "epoch": 105.27272727272727,
      "grad_norm": 0.21736545860767365,
      "learning_rate": 0.00023306453430108304,
      "loss": 2.183,
      "step": 46320
    },
    {
      "epoch": 105.29545454545455,
      "grad_norm": 0.21985909342765808,
      "learning_rate": 0.0002329745911880217,
      "loss": 2.1859,
      "step": 46330
    },
    {
      "epoch": 105.31818181818181,
      "grad_norm": 0.8444480895996094,
      "learning_rate": 0.00023288465028887845,
      "loss": 2.1837,
      "step": 46340
    },
    {
      "epoch": 105.3409090909091,
      "grad_norm": 0.24703821539878845,
      "learning_rate": 0.00023279471161534898,
      "loss": 2.1798,
      "step": 46350
    },
    {
      "epoch": 105.36363636363636,
      "grad_norm": 0.1785723865032196,
      "learning_rate": 0.00023270477517912835,
      "loss": 2.1773,
      "step": 46360
    },
    {
      "epoch": 105.38636363636364,
      "grad_norm": 0.270088791847229,
      "learning_rate": 0.00023261484099191176,
      "loss": 2.1913,
      "step": 46370
    },
    {
      "epoch": 105.4090909090909,
      "grad_norm": 0.6896940469741821,
      "learning_rate": 0.0002325249090653938,
      "loss": 2.186,
      "step": 46380
    },
    {
      "epoch": 105.43181818181819,
      "grad_norm": 0.25218647718429565,
      "learning_rate": 0.0002324349794112689,
      "loss": 2.1835,
      "step": 46390
    },
    {
      "epoch": 105.45454545454545,
      "grad_norm": 0.502269446849823,
      "learning_rate": 0.00023234505204123106,
      "loss": 2.1748,
      "step": 46400
    },
    {
      "epoch": 105.47727272727273,
      "grad_norm": 0.24503132700920105,
      "learning_rate": 0.00023225512696697435,
      "loss": 2.1829,
      "step": 46410
    },
    {
      "epoch": 105.5,
      "grad_norm": 0.1937776654958725,
      "learning_rate": 0.00023216520420019194,
      "loss": 2.1871,
      "step": 46420
    },
    {
      "epoch": 105.52272727272727,
      "grad_norm": 0.3028523325920105,
      "learning_rate": 0.00023207528375257722,
      "loss": 2.1755,
      "step": 46430
    },
    {
      "epoch": 105.54545454545455,
      "grad_norm": 0.21312201023101807,
      "learning_rate": 0.00023198536563582311,
      "loss": 2.1894,
      "step": 46440
    },
    {
      "epoch": 105.56818181818181,
      "grad_norm": 0.41117098927497864,
      "learning_rate": 0.0002318954498616223,
      "loss": 2.1834,
      "step": 46450
    },
    {
      "epoch": 105.5909090909091,
      "grad_norm": 0.26195013523101807,
      "learning_rate": 0.0002318055364416668,
      "loss": 2.1809,
      "step": 46460
    },
    {
      "epoch": 105.61363636363636,
      "grad_norm": 0.7463908195495605,
      "learning_rate": 0.0002317156253876488,
      "loss": 2.1854,
      "step": 46470
    },
    {
      "epoch": 105.63636363636364,
      "grad_norm": 0.6382191777229309,
      "learning_rate": 0.00023162571671126003,
      "loss": 2.1773,
      "step": 46480
    },
    {
      "epoch": 105.6590909090909,
      "grad_norm": 0.4603177011013031,
      "learning_rate": 0.0002315358104241917,
      "loss": 2.1814,
      "step": 46490
    },
    {
      "epoch": 105.68181818181819,
      "grad_norm": 0.17827318608760834,
      "learning_rate": 0.00023144590653813502,
      "loss": 2.1749,
      "step": 46500
    },
    {
      "epoch": 105.70454545454545,
      "grad_norm": 0.20812374353408813,
      "learning_rate": 0.0002313560050647807,
      "loss": 2.1709,
      "step": 46510
    },
    {
      "epoch": 105.72727272727273,
      "grad_norm": 0.40961769223213196,
      "learning_rate": 0.00023126610601581916,
      "loss": 2.1768,
      "step": 46520
    },
    {
      "epoch": 105.75,
      "grad_norm": 0.19956867396831512,
      "learning_rate": 0.00023117620940294047,
      "loss": 2.1952,
      "step": 46530
    },
    {
      "epoch": 105.77272727272727,
      "grad_norm": 0.3310610353946686,
      "learning_rate": 0.0002310863152378345,
      "loss": 2.184,
      "step": 46540
    },
    {
      "epoch": 105.79545454545455,
      "grad_norm": 0.19315189123153687,
      "learning_rate": 0.0002309964235321909,
      "loss": 2.1827,
      "step": 46550
    },
    {
      "epoch": 105.81818181818181,
      "grad_norm": 0.25024253129959106,
      "learning_rate": 0.0002309065342976985,
      "loss": 2.1805,
      "step": 46560
    },
    {
      "epoch": 105.8409090909091,
      "grad_norm": 0.743720531463623,
      "learning_rate": 0.00023081664754604641,
      "loss": 2.1866,
      "step": 46570
    },
    {
      "epoch": 105.86363636363636,
      "grad_norm": 0.2587364912033081,
      "learning_rate": 0.00023072676328892311,
      "loss": 2.1807,
      "step": 46580
    },
    {
      "epoch": 105.88636363636364,
      "grad_norm": 0.53473299741745,
      "learning_rate": 0.00023063688153801665,
      "loss": 2.1893,
      "step": 46590
    },
    {
      "epoch": 105.9090909090909,
      "grad_norm": 0.38463783264160156,
      "learning_rate": 0.000230547002305015,
      "loss": 2.1847,
      "step": 46600
    },
    {
      "epoch": 105.93181818181819,
      "grad_norm": 0.20005372166633606,
      "learning_rate": 0.00023045712560160588,
      "loss": 2.1822,
      "step": 46610
    },
    {
      "epoch": 105.95454545454545,
      "grad_norm": 0.25439971685409546,
      "learning_rate": 0.00023036725143947619,
      "loss": 2.1793,
      "step": 46620
    },
    {
      "epoch": 105.97727272727273,
      "grad_norm": 0.2240656316280365,
      "learning_rate": 0.00023027737983031304,
      "loss": 2.1907,
      "step": 46630
    },
    {
      "epoch": 106.0,
      "grad_norm": 0.7224458456039429,
      "learning_rate": 0.00023018751078580287,
      "loss": 2.1914,
      "step": 46640
    },
    {
      "epoch": 106.0,
      "eval_loss": 1.0992136001586914,
      "eval_runtime": 8.72,
      "eval_samples_per_second": 3489.81,
      "eval_steps_per_second": 13.647,
      "step": 46640
    },
    {
      "epoch": 106.02272727272727,
      "grad_norm": 0.1976660043001175,
      "learning_rate": 0.00023009764431763206,
      "loss": 2.1897,
      "step": 46650
    },
    {
      "epoch": 106.04545454545455,
      "grad_norm": 0.19367292523384094,
      "learning_rate": 0.00023000778043748627,
      "loss": 2.1961,
      "step": 46660
    },
    {
      "epoch": 106.06818181818181,
      "grad_norm": 0.2768658399581909,
      "learning_rate": 0.00022991791915705117,
      "loss": 2.1838,
      "step": 46670
    },
    {
      "epoch": 106.0909090909091,
      "grad_norm": 0.18104684352874756,
      "learning_rate": 0.0002298280604880121,
      "loss": 2.1779,
      "step": 46680
    },
    {
      "epoch": 106.11363636363636,
      "grad_norm": 0.25983726978302,
      "learning_rate": 0.00022973820444205373,
      "loss": 2.1831,
      "step": 46690
    },
    {
      "epoch": 106.13636363636364,
      "grad_norm": 0.2338157743215561,
      "learning_rate": 0.00022964835103086062,
      "loss": 2.1665,
      "step": 46700
    },
    {
      "epoch": 106.1590909090909,
      "grad_norm": 0.3160601258277893,
      "learning_rate": 0.00022955850026611712,
      "loss": 2.187,
      "step": 46710
    },
    {
      "epoch": 106.18181818181819,
      "grad_norm": 0.26973822712898254,
      "learning_rate": 0.00022946865215950685,
      "loss": 2.1811,
      "step": 46720
    },
    {
      "epoch": 106.20454545454545,
      "grad_norm": 0.35929006338119507,
      "learning_rate": 0.00022937880672271342,
      "loss": 2.1906,
      "step": 46730
    },
    {
      "epoch": 106.22727272727273,
      "grad_norm": 0.21117791533470154,
      "learning_rate": 0.00022928896396742004,
      "loss": 2.1755,
      "step": 46740
    },
    {
      "epoch": 106.25,
      "grad_norm": 0.274526983499527,
      "learning_rate": 0.00022919912390530946,
      "loss": 2.1773,
      "step": 46750
    },
    {
      "epoch": 106.27272727272727,
      "grad_norm": 0.303524374961853,
      "learning_rate": 0.0002291092865480641,
      "loss": 2.1825,
      "step": 46760
    },
    {
      "epoch": 106.29545454545455,
      "grad_norm": 0.28312990069389343,
      "learning_rate": 0.00022901945190736606,
      "loss": 2.1771,
      "step": 46770
    },
    {
      "epoch": 106.31818181818181,
      "grad_norm": 0.3861132264137268,
      "learning_rate": 0.00022892961999489722,
      "loss": 2.1996,
      "step": 46780
    },
    {
      "epoch": 106.3409090909091,
      "grad_norm": 0.2870348393917084,
      "learning_rate": 0.00022883979082233874,
      "loss": 2.1873,
      "step": 46790
    },
    {
      "epoch": 106.36363636363636,
      "grad_norm": 0.16334392130374908,
      "learning_rate": 0.00022874996440137182,
      "loss": 2.1869,
      "step": 46800
    },
    {
      "epoch": 106.38636363636364,
      "grad_norm": 0.1998639553785324,
      "learning_rate": 0.0002286601407436772,
      "loss": 2.1841,
      "step": 46810
    },
    {
      "epoch": 106.4090909090909,
      "grad_norm": 0.3360903561115265,
      "learning_rate": 0.00022857031986093506,
      "loss": 2.1797,
      "step": 46820
    },
    {
      "epoch": 106.43181818181819,
      "grad_norm": 0.21633858978748322,
      "learning_rate": 0.0002284805017648254,
      "loss": 2.1784,
      "step": 46830
    },
    {
      "epoch": 106.45454545454545,
      "grad_norm": 0.19492270052433014,
      "learning_rate": 0.00022839068646702786,
      "loss": 2.1894,
      "step": 46840
    },
    {
      "epoch": 106.47727272727273,
      "grad_norm": 0.45223504304885864,
      "learning_rate": 0.00022830087397922156,
      "loss": 2.1808,
      "step": 46850
    },
    {
      "epoch": 106.5,
      "grad_norm": 0.37826982140541077,
      "learning_rate": 0.00022821106431308543,
      "loss": 2.1736,
      "step": 46860
    },
    {
      "epoch": 106.52272727272727,
      "grad_norm": 0.19491848349571228,
      "learning_rate": 0.0002281212574802981,
      "loss": 2.1793,
      "step": 46870
    },
    {
      "epoch": 106.54545454545455,
      "grad_norm": 0.20910510420799255,
      "learning_rate": 0.00022803145349253756,
      "loss": 2.1804,
      "step": 46880
    },
    {
      "epoch": 106.56818181818181,
      "grad_norm": 0.44885995984077454,
      "learning_rate": 0.00022794165236148154,
      "loss": 2.1782,
      "step": 46890
    },
    {
      "epoch": 106.5909090909091,
      "grad_norm": 0.34316179156303406,
      "learning_rate": 0.00022785185409880746,
      "loss": 2.1783,
      "step": 46900
    },
    {
      "epoch": 106.61363636363636,
      "grad_norm": 0.22668448090553284,
      "learning_rate": 0.00022776205871619248,
      "loss": 2.1844,
      "step": 46910
    },
    {
      "epoch": 106.63636363636364,
      "grad_norm": 0.31371167302131653,
      "learning_rate": 0.00022767226622531306,
      "loss": 2.1751,
      "step": 46920
    },
    {
      "epoch": 106.6590909090909,
      "grad_norm": 0.23560898005962372,
      "learning_rate": 0.00022758247663784556,
      "loss": 2.1847,
      "step": 46930
    },
    {
      "epoch": 106.68181818181819,
      "grad_norm": 0.2790434658527374,
      "learning_rate": 0.00022749268996546587,
      "loss": 2.1828,
      "step": 46940
    },
    {
      "epoch": 106.70454545454545,
      "grad_norm": 0.39720502495765686,
      "learning_rate": 0.00022740290621984946,
      "loss": 2.1879,
      "step": 46950
    },
    {
      "epoch": 106.72727272727273,
      "grad_norm": 0.3539322018623352,
      "learning_rate": 0.00022731312541267144,
      "loss": 2.1777,
      "step": 46960
    },
    {
      "epoch": 106.75,
      "grad_norm": 0.43683600425720215,
      "learning_rate": 0.00022722334755560658,
      "loss": 2.1735,
      "step": 46970
    },
    {
      "epoch": 106.77272727272727,
      "grad_norm": 0.3180665373802185,
      "learning_rate": 0.00022713357266032942,
      "loss": 2.1885,
      "step": 46980
    },
    {
      "epoch": 106.79545454545455,
      "grad_norm": 0.23598331212997437,
      "learning_rate": 0.0002270438007385136,
      "loss": 2.1838,
      "step": 46990
    },
    {
      "epoch": 106.81818181818181,
      "grad_norm": 0.6699323654174805,
      "learning_rate": 0.000226954031801833,
      "loss": 2.1664,
      "step": 47000
    },
    {
      "epoch": 106.8409090909091,
      "grad_norm": 0.43250390887260437,
      "learning_rate": 0.00022686426586196074,
      "loss": 2.1845,
      "step": 47010
    },
    {
      "epoch": 106.86363636363636,
      "grad_norm": 0.35990992188453674,
      "learning_rate": 0.00022677450293056954,
      "loss": 2.2014,
      "step": 47020
    },
    {
      "epoch": 106.88636363636364,
      "grad_norm": 0.2862328290939331,
      "learning_rate": 0.00022668474301933188,
      "loss": 2.1843,
      "step": 47030
    },
    {
      "epoch": 106.9090909090909,
      "grad_norm": 0.9896671175956726,
      "learning_rate": 0.00022659498613991988,
      "loss": 2.1699,
      "step": 47040
    },
    {
      "epoch": 106.93181818181819,
      "grad_norm": 0.3129692077636719,
      "learning_rate": 0.00022650523230400505,
      "loss": 2.1841,
      "step": 47050
    },
    {
      "epoch": 106.95454545454545,
      "grad_norm": 0.23593272268772125,
      "learning_rate": 0.0002264154815232587,
      "loss": 2.1874,
      "step": 47060
    },
    {
      "epoch": 106.97727272727273,
      "grad_norm": 0.632639467716217,
      "learning_rate": 0.00022632573380935162,
      "loss": 2.1911,
      "step": 47070
    },
    {
      "epoch": 107.0,
      "grad_norm": 0.25460049510002136,
      "learning_rate": 0.0002262359891739544,
      "loss": 2.1866,
      "step": 47080
    },
    {
      "epoch": 107.0,
      "eval_loss": 1.0991803407669067,
      "eval_runtime": 8.9156,
      "eval_samples_per_second": 3413.222,
      "eval_steps_per_second": 13.347,
      "step": 47080
    },
    {
      "epoch": 107.02272727272727,
      "grad_norm": 0.21917855739593506,
      "learning_rate": 0.00022614624762873684,
      "loss": 2.1804,
      "step": 47090
    },
    {
      "epoch": 107.04545454545455,
      "grad_norm": 0.43748921155929565,
      "learning_rate": 0.00022605650918536873,
      "loss": 2.1765,
      "step": 47100
    },
    {
      "epoch": 107.06818181818181,
      "grad_norm": 0.42742764949798584,
      "learning_rate": 0.00022596677385551946,
      "loss": 2.1839,
      "step": 47110
    },
    {
      "epoch": 107.0909090909091,
      "grad_norm": 0.23098160326480865,
      "learning_rate": 0.0002258770416508576,
      "loss": 2.1751,
      "step": 47120
    },
    {
      "epoch": 107.11363636363636,
      "grad_norm": 0.20234990119934082,
      "learning_rate": 0.0002257873125830516,
      "loss": 2.1856,
      "step": 47130
    },
    {
      "epoch": 107.13636363636364,
      "grad_norm": 0.6741771697998047,
      "learning_rate": 0.00022569758666376972,
      "loss": 2.1817,
      "step": 47140
    },
    {
      "epoch": 107.1590909090909,
      "grad_norm": 0.19075363874435425,
      "learning_rate": 0.00022560786390467928,
      "loss": 2.1854,
      "step": 47150
    },
    {
      "epoch": 107.18181818181819,
      "grad_norm": 0.2725170850753784,
      "learning_rate": 0.0002255181443174476,
      "loss": 2.1858,
      "step": 47160
    },
    {
      "epoch": 107.20454545454545,
      "grad_norm": 0.17166557908058167,
      "learning_rate": 0.00022542842791374153,
      "loss": 2.1695,
      "step": 47170
    },
    {
      "epoch": 107.22727272727273,
      "grad_norm": 0.23791155219078064,
      "learning_rate": 0.00022533871470522743,
      "loss": 2.1747,
      "step": 47180
    },
    {
      "epoch": 107.25,
      "grad_norm": 0.19077464938163757,
      "learning_rate": 0.00022524900470357118,
      "loss": 2.179,
      "step": 47190
    },
    {
      "epoch": 107.27272727272727,
      "grad_norm": 0.2868976593017578,
      "learning_rate": 0.00022515929792043832,
      "loss": 2.195,
      "step": 47200
    },
    {
      "epoch": 107.29545454545455,
      "grad_norm": 0.16506049036979675,
      "learning_rate": 0.0002250695943674941,
      "loss": 2.1859,
      "step": 47210
    },
    {
      "epoch": 107.31818181818181,
      "grad_norm": 0.28301703929901123,
      "learning_rate": 0.00022497989405640304,
      "loss": 2.1843,
      "step": 47220
    },
    {
      "epoch": 107.3409090909091,
      "grad_norm": 0.3476264774799347,
      "learning_rate": 0.00022489019699882952,
      "loss": 2.1752,
      "step": 47230
    },
    {
      "epoch": 107.36363636363636,
      "grad_norm": 0.3531494438648224,
      "learning_rate": 0.00022480050320643747,
      "loss": 2.1769,
      "step": 47240
    },
    {
      "epoch": 107.38636363636364,
      "grad_norm": 0.3825644850730896,
      "learning_rate": 0.00022471081269089023,
      "loss": 2.1786,
      "step": 47250
    },
    {
      "epoch": 107.4090909090909,
      "grad_norm": 0.24656380712985992,
      "learning_rate": 0.00022462112546385075,
      "loss": 2.1777,
      "step": 47260
    },
    {
      "epoch": 107.43181818181819,
      "grad_norm": 0.18277744948863983,
      "learning_rate": 0.00022453144153698165,
      "loss": 2.1685,
      "step": 47270
    },
    {
      "epoch": 107.45454545454545,
      "grad_norm": 0.24879643321037292,
      "learning_rate": 0.00022444176092194527,
      "loss": 2.1748,
      "step": 47280
    },
    {
      "epoch": 107.47727272727273,
      "grad_norm": 0.18996071815490723,
      "learning_rate": 0.00022435208363040301,
      "loss": 2.1823,
      "step": 47290
    },
    {
      "epoch": 107.5,
      "grad_norm": 0.2643081545829773,
      "learning_rate": 0.0002242624096740164,
      "loss": 2.178,
      "step": 47300
    },
    {
      "epoch": 107.52272727272727,
      "grad_norm": 0.35369473695755005,
      "learning_rate": 0.00022417273906444627,
      "loss": 2.1893,
      "step": 47310
    },
    {
      "epoch": 107.54545454545455,
      "grad_norm": 0.43469005823135376,
      "learning_rate": 0.00022408307181335285,
      "loss": 2.1869,
      "step": 47320
    },
    {
      "epoch": 107.56818181818181,
      "grad_norm": 0.29685178399086,
      "learning_rate": 0.00022399340793239624,
      "loss": 2.1875,
      "step": 47330
    },
    {
      "epoch": 107.5909090909091,
      "grad_norm": 0.24094973504543304,
      "learning_rate": 0.00022390374743323614,
      "loss": 2.1813,
      "step": 47340
    },
    {
      "epoch": 107.61363636363636,
      "grad_norm": 0.6622296571731567,
      "learning_rate": 0.00022381409032753137,
      "loss": 2.1837,
      "step": 47350
    },
    {
      "epoch": 107.63636363636364,
      "grad_norm": 0.15607739984989166,
      "learning_rate": 0.00022372443662694078,
      "loss": 2.1662,
      "step": 47360
    },
    {
      "epoch": 107.6590909090909,
      "grad_norm": 0.20247313380241394,
      "learning_rate": 0.00022363478634312254,
      "loss": 2.1765,
      "step": 47370
    },
    {
      "epoch": 107.68181818181819,
      "grad_norm": 0.23470242321491241,
      "learning_rate": 0.00022354513948773445,
      "loss": 2.1794,
      "step": 47380
    },
    {
      "epoch": 107.70454545454545,
      "grad_norm": 0.3027147948741913,
      "learning_rate": 0.0002234554960724337,
      "loss": 2.1788,
      "step": 47390
    },
    {
      "epoch": 107.72727272727273,
      "grad_norm": 0.23109117150306702,
      "learning_rate": 0.00022336585610887734,
      "loss": 2.1841,
      "step": 47400
    },
    {
      "epoch": 107.75,
      "grad_norm": 0.16823968291282654,
      "learning_rate": 0.00022327621960872184,
      "loss": 2.1848,
      "step": 47410
    },
    {
      "epoch": 107.77272727272727,
      "grad_norm": 0.15728004276752472,
      "learning_rate": 0.00022318658658362298,
      "loss": 2.1863,
      "step": 47420
    },
    {
      "epoch": 107.79545454545455,
      "grad_norm": 0.2466980665922165,
      "learning_rate": 0.00022309695704523643,
      "loss": 2.1758,
      "step": 47430
    },
    {
      "epoch": 107.81818181818181,
      "grad_norm": 0.5118401646614075,
      "learning_rate": 0.00022300733100521732,
      "loss": 2.1925,
      "step": 47440
    },
    {
      "epoch": 107.8409090909091,
      "grad_norm": 0.19435147941112518,
      "learning_rate": 0.0002229177084752201,
      "loss": 2.1875,
      "step": 47450
    },
    {
      "epoch": 107.86363636363636,
      "grad_norm": 0.2274630218744278,
      "learning_rate": 0.000222828089466899,
      "loss": 2.1913,
      "step": 47460
    },
    {
      "epoch": 107.88636363636364,
      "grad_norm": 0.3791959881782532,
      "learning_rate": 0.0002227384739919079,
      "loss": 2.1836,
      "step": 47470
    },
    {
      "epoch": 107.9090909090909,
      "grad_norm": 0.3324669301509857,
      "learning_rate": 0.00022264886206189977,
      "loss": 2.1894,
      "step": 47480
    },
    {
      "epoch": 107.93181818181819,
      "grad_norm": 0.1829947680234909,
      "learning_rate": 0.0002225592536885276,
      "loss": 2.1773,
      "step": 47490
    },
    {
      "epoch": 107.95454545454545,
      "grad_norm": 0.2774417996406555,
      "learning_rate": 0.00022246964888344357,
      "loss": 2.1877,
      "step": 47500
    },
    {
      "epoch": 107.97727272727273,
      "grad_norm": 0.252768874168396,
      "learning_rate": 0.00022238004765829972,
      "loss": 2.1978,
      "step": 47510
    },
    {
      "epoch": 108.0,
      "grad_norm": 0.6129856705665588,
      "learning_rate": 0.00022229045002474727,
      "loss": 2.1865,
      "step": 47520
    },
    {
      "epoch": 108.0,
      "eval_loss": 1.0991942882537842,
      "eval_runtime": 8.7281,
      "eval_samples_per_second": 3486.553,
      "eval_steps_per_second": 13.634,
      "step": 47520
    },
    {
      "epoch": 108.0,
      "step": 47520,
      "total_flos": 6.568401046811443e+16,
      "train_loss": 2.280276156475247,
      "train_runtime": 12827.8801,
      "train_samples_per_second": 3508.935,
      "train_steps_per_second": 6.86
    }
  ],
  "logging_steps": 10,
  "max_steps": 88000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 200,
  "save_steps": 1000,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 20,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 20
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.568401046811443e+16,
  "train_batch_size": 256,
  "trial_name": null,
  "trial_params": null
}
